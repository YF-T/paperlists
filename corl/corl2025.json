[
    {
        "id": "06YyNxzwae",
        "title": "Fail2Progress: Learning from Real-World Robot Failures with Stein Variational Inference",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Skill effect models for long-horizon manipulation tasks are prone to failures in conditions not covered by training data distributions. Therefore, enabling robots to reason about and learn from failures is necessary. We investigate the problem of efficiently generating a dataset targeted to observed failures. After fine-tuning a skill effect model on this dataset, we evaluate the extent to which the model can recover from failures and minimize future failures. We propose Fail2Progress, an approach that leverages Stein variational inference to generate multiple simulation environments in parallel, enabling efficient data sample generation similar to observed failures. Our method is capable of handling several challenging mobile manipulation tasks, including transporting multiple objects, organizing a constrained shelf, and tabletop organization. Through large-scale simulation and real-world experiments, we demonstrate that our approach excels at learning from failures across different numbers of objects. Furthermore, we show that Fail2Progress outperforms several baselines.",
        "keywords": "Learning from failures;Variational inference;Skill effect models",
        "primary_area": "",
        "supplementary_material": "/attachment/9199a57ac435822efe500ae2fc40af4e41be0480.zip",
        "author": "Yixuan Huang;Novella Alvina;Mohanraj Devendran Shanthi;Tucker Hermans",
        "authorids": "~Yixuan_Huang1;~Novella_Alvina1;~Mohanraj_Devendran_Shanthi1;~Tucker_Hermans2",
        "gender": ";F;M;M",
        "homepage": "https://yixuanhuang98.github.io/;https://novellaalvina.github.io/;http://mohanrajds.com/;https://robot-learning.cs.utah.edu",
        "dblp": ";;;https://dblp.uni-trier.de/pid/67/4241",
        "google_scholar": ";;hNFn3aYAAAAJ;G5_VFfkAAAAJ",
        "orcid": ";;;0000-0003-2496-2768",
        "linkedin": ";;;",
        "or_profile": "~Yixuan_Huang1;~Novella_Alvina1;~Mohanraj_Devendran_Shanthi1;~Tucker_Hermans2",
        "aff": "Princeton University+University of Utah;University of Utah;, University of Utah;NVIDIA+University of Utah",
        "aff_domain": "princeton.edu+utah.edu;utah.edu;cs.utah.edu;nvidia.com+utah.edu",
        "position": "Postdoctoral Research Associate+PhD student;MS student;PhD student;Researcher+Associate Professor",
        "bibtex": "@inproceedings{\nhuang2025failprogress,\ntitle={Fail2Progress: Learning from Real-World Robot Failures with Stein Variational Inference},\nauthor={Yixuan Huang and Novella Alvina and Mohanraj Devendran Shanthi and Tucker Hermans},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=06YyNxzwae}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=06YyNxzwae",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;1;1;2+1",
        "aff_unique_norm": "Princeton University;University of Utah;NVIDIA",
        "aff_unique_dep": ";;NVIDIA Corporation",
        "aff_unique_url": "https://www.princeton.edu;https://www.utah.edu;https://www.nvidia.com",
        "aff_unique_abbr": "Princeton;Utah;NVIDIA",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "0ViTEgiFiQ",
        "title": "Disentangled Multi-Context Meta-Learning: Unlocking Robust and Generalized Task Learning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In meta-learning and its downstream tasks, many methods use implicit adaptation to represent task-specific variations. However, implicit approaches hinder interpretability and make it difficult to understand which task factors drive performance. In this work, we introduce a disentangled multi-context meta-learning framework that explicitly learns separate context vectors for different aspects that define a task. By decoupling these factors, our approach improves both robustness, through deeper task understanding, and generalization, by enabling context vector sharing across tasks with the same context. We evaluate our approach in two domains. First, on a sinusoidal regression benchmark, our model outperforms baselines on out-of-distribution tasks and generalizes to unseen sine functions by sharing context vectors associated with shared amplitudes or phase shifts. Second, in a quadruped locomotion task, we disentangle the robot-specific properties and the characteristics of the terrain in the robot dynamics model. Using these context vectors in reinforcement learning, the learned policy demonstrates improved robustness under out-of-distribution conditions, compared to a model using a single unified context. Furthermore, by effectively sharing context, our model enables successful sim-to-real policy transfer to challenging terrains with out-of-distribution robot-specific properties using only real data from flat terrain, which is not achievable with single-task adaptation.",
        "keywords": "Meta-Learning;Multi Task Learning;Quadruped Robot Locomotion",
        "primary_area": "",
        "supplementary_material": "/attachment/b8e495e152630f720a5029e348952e578f251e8f.zip",
        "author": "Seonsoo Kim;Jun-Gill Kang;Taehong Kim;Seongil Hong",
        "authorids": "~Seonsoo_Kim1;~Jun-Gill_Kang1;~Taehong_Kim3;~Seongil_Hong1",
        "gender": "M;;;M",
        "homepage": ";;;",
        "dblp": ";;;",
        "google_scholar": ";nLzEIlcAAAAJ;https://scholar.google.co.kr/citations?hl=ko;https://scholar.google.co.kr/citations?user=7z5V0LgAAAAJ",
        "orcid": ";;;",
        "linkedin": "https://kr.linkedin.com/in/%EC%84%A0%EC%88%98-%EA%B9%80-1522ba347;;;",
        "or_profile": "~Seonsoo_Kim1;~Jun-Gill_Kang1;~Taehong_Kim3;~Seongil_Hong1",
        "aff": "Agency for Defense Development;Agency For Defense Development;Agency for defense development;Agency for Defense Development",
        "aff_domain": "re.kr;add.re.kr;add.re.kr;add.re.kr",
        "position": "Researcher;Researcher;Researcher;Researcher",
        "bibtex": "@inproceedings{\nkim2025disentangled,\ntitle={Disentangled Multi-Context Meta-Learning: Unlocking Robust and Generalized Task Learning},\nauthor={Seonsoo Kim and Jun-Gill Kang and Taehong Kim and Seongil Hong},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=0ViTEgiFiQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=0ViTEgiFiQ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Agency for Defense Development;Agency For Defense Development;Agency for defense development",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.add.re.kr;;",
        "aff_unique_abbr": "ADD;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "South Korea;"
    },
    {
        "id": "19LSN4QnV4",
        "title": "FOMO-3D: Using Vision Foundation Models for Long-Tailed 3D Object Detection",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In order to navigate complex traffic environments, self-driving vehicles\nmust recognize many semantic classes pertaining to vulnerable road users or\ntraffic control devices.  However, many safety-critical objects (e.g.,\nconstruction worker) appear infrequently in nominal traffic conditions, leading to a\nsevere shortage of training examples from driving data alone. \nRecent vision foundation models, which are trained on a large corpus of\ndata, can serve as a good source of external prior knowledge to improve\ngeneralization. We propose FOMO-3D, the first 3D detector\nto leverage vision foundation models for long-tailed 3D detection. Specifically,\nFOMO-3D exploits rich semantic and depth priors from OWLv2 and Metric3Dv2 within\na two-stage detection paradigm that first generates proposals with a\nLiDAR-based branch and a novel camera-based branch, and refines them with\nattention especially to image features from OWL. Evaluations on real-world\ndriving data show that using rich priors from vision\nfoundation models with careful multimodal fusion designs leads to large gains\nfor long-tailed 3D detection.",
        "keywords": "Long-Tailed 3D Object Detection;Vision Foundation Model;Multimodal Fusion;Autonomous Vehicles",
        "primary_area": "",
        "supplementary_material": "/attachment/f27ebcd89d157f0c0cae5bf8c325862fe3e4d8e0.zip",
        "author": "Anqi Joyce Yang;James Tu;Nikita Dvornik;Enxu Li;Raquel Urtasun",
        "authorids": "~Anqi_Joyce_Yang1;~James_Tu1;~Nikita_Dvornik1;~Enxu_Li1;~Raquel_Urtasun1",
        "gender": "F;M;M;;F",
        "homepage": "https://www.cs.toronto.edu/~ajyang/;;https://dvornikita.github.io/;https://www.cs.toronto.edu/~tli/;http://www.cs.toronto.edu/~urtasun/",
        "dblp": "283/5790;;205/2510;285/4934;u/RaquelUrtasun",
        "google_scholar": "DxnwQqgAAAAJ;https://scholar.google.ca/citations?user=x6gPeg4AAAAJ;UOLJQTIAAAAJ;Bk4LuGYAAAAJ;https://scholar.google.ca/citations?user=jyxO2akAAAAJ",
        "orcid": ";;;;",
        "linkedin": "ajyang99/;;;thomas-enxu-li/;",
        "or_profile": "~Anqi_Joyce_Yang1;~James_Tu1;~Nikita_Dvornik1;~Enxu_Li1;~Raquel_Urtasun1",
        "aff": "University of Toronto+Waabi Innovation Inc;Department of Computer Science, University of Toronto;Palona AI;Department of Computer Science, University of Toronto+Waabi;Waabi+Department of Computer Science, University of Toronto",
        "aff_domain": "toronto.edu+waabi.ai;cs.toronto.edu;palona.ai;cs.toronto.edu+waabi.ai;waabi.ai+cs.toronto.edu",
        "position": "PhD student+Researcher;PhD student;Principal Researcher;PhD student+Researcher;Principal Researcher+Full Professor",
        "bibtex": "@inproceedings{\nyang2025fomod,\ntitle={{FOMO}-3D: Using Vision Foundation Models for Long-Tailed 3D Object Detection},\nauthor={Anqi Joyce Yang and James Tu and Nikita Dvornik and Enxu Li and Raquel Urtasun},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=19LSN4QnV4}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=19LSN4QnV4",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;0;2;0+3;3+0",
        "aff_unique_norm": "University of Toronto;Waabi Innovation Inc;Palona AI;Waabi",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.waabi.ai;;",
        "aff_unique_abbr": "U of T;;;",
        "aff_campus_unique_index": ";1;1;1",
        "aff_campus_unique": ";Toronto",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Canada;"
    },
    {
        "id": "1D6XYy6ofW",
        "title": "LaVA-Man: Learning Visual Action Representations for Robot Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Visual-textual understanding is essential for language-guided robot manipulation. Recent works leverage pre-trained vision-language models to measure the similarity between encoded visual observations and textual instructions, and then train a model to map this similarity to robot actions. However, this two-step approach limits the model to capture the relationship between visual observations and textual instructions, leading to reduced precision in manipulation tasks. We propose to learn visual-textual associations through a self-supervised pretext task: reconstructing a masked goal image conditioned on an input image and textual instructions. This formulation allows the model to learn visual-action representations without robot action supervision. The learned representations can then be fine-tuned for manipulation tasks with only a few demonstrations. We also introduce the \\textit{Omni-Object Pick-and-Place} dataset, which consists of annotated robot tabletop manipulation episodes, including 180 object classes and 3,200 instances with corresponding textual instructions. This dataset enables the model to acquire diverse object priors and allows for a more comprehensive evaluation of its generalisation capability across object instances. Experimental results on the five benchmarks, including both simulated and real-robot validations, demonstrate that our method outperforms prior art.",
        "keywords": "Robot manipulation;self-supervised representation learning",
        "primary_area": "",
        "supplementary_material": "/attachment/c85c01ea1e7346775d68345268f15c316a5f7dc7.zip",
        "author": "Chaoran Zhu;Hengyi Wang;Yik Lung Pang;Changjae Oh",
        "authorids": "~Chaoran_Zhu1;~Hengyi_Wang2;~Yik_Lung_Pang1;~Changjae_Oh1",
        "gender": "M;M;;",
        "homepage": ";https://hengyiwang.github.io/;;",
        "dblp": ";;;",
        "google_scholar": "12U1mcgAAAAJ;2d9j2_wAAAAJ;xPEdLeoAAAAJ;",
        "orcid": ";;;",
        "linkedin": ";hengyi-wang-a335b11bb/;;",
        "or_profile": "~Chaoran_Zhu1;~Hengyi_Wang2;~Yik_Lung_Pang1;~Changjae_Oh1",
        "aff": "Queen Mary University of London;University College London;;",
        "aff_domain": "qmul.ac.uk;ucl.ac.uk;;",
        "position": "PhD student;PhD student;;",
        "bibtex": "@inproceedings{\nzhu2025lavaman,\ntitle={La{VA}-Man: Learning Visual Action Representations for Robot Manipulation},\nauthor={Chaoran Zhu and Hengyi Wang and Yik Lung Pang and Changjae Oh},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1D6XYy6ofW}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1D6XYy6ofW",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;2",
        "aff_unique_norm": "Queen Mary University of London;University College London;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.qmul.ac.uk;https://www.ucl.ac.uk;",
        "aff_unique_abbr": "QMUL;UCL;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "London;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "1HW2UhshIT",
        "title": "Force-Modulated Visual Policy for Robot-Assisted Dressing with Arm Motions",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Robot-assisted dressing has the potential to significantly improve the lives of individuals with mobility impairments. To ensure an effective and comfortable dressing experience, the robot must be able to handle challenging deformable garments, apply appropriate forces, and adapt to limb movements throughout the dressing process. Prior work often makes simplifying assumptions\u2014such as static human limbs during dressing\u2014which limits real-world applicability. In this work, we develop a robot-assisted dressing system capable of handling partial observations with visual occlusions, as well as robustly adapting to arm motions during the dressing process. Given a policy trained in simulation with partial observations, we propose a method to fine-tune it in the real world using a small amount of data and multi-modal feedback from vision and force sensing, to further improve the policy\u2019s adaptability to arm motions and enhance safety. We evaluate our method in simulation with simplified articulated human meshes and in a real world human study with 12 participants across 264 dressing trials. Our policy successfully dresses two long-sleeve everyday garments onto the participants while being adaptive to various kinds of arm motions, and greatly outperforms prior baselines in terms of task completion and user feedback.",
        "keywords": "Robot-Assisted Dressing;Multi-Modal Learning;Physical Human Robot Interaction;Deformable Object Manipulation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alexis Yihong Hao;Yufei Wang;Navin Sriram Ravie;Bharath Hegde;David Held;Zackory Erickson",
        "authorids": "~Alexis_Yihong_Hao1;~Yufei_Wang4;~Navin_Sriram_Ravie1;~Bharath_Hegde2;~David_Held1;~Zackory_Erickson1",
        "gender": "F;;M;M;M;M",
        "homepage": ";https://yufeiwang63.github.io/;https://n47in.github.io/N47IN-N47IN.github.io/;https://bharath-hegde.github.io/;http://davheld.github.io/;https://zackory.com",
        "dblp": ";;;;22/11147;",
        "google_scholar": ";HQl9718AAAAJ;;;0QtU-NsAAAAJ;wElkTtIAAAAJ",
        "orcid": "0000-0002-7047-3917;;;;;",
        "linkedin": ";;navin-sriram-5b5690256?trk=contact-info;bharathhegde101/;;",
        "or_profile": "~Alexis_Yihong_Hao1;~Yufei_Wang4;~Navin_Sriram_Ravie1;~Bharath_Hegde2;~David_Held1;~Zackory_Erickson1",
        "aff": "Carnegie Mellon University;School of Computer Science, Carnegie Mellon University;Indian Institute of Technology Madras;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;cs.cmu.edu;iiitm.ac.in;andrew.cmu.edu;cmu.edu;cmu.edu",
        "position": "Intern;PhD student;Undergrad student;MS student;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nhao2025forcemodulated,\ntitle={Force-Modulated Visual Policy for Robot-Assisted Dressing with Arm Motions},\nauthor={Alexis Yihong Hao and Yufei Wang and Navin Sriram Ravie and Bharath Hegde and David Held and Zackory Erickson},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1HW2UhshIT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1HW2UhshIT",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "Carnegie Mellon University;Indian Institute of Technology Madras",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;https://www.iitm.ac.in",
        "aff_unique_abbr": "CMU;IIT Madras",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Pittsburgh;Madras",
        "aff_country_unique_index": "0;0;1;0;0;0",
        "aff_country_unique": "United States;India"
    },
    {
        "id": "1K3kjo91Q1",
        "title": "Learning from 10 Demos: Generalisable and Sample-Efficient Policy Learning with Oriented Affordance Frames",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Imitation learning has unlocked the potential for robots to exhibit highly dexterous behaviours. However, it still struggles with long-horizon, multi-object tasks due to poor sample efficiency and limited generalisation. Existing methods require a substantial number of demonstrations to cover possible task variations, making them costly and often impractical for real-world deployment. \nWe address this challenge by introducing \\emph{oriented affordance frames}, a structured representation for state and action spaces that improves spatial and intra-category generalisation and enables policies to be learned efficiently from only 10 demonstrations. More importantly, we show how this abstraction allows for compositional generalisation of independently trained sub-policies to solve long-horizon, multi-object tasks. To seamlessly transition between sub-policies, we introduce the notion of self-progress prediction, which we directly derive from the duration of the training demonstrations. We validate our method across three real-world tasks, each requiring multi-step, multi-object interactions. Despite the small dataset, our policies generalise robustly to unseen object appearances, geometries, and spatial arrangements, achieving high success rates without reliance on exhaustive training data. Video demonstration can be found on our anonymised project page: https://affordance-policy.github.io/.",
        "keywords": "behaviour cloning;spatial generalisation;intra-category generalisation;long-horizon tasks;affordances",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Krishan Rana;Jad Abou-Chakra;Sourav Garg;Robert Lee;Ian Reid;Niko Suenderhauf",
        "authorids": "~Krishan_Rana1;~Jad_Abou-Chakra1;~Sourav_Garg1;~Robert_Lee1;~Ian_Reid1;~Niko_Suenderhauf1",
        "gender": "M;M;M;M;M;M",
        "homepage": "https://krishanrana.github.io/;;https://oravus.github.io/;;;http://nikosuenderhauf.info",
        "dblp": "70/4142;;142/0073;;r/IanDReid1;",
        "google_scholar": "-hYjPxsAAAAJ;;oVS3HHIAAAAJ;https://scholar.google.com.au/citations?hl=en;https://scholar.google.com.au/citations?user=ATkNLcQAAAAJ;https://scholar.google.com.au/citations?user=WnKjfFEAAAAJ",
        "orcid": "0000-0002-9028-9295;0000-0002-9122-3132;0000-0001-6068-3307;;0000-0001-7790-6423;",
        "linkedin": "krishanrana/;;gargsourav/;robert-lee-a8a98922b?trk=public_profile_browsemap;;nikosuenderhauf/",
        "or_profile": "~Krishan_Rana1;~Jad_Abou-Chakra1;~Sourav_Garg1;~Robert_Lee1;~Ian_Reid1;~Niko_Suenderhauf1",
        "aff": "Queensland University of Technology;The AI Institute+Queensland University of Technology;University of Adelaide;Woven By Toyota, Inc.;Mohamed bin Zayed University of Artificial Intelligence+University of Adelaide;Queensland University of Technology",
        "aff_domain": "qut.edu.au;theaiinstitute.com+qut.edu.au;adelaide.edu.au;woven.toyota;mbzuai.ac.ae+adelaide.edu.au;qut.edu.au",
        "position": "Postdoc;Intern+PhD student;Postdoc;Researcher;Full Professor+Professor;Full Professor",
        "bibtex": "@inproceedings{\nrana2025learning,\ntitle={Learning from 10 Demos: Generalisable and Sample-Efficient Policy Learning with Oriented Affordance Frames},\nauthor={Krishan Rana and Jad Abou-Chakra and Sourav Garg and Robert Lee and Ian Reid and Niko Suenderhauf},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1K3kjo91Q1}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1K3kjo91Q1",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1+0;2;3;4+2;0",
        "aff_unique_norm": "Queensland University of Technology;AI Institute;University of Adelaide;Woven By Toyota, Inc.;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.qut.edu.au;;https://www.adelaide.edu.au;;https://mbzuai.ac.ae",
        "aff_unique_abbr": "QUT;;Adelaide;;MBZUAI",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1+0;0;3+0;0",
        "aff_country_unique": "Australia;United States;;United Arab Emirates"
    },
    {
        "id": "1NBdplgILy",
        "title": "Vision in Action: Learning Active Perception from Human Demonstrations",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We present Vision in Action (ViA), an active perception system for bimanual robot manipulation. ViA learns task-relevant active perceptual strategies (e.g., searching, tracking, and focusing) directly from human demonstrations. On the hardware side, ViA employs a simple yet effective 6-DoF robotic neck to enable flexible, human-like head movements. To capture human active perception strategies, we design a VR-based teleoperation interface that creates a shared observation space between the robot and the human operator. To mitigate VR motion sickness caused by latency in the robot's physical movements, the interface uses an intermediate 3D scene representation, enabling real-time view rendering on the operator side while asynchronously updating the scene with the robot's latest observations. Together, these design elements enable the learning of robust visuomotor policies for three complex, multi-stage bimanual manipulation tasks involving visual occlusions, significantly outperforming baseline systems.",
        "keywords": "Active Perception;Bimanual Manipulation;Imitation Learning;Teleoperation Systems",
        "primary_area": "",
        "supplementary_material": "/attachment/b68ffcf63b31127c2836e7a68cb0c8efd8264036.zip",
        "author": "Haoyu Xiong;Xiaomeng Xu;Jimmy Wu;Yifan Hou;Jeannette Bohg;Shuran Song",
        "authorids": "~Haoyu_Xiong3;~Xiaomeng_Xu1;~Jimmy_Wu1;~Yifan_Hou2;~Jeannette_Bohg1;~Shuran_Song3",
        "gender": "M;F;M;M;;F",
        "homepage": "https://haoyu-x.github.io/;https://xxm19.github.io/;http://jimmyyhwu.github.io;https://profiles.stanford.edu/yifan-hou;https://web.stanford.edu/~bohg/;https://shurans.github.io/",
        "dblp": ";160/9754;00/8739;;52/7377;",
        "google_scholar": ";af_4iHYAAAAJ;UoQdAc4AAAAJ;85D2bRgAAAAJ;rjnJnEkAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;0000-0001-8446-941X;0000-0002-4921-7193;",
        "linkedin": ";;;;;",
        "or_profile": "~Haoyu_Xiong3;~Xiaomeng_Xu1;~Jimmy_Wu1;~Yifan_Hou2;~Jeannette_Bohg1;~Shuran_Song3",
        "aff": "Massachusetts Institute of Technology;Stanford University;Princeton University;Stanford University;Stanford University;Stanford University",
        "aff_domain": "mit.edu;stanford.edu;princeton.edu;stanford.edu;stanford.edu;stanford.edu",
        "position": "PhD student;PhD student;PhD student;Postdoc;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nxiong2025vision,\ntitle={Vision in Action: Learning Active Perception from Human Demonstrations},\nauthor={Haoyu Xiong and Xiaomeng Xu and Jimmy Wu and Yifan Hou and Jeannette Bohg and Shuran Song},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1NBdplgILy}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1NBdplgILy",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;1;1;1",
        "aff_unique_norm": "Massachusetts Institute of Technology;Stanford University;Princeton University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://web.mit.edu;https://www.stanford.edu;https://www.princeton.edu",
        "aff_unique_abbr": "MIT;Stanford;Princeton",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "1TdRe3wPqK",
        "title": "Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "End-to-end visuomotor policies trained using behavior cloning have shown a remarkable ability to generate complex, multi-modal low-level robot behaviors. However, at deployment time, these policies still struggle to act reliably when faced with out-of-distribution (OOD) visuals induced by objects, backgrounds, or environment changes. Prior works in interactive imitation learning solicit corrective expert demonstrations under the OOD conditions---but this can be costly and inefficient. We observe that task success under OOD conditions does not always warrant novel robot behaviors. In-distribution (ID) behaviors can directly be transferred to OOD conditions that share functional similarities with ID conditions. For example, behaviors trained to interact with in-distribution (ID) pens can apply to interacting with a visually-OOD pencil. The key challenge lies in disambiguating which ID observations functionally correspond to the OOD observation for the task at hand. We propose that an expert can provide this OOD-to-ID functional correspondence. Thus, instead of collecting new demonstrations and re-training at every OOD encounter, our method: (1) detects the need for feedback by checking if current observations are OOD and the most similar training observations show divergent behaviors (2) solicits functional correspondence feedback to disambiguate between those behaviors, and (3) intervenes on the OOD observations with the functionally corresponding ID observations to perform deployment-time generalization. We validate our method across diverse real-world robotic manipulation tasks with a Franka Panda robotic manipulator. Our results show that test-time functional correspondences can improve the generalization of a vision-based diffusion policy to OOD objects and environment conditions with low feedback.",
        "keywords": "Visuomotor Policy;Out-of-Distribution Generalization;Functional Correspondence;Deployment-Time Adaptation",
        "primary_area": "",
        "supplementary_material": "/attachment/70d35e6fd5c51e7140c3657a09a60ac4452147c3.pdf",
        "author": "Pranay Gupta;Henny Admoni;Andrea Bajcsy",
        "authorids": "~Pranay_Gupta1;~Henny_Admoni1;~Andrea_Bajcsy1",
        "gender": "M;;",
        "homepage": "https://pranaygupta36.github.io;https://hennyadmoni.com;",
        "dblp": ";44/7075;",
        "google_scholar": ";XXiZaA4AAAAJ;",
        "orcid": ";;",
        "linkedin": "pranay-gupta-825713134/;;",
        "or_profile": "~Pranay_Gupta1;~Henny_Admoni1;~Andrea_Bajcsy1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University;",
        "aff_domain": "cmu.edu;cmu.edu;",
        "position": "PhD student;Assistant Professor;",
        "bibtex": "@inproceedings{\ngupta2025adapting,\ntitle={Adapting by Analogy: {OOD} Generalization of Visuomotor Policies via Functional Correspondence},\nauthor={Pranay Gupta and Henny Admoni and Andrea Bajcsy},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1TdRe3wPqK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1TdRe3wPqK",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Carnegie Mellon University;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;",
        "aff_unique_abbr": "CMU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "1cA6OYsfoJ",
        "title": "From Real World to Logic and Back: Learning Generalizable Relational Concepts For Long Horizon Robot Planning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Humans efficiently generalize from limited demonstrations, but robots still struggle to transfer learned knowledge to complex, unseen tasks with longer horizons and increased complexity. \nWe propose the first known method enabling robots to autonomously invent relational concepts directly from small sets of unannotated, unsegmented demonstrations. The learned symbolic concepts are grounded into logic-based world models, facilitating efficient zero-shot generalization to significantly more complex tasks. Empirical results demonstrate that our approach achieves performance comparable to hand-crafted models, successfully scaling execution horizons and handling up to 18 times more objects than seen in training, providing the first autonomous framework for learning transferable symbolic abstractions from raw robot trajectories.",
        "keywords": "Learnng symbolic abstractions;Symbolic world model learning;Learning for task and motion planning;learning for planning",
        "primary_area": "",
        "supplementary_material": "/attachment/62ee16d18189ce71d6a12147bffd728dc50d44e2.zip",
        "author": "Naman Shah;Jayesh Nagpal;Siddharth Srivastava",
        "authorids": "~Naman_Shah1;~Jayesh_Nagpal1;~Siddharth_Srivastava2",
        "gender": "M;M;",
        "homepage": "https://namanshah.net;;",
        "dblp": "194/7474-2.html;;",
        "google_scholar": "q4v6iroAAAAJ;5fo5ymQAAAAJ;",
        "orcid": ";;",
        "linkedin": ";;",
        "or_profile": "~Naman_Shah1;~Jayesh_Nagpal1;~Siddharth_Srivastava2",
        "aff": "Allen Institute for Artificial Intelligence+Brown University;;",
        "aff_domain": "allenai.org+brown.edu;;",
        "position": "Researcher+Postdoc;;",
        "bibtex": "@inproceedings{\nshah2025from,\ntitle={From Real World to Logic and Back: Learning Generalizable Relational Concepts For Long Horizon Robot Planning},\nauthor={Naman Shah and Jayesh Nagpal and Siddharth Srivastava},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1cA6OYsfoJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1cA6OYsfoJ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;2;2",
        "aff_unique_norm": "Allen Institute for Artificial Intelligence;Brown University;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://allenai.org;https://www.brown.edu;",
        "aff_unique_abbr": "AI2;Brown;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "1n1Liq6So4",
        "title": "Meta-Optimization and Program Search using Language Models for Task and Motion Planning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Intelligent interaction with the real world requires robotic agents to jointly reason over high-level plans and low-level controls. This requirement is formalized in the task and motion planning (TAMP) problem, in which symbolic planning and continuous trajectory generation must be solved in a coordinated manner. Recently, foundation model-based approaches to TAMP have presented impressive results, including fast planning times and the execution of natural language instructions. Yet, the optimal interface between high-level plan and low-level motion generation remains to be found: prior approaches are limited by either too much abstraction (e.g., chaining simplified skill primitives) or a lack thereof (e.g., direct joint angle prediction). Our method introduces a novel technique employing a form of meta-optimization to address these shortcomings by: (i) using program search over trajectory optimization problems as an interface between foundation model and robot controllers, and (ii) leveraging a zero-order method to optimize numerical values in the foundation model output. Results on challenging object manipulation and drawing tasks confirm that our proposed method improves over prior TAMP approaches.",
        "keywords": "Task and Motion Planning;LLMs as Optimizers;Trajectory Optimization",
        "primary_area": "",
        "supplementary_material": "/attachment/042046c99c92632a036e6e40fd0e122011998816.zip",
        "author": "Denis Shcherba;Eckart Cobo-Briesewitz;Cornelius V. Braun;Marc Toussaint",
        "authorids": "~Denis_Shcherba1;~Eckart_Cobo-Briesewitz1;~Cornelius_V._Braun1;~Marc_Toussaint3",
        "gender": "M;M;M;M",
        "homepage": ";https://tupryk.github.io/;;https://www.user.tu-berlin.de/mtoussai/",
        "dblp": ";;;t/MarcToussaint",
        "google_scholar": ";W3Oaxa8AAAAJ;Fh-XpPkAAAAJ;t2X4Mg8AAAAJ",
        "orcid": ";;;0000-0002-5487-6767",
        "linkedin": "denis-shcherba-2a363b161/;https://linkedin.com/in/eckart-cobo-briesewitz;;marctoussaint/",
        "or_profile": "~Denis_Shcherba1;~Eckart_Cobo-Briesewitz1;~Cornelius_V._Braun1;~Marc_Toussaint3",
        "aff": "Technische Universit\u00e4t Berlin;Technische Universit\u00e4t Berlin;Technische Universit\u00e4t Berlin;TU Berlin",
        "aff_domain": "tu-berlin.de;tu-berlin.de;tu-berlin.de;tu-berlin.de",
        "position": "MS student;MS student;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nshcherba2025metaoptimization,\ntitle={Meta-Optimization and Program Search using Language Models for Task and Motion Planning},\nauthor={Denis Shcherba and Eckart Cobo-Briesewitz and Cornelius V. Braun and Marc Toussaint},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1n1Liq6So4}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1n1Liq6So4",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Technische Universit\u00e4t Berlin",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tu-berlin.de",
        "aff_unique_abbr": "TU Berlin",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berlin",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "1otaE496Vm",
        "title": "CaRL: Learning Scalable Planning Policies with Simple Rewards",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We investigate reinforcement learning (RL) for privileged planning in autonomous driving. \nState-of-the-art approaches for this task are rule-based, but these methods do not scale to the long tail.\nRL, on the other hand, is scalable and does not suffer from compounding errors like imitation learning.\nContemporary RL approaches for driving use complex shaped rewards that sum multiple individual rewards, \\eg~progress, position, or orientation rewards. \nWe show that PPO fails to optimize a popular version of these rewards when the mini-batch size is increased, which limits the scalability of these approaches.\nInstead, we propose a new reward design based primarily on optimizing a single intuitive reward term: route completion. \nInfractions are penalized by terminating the episode or multiplicatively reducing route completion.\nWe find that PPO scales well with higher mini-batch sizes when trained with our simple reward, even improving performance.\nTraining with large mini-batch sizes enables efficient scaling via distributed data parallelism. \nWe scale PPO to 300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node.\nThe resulting model achieves 64 DS on the CARLA longest6 v2 benchmark, outperforming other RL methods with more complex rewards by a large margin.\nRequiring only minimal adaptations from its use in CARLA, the same method is the best learning-based approach on nuPlan.\nIt scores 91.3 in non-reactive and 90.6 in reactive traffic on the Val14 benchmark while being an order of magnitude faster than prior work.",
        "keywords": "Autonomous Driving;Reinforcement Learning;Planning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Bernhard Jaeger;Daniel Dauner;Jens Bei\u00dfwenger;Simon Gerstenecker;Kashyap Chitta;Andreas Geiger",
        "authorids": "~Bernhard_Jaeger1;~Daniel_Dauner1;~Jens_Bei\u00dfwenger1;simongerstenecker@gmail.com;~Kashyap_Chitta1;~Andreas_Geiger3",
        "gender": "M;;M;;M;M",
        "homepage": "https://kait0.github.io/;https://danieldauner.github.io/;https://j-beisswenger.github.io/;;https://kashyap7x.github.io/;http://www.cvlibs.net",
        "dblp": "327/9257;349/4864;;;220/3765;40/5825-1",
        "google_scholar": "https://scholar.google.de/citations?user=JpceFvgAAAAJ;tZqIYDcAAAAJ;https://scholar.google.de/citations?user=bnJaIP4AAAAJ;;vX5i2CcAAAAJ;https://scholar.google.ca/citations?hl=en",
        "orcid": "0000-0001-6657-2499;;;;;0000-0002-8151-3726",
        "linkedin": "bernhard-jaeger-289b65160/;;jens-bei%C3%9Fwenger-a82430258/;;;",
        "or_profile": "~Bernhard_Jaeger1;~Daniel_Dauner1;~Jens_Bei\u00dfwenger1;simongerstenecker@gmail.com;~Kashyap_Chitta1;~Andreas_Geiger3",
        "aff": "Eberhard-Karls-Universit\u00e4t T\u00fcbingen;Eberhard-Karls-Universit\u00e4t T\u00fcbingen;Max-Planck-Institute for Intelligent Systems, Max-Planck Institute+Eberhard-Karls-Universit\u00e4t T\u00fcbingen;;NVIDIA+University of T\u00fcbingen;University of Tuebingen",
        "aff_domain": "uni-tuebingen.de;uni-tuebingen.de;is.mpg.de+uni-tuebingen.de;;nvidia.com+uni-tuebingen.de;uni-tuebingen.de",
        "position": "PhD student;PhD student;PhD student+MS student;;Postdoc+PhD student;Professor",
        "bibtex": "@inproceedings{\njaeger2025carl,\ntitle={Ca{RL}: Learning Scalable Planning Policies with Simple Rewards},\nauthor={Bernhard Jaeger and Daniel Dauner and Jens Bei{\\ss}wenger and Simon Gerstenecker and Kashyap Chitta and Andreas Geiger},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=1otaE496Vm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=1otaE496Vm",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1+0;2;3+4;5",
        "aff_unique_norm": "Eberhard Karls University of T\u00fcbingen;Max-Planck-Institute for Intelligent Systems;;NVIDIA;University of T\u00fcbingen;University of Tuebingen",
        "aff_unique_dep": ";Intelligent Systems;;NVIDIA Corporation;;",
        "aff_unique_url": "https://www.uni-tuebingen.de/;https://www.mpi-is.mpg.de;;https://www.nvidia.com;https://www.uni-tuebingen.de/;https://www.uni-tuebingen.de/",
        "aff_unique_abbr": "Uni T\u00fcbingen;MPI-IS;;NVIDIA;Uni T\u00fcbingen;Uni T\u00fcbingen",
        "aff_campus_unique_index": "0;0;0;",
        "aff_campus_unique": "T\u00fcbingen;",
        "aff_country_unique_index": "0;0;0+0;2+0;0",
        "aff_country_unique": "Germany;;United States"
    },
    {
        "id": "23FdMTxEh7",
        "title": "Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level controllers, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9% win rate and a 71.5% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme.",
        "keywords": "Multi-Agent;Reinforcement Learning;Self-play;Drone Volleyball",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ruize Zhang;Sirui Xiang;Zelai Xu;Feng Gao;Shilong Ji;Wenhao Tang;Wenbo Ding;Chao Yu;Yu Wang",
        "authorids": "~Ruize_Zhang3;~Sirui_Xiang1;~Zelai_Xu1;~Feng_Gao5;~Shilong_Ji1;~Wenhao_Tang2;~Wenbo_Ding1;~Chao_Yu1;~Yu_Wang3",
        "gender": "M;M;M;M;M;M;M;F;M",
        "homepage": "https://nicsefc.ee.tsinghua.edu.cn/people/RuizeZhang;https://github.com/XiangSirui;https://nicsefc.ee.tsinghua.edu.cn/people/ZelaiXu;;;https://nicsefc.ee.tsinghua.edu.cn/people/WenhaoTang;http://ssr-group.net/;http://zoeyuchao.github.io;https://nicsefc.ee.tsinghua.edu.cn",
        "dblp": ";;;;;;;36/6789-5;w/YuWang2.html",
        "google_scholar": "LtfUivUAAAAJ;;3JjcAnoAAAAJ;wzcIdLAAAAAJ;7JIdV8sAAAAJ;HM-lVugAAAAJ;xo2FkgIAAAAJ;BYoq_bwAAAAJ;https://scholar.google.com.hk/citations?user=j8JGVvoAAAAJ",
        "orcid": "0009-0007-6666-3906;;0000-0001-5578-199X;;0009-0005-0113-0485;0009-0003-4126-2269;;0000-0001-6975-0158;0000-0001-6108-5157",
        "linkedin": "ruize-zhang-0a1464360/;;;;;;;;",
        "or_profile": "~Ruize_Zhang3;~Sirui_Xiang1;~Zelai_Xu1;~Feng_Gao5;~Shilong_Ji1;~Wenhao_Tang2;~Wenbo_Ding1;~Chao_Yu1;~Yu_Wang3",
        "aff": "Tsinghua University;Tsinghua University;Tsinghua University;IIIS, Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua Univeresity;Tsinghua University;Tsinghua University",
        "aff_domain": "mail.tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;mail.tsinghua.edu.cn;mails.tsinghua.edu.cn;sz.tsinghua.edu.cn;mail.tsinghua.edu.cn;tsinghua.edu.cn",
        "position": "MS student;Undergrad student;PhD student;PhD student;Intern;MS student;Associate Professor;Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nzhang2025mastering,\ntitle={Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning},\nauthor={Ruize Zhang and Sirui Xiang and Zelai Xu and Feng Gao and Shilong Ji and Wenhao Tang and Wenbo Ding and Chao Yu and Yu Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=23FdMTxEh7}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=23FdMTxEh7",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Tsinghua University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.tsinghua.edu.cn",
        "aff_unique_abbr": "THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "2CIKnIwSta",
        "title": "Rapid Mismatch Estimation via Neural Network Informed Variational Inference",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "With robots increasingly operating in human-centric environments, ensuring soft and safe physical interactions, whether with humans, surroundings, or other machines, is essential. While compliant hardware can facilitate such interactions, this work focuses on impedance controllers that allow torque-controlled robots to safely and passively respond to contact while accurately executing tasks. From inverse dynamics to quadratic programming based controllers, the effectiveness of these methods relies on accurate dynamics models of the robot and the object it manipulates. Any model mismatch results in task failures and unsafe behaviors. Thus, we introduce Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic, probabilistic framework that estimates end-effector dynamics mismatches online, without relying on external force-torque sensors. From the robot's proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a prior for a Variational Inference solver, which rapidly converges to the unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator driven by a state-of-the-art passive impedance controller, RME adapts to sudden changes in mass and center of mass at the end-effector in $\\sim400$ ms, in static and dynamic settings. We demonstrate RME in a collaborative scenario where a human attaches an unknown basket to the robot's end-effector and dynamically adds/removes heavy items, showcasing fast and safe adaptation to changing dynamics during physical interaction without any external sensory system.",
        "keywords": "Passive Impedance Control;Learning Residual Inverse Dynamics;Model Mismatch Estimation",
        "primary_area": "",
        "supplementary_material": "/attachment/e07ba61eab56bfa68b852afda8956dbbc4c18dec.zip",
        "author": "Mateusz Jaszczuk;Nadia Figueroa",
        "authorids": "~Mateusz_Jaszczuk1;~Nadia_Figueroa1",
        "gender": "M;F",
        "homepage": ";https://nbfigueroa.github.io/",
        "dblp": ";116/8822",
        "google_scholar": "iocFSbsAAAAJ;1NQRXHQAAAAJ",
        "orcid": ";0000-0002-6873-4671",
        "linkedin": "mateusz-jaszczuk-2a216a263/;nadiabarbara/",
        "or_profile": "~Mateusz_Jaszczuk1;~Nadia_Figueroa1",
        "aff": "University of Pennsylvania;University of Pennsylvania",
        "aff_domain": "seas.upenn.edu;seas.upenn.edu",
        "position": "MS student;Assistant Professor",
        "bibtex": "@inproceedings{\njaszczuk2025rapid,\ntitle={Rapid Mismatch Estimation via Neural Network Informed Variational Inference},\nauthor={Mateusz Jaszczuk and Nadia Figueroa},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=2CIKnIwSta}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2CIKnIwSta",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Pennsylvania",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.upenn.edu",
        "aff_unique_abbr": "UPenn",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "2dXMfk3qRU",
        "title": "First Order Model-Based RL through Decoupled Backpropagation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "There is growing interest in reinforcement learning (RL) methods that leverage the simulator's derivatives to improve learning efficiency. While early gradient-based approaches have demonstrated superior performance compared to derivative-free methods, accessing simulator gradients is often impractical due to their implementation cost or unavailability. Model-based RL (MBRL) can approximate these gradients via learned dynamics models, but the solver efficiency suffers from compounding prediction errors during training rollouts, which can degrade policy performance. We propose an approach that decouples trajectory generation from gradient computation: trajectories are unrolled using a simulator, while gradients are computed via backpropagation through a learned differentiable model of the simulator. This hybrid design enables efficient and consistent first-order policy optimization, even when simulator gradients are unavailable, as well as learning a critic from simulation rollouts, which is more accurate. Our method achieves the sample efficiency and speed of specialized optimizers such as SHAC, while maintaining the generality of standard approaches like PPO and avoiding ill behaviors observed in other first-order MBRL methods. We empirically validate our algorithm on benchmark control tasks and demonstrate its effectiveness on a real Go2 quadruped robot, across both quadrupedal and bipedal locomotion tasks.",
        "keywords": "Model-Based Reinforcement Learning;Quadruped Locomotion;Sim-to-Real Transfer",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Joseph Amigo;Rooholla Khorrambakht;Elliot Chane-Sane;Nicolas Mansard;Ludovic Righetti",
        "authorids": "~Joseph_Amigo1;~Rooholla_Khorrambakht1;~Elliot_Chane-Sane1;~Nicolas_Mansard1;~Ludovic_Righetti1",
        "gender": "M;;M;Unspecified;M",
        "homepage": ";;;https://gepettoweb.laas.fr/index.php/Members/NicolasMansard;https://engineering.nyu.edu/faculty/ludovic-righetti",
        "dblp": ";;;90/5900;",
        "google_scholar": ";VdgZUjoAAAAJ;ejHZv20AAAAJ;rq-9xAkAAAAJ;LuA1j4oAAAAJ",
        "orcid": ";;;;0000-0002-6458-9112",
        "linkedin": "joseph-amigo-bb876a174;;https://fr.linkedin.com/in/elliot-chane-sane;;",
        "or_profile": "~Joseph_Amigo1;~Rooholla_Khorrambakht1;~Elliot_Chane-Sane1;~Nicolas_Mansard1;~Ludovic_Righetti1",
        "aff": "New York University;New York University;LAAS / CNRS;LAAS / CNRS;New York University+Max-Planck Institute",
        "aff_domain": "nyu.edu;nyu.edu;laas.fr;laas.fr;nyu.edu+mpg.de",
        "position": "PhD student;PhD student;Postdoc;Researcher;Associate Professor+Research Group Leader",
        "bibtex": "@inproceedings{\namigo2025first,\ntitle={First Order Model-Based {RL} through Decoupled Backpropagation},\nauthor={Joseph Amigo and Rooholla Khorrambakht and Elliot Chane-Sane and Nicolas Mansard and Ludovic Righetti},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=2dXMfk3qRU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2dXMfk3qRU",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;1;0+2",
        "aff_unique_norm": "New York University;LAAS;Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V.",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.nyu.edu;https://www.laas.fr/;https://www.mpg.de",
        "aff_unique_abbr": "NYU;LAAS;MPG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;1;0+2",
        "aff_country_unique": "United States;France;Germany"
    },
    {
        "id": "2xvxn3Hm3n",
        "title": "NeuralSVCD for Efficient Swept Volume Collision Detection",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Robot manipulation in unstructured environments requires efficient and reliable Swept Volume Collision Detection (SVCD) for safe motion planning. Traditional discrete methods potentially miss collisions between these points, whereas SVCD continuously checks for collisions along the entire trajectory. Existing SVCD methods typically face a trade-off between efficiency and accuracy, limiting practical use. In this paper, we introduce NeuralSVCD, a novel neural encoder-decoder architecture tailored to overcome this trade-off. Our approach leverages shape locality and temporal locality through distributed geometric representations and temporal optimization. This enhances computational efficiency without sacrificing accuracy. Comprehensive experiments show that NeuralSVCD consistently outperforms existing state-of-the-art SVCD methods in terms of both collision detection accuracy and computational efficiency, demonstrating its robust applicability across diverse robotic manipulation scenarios. Code and videos are available at https://neuralsvcd.github.io/.",
        "keywords": "Neural swept-volume collision detection;Motion planning",
        "primary_area": "",
        "supplementary_material": "/attachment/dba1ecb4cb05c6f9a74d297ed549563fd3c4b709.zip",
        "author": "Hojin Jung;Dongwon Son;Beomjoon Kim",
        "authorids": "~Hojin_Jung1;~Dongwon_Son1;~Beomjoon_Kim2",
        "gender": "M;M;M",
        "homepage": "https://dev.hojins.life;https://dongwon-son.github.io/;https://beomjoonkim.github.io/",
        "dblp": ";226/6343;88/1505",
        "google_scholar": ";https://scholar.google.co.kr/citations?user=oaUQsWgAAAAJ;https://scholar.google.ca/citations?user=dw3rEwgAAAAJ",
        "orcid": ";0000-0003-1446-8125;",
        "linkedin": ";;",
        "or_profile": "~Hojin_Jung1;~Dongwon_Son1;~Beomjoon_Kim2",
        "aff": "Korea Advanced Institute of Science & Technology;KAIST;Korea Advanced Institute of Science & Technology",
        "aff_domain": "kaist.edu;kaist.ac.kr;kaist.ac.kr",
        "position": "MS student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\njung2025neuralsvcd,\ntitle={Neural{SVCD} for Efficient Swept Volume Collision Detection},\nauthor={Hojin Jung and Dongwon Son and Beomjoon Kim},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=2xvxn3Hm3n}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2xvxn3Hm3n",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "2y7TSgwqAB",
        "title": "GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We focus on the task of identifying the location of target regions from a natural language instruction and a front camera image captured by a mobility.\nThis task is challenging because it requires both existence prediction and segmentation mask generation, particularly for stuff-type target regions with ambiguous boundaries.\nExisting methods often underperform in handling stuff-type target regions, in addition to absent or multiple targets.\nTo overcome these limitations, we propose GENNAV, which predicts target existence and generates segmentation masks for multiple stuff-type target regions. \nTo evaluate GENNAV, we constructed a novel benchmark called GRiN-Drive, which includes three distinct types of samples: no-target, single-target, and multi-target.\nGENNAV achieved superior performance over baseline methods on standard evaluation metrics.\nFurthermore, we conducted real-world experiments with four automobiles operated in five geographically distinct urban areas to validate its zero-shot transfer performance.\nIn these experiments, GENNAV outperformed baseline methods and demonstrated its robustness across diverse real-world environments.",
        "keywords": "Autonomous driving;Vision and Language;Semantic Understanding",
        "primary_area": "",
        "supplementary_material": "/attachment/2cc982cf010a4e5781cc4033589e3844d57cbd56.zip",
        "author": "Kei Katsumata;Yui Iioka;Naoki Hosomi;Teruhisa Misu;Kentaro Yamada;Komei Sugiura",
        "authorids": "~Kei_Katsumata1;~Yui_Iioka1;~Naoki_Hosomi1;~Teruhisa_Misu2;~Kentaro_Yamada1;~Komei_Sugiura1",
        "gender": "M;M;;M;M;M",
        "homepage": ";;;;;https://komeisugiura.jp/index_en.html",
        "dblp": "398/2841;;;https://dblp.org/pers/m/Misu:Teruhisa.html;;77/2654",
        "google_scholar": "https://scholar.google.co.jp/citations?user=TgNh2GUAAAAJ;;;4LAT5WYAAAAJ;;1Kd0W0oAAAAJ",
        "orcid": "0009-0009-7078-457X;;0009-0002-6119-418X;;0009-0003-8202-7417;0000-0002-0261-0510",
        "linkedin": ";yui-iioka-b99b26278?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app;;;;",
        "or_profile": "~Kei_Katsumata1;~Yui_Iioka1;~Naoki_Hosomi1;~Teruhisa_Misu2;~Kentaro_Yamada1;~Komei_Sugiura1",
        "aff": "Keio University;Keio University, Tokyo Institute of Technology;Honda R&D Co., Ltd.;Honda Research Institute USA, Inc.;Honda;Keio University",
        "aff_domain": "keio.jp;keio.jp;jp.honda;honda-ri.com;jp.honda;keio.jp",
        "position": "Undergrad student;MS student;\u3000;Scientist;Chief Engineer;Full Professor",
        "bibtex": "@inproceedings{\nkatsumata2025gennav,\ntitle={{GENNAV}: Polygon Mask Generation for Generalized Referring Navigable Regions},\nauthor={Kei Katsumata and Yui Iioka and Naoki Hosomi and Teruhisa Misu and Kentaro Yamada and Komei Sugiura},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=2y7TSgwqAB}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=2y7TSgwqAB",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;2;3;0",
        "aff_unique_norm": "Keio University;Honda R&D Co., Ltd.;Honda Research Institute USA;Honda",
        "aff_unique_dep": ";;Research Institute;",
        "aff_unique_url": "https://www.keio.ac.jp;https://www.honda.com/;https://honda-ri.com;",
        "aff_unique_abbr": "Keio;Honda R&D;HRI USA;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Tokyo",
        "aff_country_unique_index": "0;0;0;1;0",
        "aff_country_unique": "Japan;United States;"
    },
    {
        "id": "3CnxNqmklv",
        "title": "DreamGen: Unlocking Generalization in Robot Learning through Video World Models",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In this work, we unlock new capabilities in robot learning from neural trajectories, synthetic robot data generated from video world models. Our proposed recipe is simple, but powerful: we take the most recent state-of-the-art video generative models (world models), adapt them to the target robot embodiment, and generate new, synthetic robot data of the same task or even new behaviors. Since these video world models only generate videos, we explore two techniques of getting robot actions: extracting latent actions from a general-purpose latent action model and getting predicted actions from an inverse-dynamics model (IDM), giving flexibility across diverse scenarios. Our proposed approach unlocks behavior and environment generalization, allowing a humanoid robot to perform 20+ new behaviors in unseen environments while only collecting teleoperation data for pick and place in a single environment. By introducing a new world modeling benchmark, we demonstrate that stronger video world models directly correlate with improved downstream robot policy performance. This establishes a new scaling dimension beyond simply collecting additional teleoperation data, changing how we approach robot learning.",
        "keywords": "Video World Models;Synthetic Data;Behavior Generalization;Environment Generalization",
        "primary_area": "",
        "supplementary_material": "/attachment/2682e8bf8a23c00c76e45c4c6d7f34f80cb0163f.zip",
        "author": "Joel Jang;Seonghyeon Ye;Zongyu Lin;Jiannan Xiang;Johan Bjorck;Yu Fang;Fengyuan Hu;Spencer Huang;Kaushil Kundalia;Yen-Chen Lin;Lo\u00efc Magne;Ajay Mandlekar;Avnish Narayan;You Liang Tan;Guanzhi Wang;Jing Wang;Qi Wang;Yinzhen Xu;Xiaohui Zeng;Kaiyuan Zheng;Ruijie Zheng;Ming-Yu Liu;Luke Zettlemoyer;Dieter Fox;Jan Kautz;Scott Reed;Yuke Zhu;Linxi Fan",
        "authorids": "~Joel_Jang1;~Seonghyeon_Ye1;~Zongyu_Lin1;~Jiannan_Xiang1;~Johan_Bjorck2;~Yu_Fang2;~Fengyuan_Hu2;spencerh@nvidia.com;~Kaushil_Kundalia1;~Yen-Chen_Lin1;~Lo\u00efc_Magne1;~Ajay_Mandlekar1;avnishn@nvidia.com;~You_Liang_Tan1;~Guanzhi_Wang1;~Jing_Wang79;qiwang@nvidia.com;~Yinzhen_Xu1;~Xiaohui_Zeng2;~Kaiyuan_Zheng1;~Ruijie_Zheng1;~Ming-Yu_Liu1;~Luke_Zettlemoyer1;~Dieter_Fox1;~Jan_Kautz1;~Scott_Reed1;~Yuke_Zhu1;~Linxi_Fan2",
        "gender": "M;M;M;M;M;M;;;;M;M;M;;;M;M;;M;;M;;M;M;M;;;M;",
        "homepage": "https://joeljang.github.io/;https://vano1205.github.io/;;https://szxiangjn.github.io/;https://nilsjohanbjorck.github.io/;http://squarefk.com/;;;;http://yenchenlin.me/;https://github.com/loicmagne;https://ai.stanford.edu/~amandlek/;;https://youliangtan.github.io/;https://www.guanzhi.me/;;;https://xyz-99.github.io;;;http://www.ruijiezheng.com;http://mingyuliu.net;https://www.cs.washington.edu/people/faculty/lsz/;https://homes.cs.washington.edu/~fox/;http://jankautz.com;https://scottreed.info;https://yukezhu.me/;",
        "dblp": ";301/8927;273/7646;230/3430;188/6399;;;;;180/0954;;https://dblp.uni-trier.de/pers/hd/m/Mandlekar:Ajay;;;239/8731;;;327/1997;;;294/8474;17/8368-1;21/6793;f/DieterFox;48/6214;;133/1772;154/6778",
        "google_scholar": "xL-7eFEAAAAJ;https://scholar.google.co.kr/citations?user=JfGGjBoAAAAJ;4ahRAd4AAAAJ;l8BS2wsAAAAJ;https://scholar.google.com/citations?hl=en;;;;;RbCKRPcAAAAJ;;MEz23joAAAAJ;;;QDmEj4MAAAAJ;cdL5PqgAAAAJ;;VaFCcJ8AAAAJ;;;;y-f-MZgAAAAJ;https://scholar.google.com.tw/citations?user=UjpbO6IAAAAJ;DqXsbPAAAAAJ;P9FclNEAAAAJ;jEANvfgAAAAJ;mWGyYMsAAAAJ;sljtWIUAAAAJ",
        "orcid": ";;;;;;;;;;;;;;;;;;;;;0000-0002-2951-2398;;;;;;",
        "linkedin": "joel-jang-1289331a5/;;;;;;;;kaushilk?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app;;;;;youliang-tan/;;jing-wang-457a67153/;;yinzhen-xu-620615190/;;https://linkedin.com/in/kaiyuan-zheng-94984a236;;mingyuliu/;luke-zettlemoyer-a0109b226/;;;;;",
        "or_profile": "~Joel_Jang1;~Seonghyeon_Ye1;~Zongyu_Lin1;~Jiannan_Xiang1;~Johan_Bjorck2;~Yu_Fang2;~Fengyuan_Hu2;spencerh@nvidia.com;~Kaushil_Kundalia1;~Yen-Chen_Lin1;~Lo\u00efc_Magne1;~Ajay_Mandlekar1;avnishn@nvidia.com;~You_Liang_Tan1;~Guanzhi_Wang1;~Jing_Wang79;qiwang@nvidia.com;~Yinzhen_Xu1;~Xiaohui_Zeng2;~Kaiyuan_Zheng1;~Ruijie_Zheng1;~Ming-Yu_Liu1;~Luke_Zettlemoyer1;~Dieter_Fox1;~Jan_Kautz1;~Scott_Reed1;~Yuke_Zhu1;~Linxi_Fan2",
        "aff": "Department of Computer Science, University of Washington;Korea Advanced Institute of Science & Technology;University of California, Los Angeles;University of California, San Diego;Microsoft;;;;NVIDIA;Massachusetts Institute of Technology;;NVIDIA;;NVIDIA;California Institute of Technology;Nanyang Technological University;;NVIDIA;;University of Washington+University of Washington;University of Maryland, College Park;NVIDIA;University of Washington+Meta Facebook+Meta;NVIDIA Research+Department of Computer Science;NVIDIA;NVIDIA;Computer Science Department, University of Texas, Austin;NVIDIA",
        "aff_domain": "cs.washington.edu;kaist.ac.kr;cs.ucla.edu;ucsd.edu;microsoft.com;;;;nvidia.com;mit.edu;;nvidia.com;;nvidia.com;caltech.edu;ntu.edu.sg;;nvidia.com;;uw.edu+uw.edu;cs.umd.edu;nvidia.com;cs.washington.edu+fb.com+meta.com;nvidia.com+cs.washington.edu;nvidia.com;nvidia.com;cs.utexas.edu;nvidia.com",
        "position": "PhD student;PhD student;PhD student;PhD student;Researcher;;;;Research Engineer;PhD student;;Researcher;;Researcher;PhD student;PhD student;;Researcher;;PhD student+MS student;PhD student;Researcher;Full Professor+Researcher+Researcher;Senior Director of Robotics Research+Full Professor;VP Research;Principal Researcher;Associate Professor;Researcher",
        "bibtex": "@inproceedings{\njang2025dreamgen,\ntitle={DreamGen: Unlocking Generalization in Robot Learning through Video World Models},\nauthor={Joel Jang and Seonghyeon Ye and Zongyu Lin and Jiannan Xiang and Johan Bjorck and Yu Fang and Fengyuan Hu and Spencer Huang and Kaushil Kundalia and Yen-Chen Lin and Lo{\\\"\\i}c Magne and Ajay Mandlekar and Avnish Narayan and You Liang Tan and Guanzhi Wang and Jing Wang and Qi Wang and Yinzhen Xu and Xiaohui Zeng and Kaiyuan Zheng and Ruijie Zheng and Ming-Yu Liu and Luke Zettlemoyer and Dieter Fox and Jan Kautz and Scott Reed and Yuke Zhu and Linxi Fan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=3CnxNqmklv}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=3CnxNqmklv",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            28,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3;4;5;5;5;6;7;5;6;5;6;8;9;5;6;5;0+0;10;6;0+11+11;6+12;6;6;13;6",
        "aff_unique_norm": "University of Washington;Korea Advanced Institute of Science and Technology;University of California, Los Angeles;University of California, San Diego;Microsoft;;NVIDIA;Massachusetts Institute of Technology;California Institute of Technology;Nanyang Technological University;University of Maryland;Meta;Unknown Institution;University of Texas at Austin",
        "aff_unique_dep": "Department of Computer Science;;;;Microsoft Corporation;;NVIDIA Corporation;;;;;Meta Platforms, Inc.;Department of Computer Science;Computer Science Department",
        "aff_unique_url": "https://www.washington.edu;https://www.kaist.ac.kr;https://www.ucla.edu;https://www.ucsd.edu;https://www.microsoft.com;;https://www.nvidia.com;https://web.mit.edu;https://www.caltech.edu;https://www.ntu.edu.sg;https://www/umd.edu;https://meta.com;;https://www.utexas.edu",
        "aff_unique_abbr": "UW;KAIST;UCLA;UCSD;Microsoft;;NVIDIA;MIT;Caltech;NTU;UMD;Meta;;UT Austin",
        "aff_campus_unique_index": "0;2;3;4;;5;;;6",
        "aff_campus_unique": "Seattle;;Los Angeles;San Diego;Pasadena;College Park;Austin",
        "aff_country_unique_index": "0;1;0;0;0;0;0;0;0;0;3;0;0+0;0;0;0+0+0;0;0;0;0;0",
        "aff_country_unique": "United States;South Korea;;Singapore"
    },
    {
        "id": "3p7rTnLJM8",
        "title": "Lucid-XR: An Extended-Reality Data Engine for Robotic Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We introduce Lucid-XR, a generative data engine for creating diverse and realistic-looking data to train real-world robot systems. At the core of Lucid-XR is vuer, a web-based physics simulation environment that runs directly on the XR headset, enabling internet-scale access to immersive, latency-free virtual interactions without requiring specialized equipment. The complete system integrates on-device physics simulation with on-device human-to-robot pose retargeting, that are further amplified by a physics-guided video generation pipeline commandable with natural language specifications.\u00a0We demonstrate zero-shot sim-to-real transfer of robot visual policies, trained entirely on Lucid-XR's synthetic data, across bimanual and dexterous manipulation tasks that involve flexible materials, adhesive interaction between particles, and rigid body contact.",
        "keywords": "mixed reality;extended reality;robot manipulation;simulation;mujoco",
        "primary_area": "",
        "supplementary_material": "/attachment/cebefde973292eb1d726628514efbb0f91e0d5ce.zip",
        "author": "Yajvan Ravan;Adam Rashid;Alan Yu;Kai McClennen;Gio Huh;Kevin Yang;Zhutian Yang;Qinxi Yu;Xiaolong Wang;Phillip Isola;Ge Yang",
        "authorids": "~Yajvan_Ravan1;~Adam_Rashid1;~Alan_Yu2;~Kai_McClennen1;~Gio_Huh1;~Kevin_Yang3;~Zhutian_Yang1;~Qinxi_Yu1;~Xiaolong_Wang3;~Phillip_Isola1;~Ge_Yang1",
        "gender": ";M;M;M;M;M;F;M;M;M;M",
        "homepage": "https://www.linkedin.com/in/yajvan-ravan/;;https://alany1.github.io;;https://github.com/geo-179;http://kvnyng.com;https://zt-yang.com;https://quincy-u.github.io/;https://xiaolonw.github.io/;http://web.mit.edu/phillipi/;http://www.episodeyang.com",
        "dblp": ";;;;;;;;91/952-4;36/9988;48/4561-3",
        "google_scholar": ";;https://scholar.google.com/citations?hl=en;;;;vW5LLmUAAAAJ;3k7hNDIAAAAJ;Y8O9N_0AAAAJ;ROILf3EAAAAJ;vaQcF6kAAAAJ",
        "orcid": ";;;;;;;;;0000-0002-1411-6704;0000-0001-7520-7055",
        "linkedin": "yajvan-ravan/;adam-rashid-83a94a1b6/;;kai-mcclennen-130b20252/;;;zhutian-yang/;qinxi-yu/;;phillip-isola-a9955b20/;",
        "or_profile": "~Yajvan_Ravan1;~Adam_Rashid1;~Alan_Yu2;~Kai_McClennen1;~Gio_Huh1;~Kevin_Yang3;~Zhutian_Yang1;~Qinxi_Yu1;~Xiaolong_Wang3;~Phillip_Isola1;~Ge_Yang1",
        "aff": "Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;California Institute of Technology;;Google;University of Illinois, Urbana Champaign;University of California, San Diego;Massachusetts Institute of Technology;Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;mit.edu;mit.edu;caltech.edu;;google.com;illinois.edu;ucsd.edu;mit.edu;mit.edu",
        "position": "Undergrad student;PhD student;MS student;Undergrad student;Undergrad student;;Researcher;PhD student;Assistant Professor;Associate Professor;Postdoc",
        "bibtex": "@inproceedings{\nravan2025lucidxr,\ntitle={Lucid-{XR}: An Extended-Reality Data Engine for Robotic Manipulation},\nauthor={Yajvan Ravan and Adam Rashid and Alan Yu and Kai McClennen and Gio Huh and Kevin Yang and Zhutian Yang and Qinxi Yu and Xiaolong Wang and Phillip Isola and Ge Yang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=3p7rTnLJM8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=3p7rTnLJM8",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;1;2;3;4;5;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;California Institute of Technology;;Google;University of Illinois Urbana-Champaign;University of California, San Diego",
        "aff_unique_dep": ";;;Google;;",
        "aff_unique_url": "https://web.mit.edu;https://www.caltech.edu;;https://www.google.com;https://illinois.edu;https://www.ucsd.edu",
        "aff_unique_abbr": "MIT;Caltech;;Google;UIUC;UCSD",
        "aff_campus_unique_index": "1;2;3;4",
        "aff_campus_unique": ";Pasadena;Mountain View;Urbana-Champaign;San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "4IuTfpWGDR",
        "title": "TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Robots often struggle to generalize from a single demonstration due to the lack of a transferable and interpretable spatial representation. In this work, we introduce TReF-6, a method that infers a simplified, abstracted 6DoF Task-Relevant Frame from a single trajectory. Our approach identifies an influence point purely from the trajectory geometry to define the origin for a local frame, which serves as a reference for parameterizing a Dynamic Movement Primitive (DMP). This influence point captures the task's spatial structure, extending the standard DMP formulation beyond start-goal imitation. The inferred frame is semantically grounded via a vision-language model and localized in novel scenes by Grounded-SAM, enabling functionally consistent skill generalization. We validate TReF-6 in simulation and demonstrate robustness to trajectory noise. We further deploy an end-to-end pipeline on real-world manipulation tasks, showing that TReF-6 supports one-shot imitation learning that preserves task intent across diverse object configurations.",
        "keywords": "Spatial Reference Frames;One-Shot Imitation Learning;Dynamic Movement Primitives",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yuxuan Ding;Shuangge Wang;Tesca Fitzgerald",
        "authorids": "~Yuxuan_Ding2;~Shuangge_Wang1;~Tesca_Fitzgerald1",
        "gender": "M;Not Specified;F",
        "homepage": ";;http://www.tescafitzgerald.com",
        "dblp": ";;159/0410",
        "google_scholar": ";;UTmj6K4AAAAJ",
        "orcid": ";;0000-0003-0867-0546",
        "linkedin": "eason-d-a23a302a9;shuanggewang/;",
        "or_profile": "~Yuxuan_Ding2;~Shuangge_Wang1;~Tesca_Fitzgerald1",
        "aff": ";Yale University;Yale University",
        "aff_domain": ";yale.edu;yale.edu",
        "position": ";PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nding2025tref,\ntitle={{TR}eF-6: Inferring Task-Relevant Frames from a Single Demonstration for One-Shot Skill Generalization},\nauthor={Yuxuan Ding and Shuangge Wang and Tesca Fitzgerald},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=4IuTfpWGDR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=4IuTfpWGDR",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1",
        "aff_unique_norm": ";Yale University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.yale.edu",
        "aff_unique_abbr": ";Yale",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "4Po2mqLjrQ",
        "title": "Motion Blender Gaussian Splatting for Dynamic Reconstruction",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes.  However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application. To address this, we propose Motion Blender Gaussian Splatting (MB-GS), a novel framework that uses motion graph as an explicit and sparse motion representation. The motion of graph links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions determining the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MB-GS achieves state-of-the-art performance on the iPhone dataset while being competitive on HyperNeRF. Additionally, we demonstrate the application potential of our method in animating novel object motions, synthesizing robot demonstrations through motion editing, and predicting robot actions through visual planning.",
        "keywords": "Dynamic Reconstruction;Gaussian Splatting",
        "primary_area": "",
        "supplementary_material": "/attachment/8c3c0ded5408f78b5cb29d881200068b4bb38af5.zip",
        "author": "Xinyu Zhang;Haonan Chang;Yuhan Liu;Abdeslam Boularias",
        "authorids": "~Xinyu_Zhang7;~Haonan_Chang1;~Yuhan_Liu2;~Abdeslam_Boularias1",
        "gender": "M;M;M;M",
        "homepage": "https://mlzxy.github.io/;https://github.com/changhaonan;;http://rl.cs.rutgers.edu/",
        "dblp": ";;125/8141;57/2269",
        "google_scholar": "M7hnG9oAAAAJ;;https://scholar.google.com/citations?hl=en;https://scholar.google.com.tw/citations?user=8AF3RCsAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Xinyu_Zhang7;~Haonan_Chang1;~Yuhan_Liu2;~Abdeslam_Boularias1",
        "aff": "Rutgers University;;Rutgers University;, Rutgers University",
        "aff_domain": "rutgers.edu;;rutgers.edu;cs.rutgers.edu",
        "position": "PhD student;;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nzhang2025motion,\ntitle={Motion Blender Gaussian Splatting for Dynamic Reconstruction},\nauthor={Xinyu Zhang and Haonan Chang and Yuhan Liu and Abdeslam Boularias},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=4Po2mqLjrQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=4Po2mqLjrQ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "Rutgers University;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.rutgers.edu;",
        "aff_unique_abbr": "Rutgers;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "4XKKUifQ9c",
        "title": "ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Dexterous grasping in cluttered scenes presents significant challenges due to diverse object geometries, occlusions, and potential collisions. Existing methods primarily focus on single-object grasping or grasp-pose prediction without interaction, which are insufficient for complex, cluttered scenes. Recent vision-language-action models offer a potential solution but require extensive real-world demonstrations, making them costly and difficult to scale. To address these limitations, we revisit the sim-to-real transfer pipeline and develop key techniques that enable zero-shot deployment in reality while maintaining robust generalization. We propose ClutterDexGrasp, a two-stage teacher-student framework for closed-loop target-oriented dexterous grasping in cluttered scenes. The framework features a teacher policy trained in simulation using clutter density curriculum learning, incorporating both a novel geometry- and spatially-embedded scene representation and a comprehensive safety curriculum, enabling general, dynamic, and safe grasping behaviors. Through imitation learning, we distill the teacher's knowledge into a student 3D diffusion policy (DP3) that operates on partial point cloud observations. To the best of our knowledge, this represents the first zero-shot sim-to-real closed-loop system for target oriented dexterous grasping in cluttered scenes, demonstrating robust performance across diverse objects and layouts.",
        "keywords": "Cluttered Scene;Dexterous Grasping;Sim-to-Real",
        "primary_area": "",
        "supplementary_material": "/attachment/a2a8303da3ce60838859bf4d3d853a66c93fb6b6.zip",
        "author": "Zeyuan Chen;Qiyang Yan;Yuanpei Chen;Tianhao Wu;Jiyao Zhang;Zihan Ding;Jinzhou Li;Yaodong Yang;Hao Dong",
        "authorids": "~Zeyuan_Chen10;~Qiyang_Yan1;~Yuanpei_Chen2;~Tianhao_Wu2;~Jiyao_Zhang1;~Zihan_Ding1;~Jinzhou_Li2;~Yaodong_Yang1;~Hao_Dong3",
        "gender": "M;M;M;M;M;M;M;;M",
        "homepage": "https://chenzyn.github.io/;https://qiyangyan.github.io/web/;https://cypypccpy.github.io/;https://tianhaowuhz.github.io/;https://jiyao06.github.io;https://quantumiracle.github.io/webpage/;https://kingchou007.github.io/;;https://zsdonghao.github.io",
        "dblp": ";;1234567;17/1976-1;;;;;14/1525-3.html",
        "google_scholar": ";;https://scholar.google.com/citations?hl=en;eAW0tjMAAAAJ;nf1Q7P4AAAAJ;t5DgPBAAAAAJ;s93qhQ8AAAAJ;;xLFL4sMAAAAJ",
        "orcid": ";;0000-0002-0033-492X;;;;0000-0001-6911-9446;;0000-0003-2261-9122",
        "linkedin": ";;;;;;jinzhou-l-78001212a/;;",
        "or_profile": "~Zeyuan_Chen10;~Qiyang_Yan1;~Yuanpei_Chen2;~Tianhao_Wu2;~Jiyao_Zhang1;~Zihan_Ding1;~Jinzhou_Li2;~Yaodong_Yang1;~Hao_Dong3",
        "aff": "AgiBot+Peking University;AgiBot;PsiRobot;Peking University;Peking University;Princeton University;Duke University+Peking University;;Peking University+Peking University",
        "aff_domain": "agibot.com+stu.pku.edu.cn;agibot.com;psibot.ai;pku.edu.cn;pku.edu.cn;princeton.edu;duke.edu+pku.edu.cn;;pku.edu.cn+pku.edu.cn",
        "position": "Intern+MS student;Intern;Researcher;PhD student;PhD student;PhD student;PhD student+Research Intern;;Associate Professor+Assistant Professor",
        "bibtex": "@inproceedings{\nchen2025clutterdexgrasp,\ntitle={ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes},\nauthor={Zeyuan Chen and Qiyang Yan and Yuanpei Chen and Tianhao Wu and Jiyao Zhang and Zihan Ding and Jinzhou Li and Yaodong Yang and Hao Dong},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=4XKKUifQ9c}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=4XKKUifQ9c",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;0;2;1;1;3;4+1;5;1+1",
        "aff_unique_norm": "AgiBot;Peking University;PsiRobot;Princeton University;Duke University;",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": ";http://www.pku.edu.cn;;https://www.princeton.edu;https://www.duke.edu;",
        "aff_unique_abbr": ";Peking U;;Princeton;Duke;",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1;2;2+1;1+1",
        "aff_country_unique": ";China;United States"
    },
    {
        "id": "4eMWCoWUKR",
        "title": "Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "As robots become increasingly capable of operating over extended periods\u2014spanning days, weeks, and even months\u2014they are expected to accumulate knowledge of their environments and leverage this experience to assist humans more effectively. This paper studies the problem of Long-term Active Embodied Question Answering (LA-EQA), a new task in which a robot must both recall past experiences and actively explore its environment to answer complex, temporally-grounded questions. Unlike traditional EQA settings, which typically focus either on understanding the present environment alone or on recalling a single past observation, LA-EQA challenges an agent to reason over past, present, and possible future states, deciding when to explore, when to consult its memory, and when to stop gathering observations and provide a final answer. Standard EQA approaches based on large models struggle in this setting due to limited context windows, absence of persistent memory, and an inability to combine memory recall with active exploration. To address this, we propose a structured memory system for robots, inspired by the mind palace method from cognitive science. Our method encodes episodic experiences as scene-graph-based world instances, forming a reasoning and planning algorithm that enables targeted memory retrieval and guided navigation. To balance the exploration-recall trade-off, we introduce value-of-information-based stopping criteria that determines when the agent has gathered sufficient information. We evaluate our method on real-world experiments and introduce a new benchmark that spans popular simulation environments and actual industrial sites. Our approach significantly outperforms state-of-the-art baselines, yielding substantial gains in both answer accuracy and exploration efficiency.",
        "keywords": "embodied question answering;long-term reasoning;vision-language navigation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Muhammad Fadhil Ginting;Dong-Ki Kim;Xiangyun Meng;Andrzej Marek Reinke;Bandi Jai Krishna;Navid Kayhani;Oriana Peltzer;David Fan;Amirreza Shaban;Sung-Kyun Kim;Mykel Kochenderfer;Ali-akbar Agha-mohammadi;Shayegan Omidshafiei",
        "authorids": "~Muhammad_Fadhil_Ginting1;~Dong-Ki_Kim1;~Xiangyun_Meng1;~Andrzej_Marek_Reinke1;jai@fieldai.com;~Navid_Kayhani1;oriana@fieldai.com;~David_Fan1;~Amirreza_Shaban1;skysk7@gmail.com;~Mykel_Kochenderfer1;~Ali-akbar_Agha-mohammadi1;~Shayegan_Omidshafiei1",
        "gender": "M;;;;;M;;;Unspecified;;M;;",
        "homepage": "https://mfadhilgtg.github.io;https://dkkim93.github.io/;https://homes.cs.washington.edu/~xiangyun;;;;;https://scholar.google.com/citations?user=vbhA9hwAAAAJ&hl=en;;;https://mykel.kochenderfer.com;;",
        "dblp": ";199/2089;169/3352;;;;;;99/9987;;34/2029.html;;153/7735",
        "google_scholar": "RTycc8AAAAAJ;https://scholar.google.com/citations?hl=en;;;;https://scholar.google.ca/citations?user=U6wgFwwAAAAJ;;;6Q6TCkkAAAAJ;;cAy9G6oAAAAJ;;nm5wMNUAAAAJ",
        "orcid": ";;;;;0000-0001-8139-7254;;;;;0000-0002-7238-9663;;",
        "linkedin": ";;;femust/;;navid-kayhani/?originalSubdomain=ca;;;;;mykel-kochenderfer;;",
        "or_profile": "~Muhammad_Fadhil_Ginting1;~Dong-Ki_Kim1;~Xiangyun_Meng1;~Andrzej_Marek_Reinke1;jai@fieldai.com;~Navid_Kayhani1;oriana@fieldai.com;~David_Fan1;~Amirreza_Shaban1;skysk7@gmail.com;~Mykel_Kochenderfer1;~Ali-akbar_Agha-mohammadi1;~Shayegan_Omidshafiei1",
        "aff": "Stanford University;Field AI;University of Washington;;;Field AI;;Jet Propulsion Laboratory;University of Washington, Seattle;;Stanford University;;FieldAI",
        "aff_domain": "stanford.edu;fieldai.com;washington.edu;;;fieldai.com;;jpl.nasa.gov;uw.edu;;stanford.edu;;fieldai.com",
        "position": "PhD student;Staff Research Scientist;PhD student;;;Researcher;;Researcher;Postdoc;;Associate Professor;;Researcher",
        "bibtex": "@inproceedings{\nginting2025enter,\ntitle={Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering},\nauthor={Muhammad Fadhil Ginting and Dong-Ki Kim and Xiangyun Meng and Andrzej Marek Reinke and Bandi Jai Krishna and Navid Kayhani and Oriana Peltzer and David Fan and Amirreza Shaban and Sung-Kyun Kim and Mykel Kochenderfer and Ali-akbar Agha-mohammadi and Shayegan Omidshafiei},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=4eMWCoWUKR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=4eMWCoWUKR",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            13,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3;3;1;3;4;2;3;0;3;5",
        "aff_unique_norm": "Stanford University;Field AI;University of Washington;;Jet Propulsion Laboratory;FieldAI",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.stanford.edu;https://www.field.ai;https://www.washington.edu;;https://www.jpl.nasa.gov;",
        "aff_unique_abbr": "Stanford;Field AI;UW;;JPL;",
        "aff_campus_unique_index": "0;2;3;0",
        "aff_campus_unique": "Stanford;;Pasadena;Seattle",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "4eSv0QeYlz",
        "title": "Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Generating collision-free motion in dynamic, partially observable environments is a fundamental challenge for robotic manipulators. Classical motion planners can compute globally optimal trajectories but require full environment knowledge and are typically too slow for dynamic scenes. Neural motion policies offer a promising alternative by operating in closed-loop directly on raw sensory inputs but often struggle to generalize in complex or dynamic settings.\n\nWe propose Deep Reactive Policy (DRP), a visuo-motor neural motion policy designed for reactive motion generation in diverse dynamic environments, operating directly on point cloud sensory input. At its core is IMPACT, a transformer-based neural motion policy pretrained on 10 million generated expert trajectories across diverse simulation scenarios. We further improve IMPACT\u2019s static obstacle avoidance through iterative student-teacher finetuning. We additionally enhance the policy's dynamic obstacle avoidance at inference time using DCP-RMP, a locally reactive goal-proposal module.\n\nWe evaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving obstacles, and goal obstructions. DRP achieves strong generalization, outperforming prior classical and neural methods in success rate across both simulated and real-world settings. We will release the dataset, simulation environments, and trained models upon acceptance. Refer to supplementary material for videos.",
        "keywords": "Motion Planning;Visuo-Motor Policy;Reactive Control",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jiahui Yang;Jason Jingzhou Liu;Yulong Li;Youssef Khaky;Kenneth Shaw;Deepak Pathak",
        "authorids": "~Jiahui_Yang3;~Jason_Jingzhou_Liu1;~Yulong_Li1;~Youssef_Khaky1;~Kenneth_Shaw1;~Deepak_Pathak1",
        "gender": "M;;M;M;M;M",
        "homepage": "https://jim-young6709.github.io/;;https://yulongli42.github.io;;https://www.linkedin.com/in/kenny-shaw/;https://www.cs.cmu.edu/~dpathak/",
        "dblp": ";;;;;155/9860",
        "google_scholar": "4w2l0uEAAAAJ;;;;;https://scholar.google.cl/citations?user=AEsPCAUAAAAJ",
        "orcid": ";;;;;",
        "linkedin": ";;;youssefkhaky/;kenny-shaw/;pathak22/",
        "or_profile": "~Jiahui_Yang3;~Jason_Jingzhou_Liu1;~Yulong_Li1;~Youssef_Khaky1;~Kenneth_Shaw1;~Deepak_Pathak1",
        "aff": "Carnegie Mellon University;;Massachusetts Institute of Technology;Carnegie Mellon University;Carnegie Mellon University;Skild AI+Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;;mit.edu;cmu.edu;cmu.edu;skild.ai+cmu.edu",
        "position": "MS student;;PhD student;Robotics Engineer;PhD student;Founder+Assistant Professor",
        "bibtex": "@inproceedings{\nyang2025deep,\ntitle={Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments},\nauthor={Jiahui Yang and Jason Jingzhou Liu and Yulong Li and Youssef Khaky and Kenneth Shaw and Deepak Pathak},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=4eSv0QeYlz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=4eSv0QeYlz",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;0;0;3+0",
        "aff_unique_norm": "Carnegie Mellon University;;Massachusetts Institute of Technology;Skild AI",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.cmu.edu;;https://web.mit.edu;",
        "aff_unique_abbr": "CMU;;MIT;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "5htQM8jqOe",
        "title": "D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Mastering deformable object manipulation often necessitates the use of anthropomorphic, high-degree-of-freedom robot hands capable of precise, contact-rich control. However, current trajectory optimisation methods often struggle in these settings due to the large search space and the sparse task information available from shape-matching cost functions, particularly when contact is absent. In this work, we propose D-Cubed, a novel trajectory optimisation method using a latent diffusion model (LDM) trained from a task-agnostic play dataset to solve dexterous deformable object manipulation tasks. D-Cubed learns a skill-latent space that encodes short-horizon actions from a play dataset using a VAE and trains a LDM to compose the skill latents into a skill trajectory, representing a long-horizon action trajectory. To optimise a trajectory for a target task, we introduce a novel gradient-free guided sampling method that employs the Cross-Entropy method within the reverse diffusion process. In particular, D-Cubed samples a small number of noisy skill trajectories using the LDM for exploration and evaluates the trajectories in simulation. Then D-Cubed selects the trajectory with the lowest cost for the subsequent reverse process. This effectively explores promising solution areas and optimises the sampled trajectories towards a target task throughout the reverse diffusion process. Through empirical evaluation on a published benchmark of dexterous deformable object manipulation tasks, we demonstrate that D-Cubed outperforms traditional trajectory optimisation and competitive baseline approaches by a significant margin.",
        "keywords": "Trajectory Optimisation;Dexterous Deformable Object Manipulation;Latent Diffusion Model;Gradient-Free Guidance",
        "primary_area": "",
        "supplementary_material": "/attachment/9ea2b5298e26bd3ea4abbbbc464a87d7aef35d82.zip",
        "author": "Jun Yamada;Shaohong Zhong;Jack Collins;Ingmar Posner",
        "authorids": "~Jun_Yamada1;~Shaohong_Zhong1;~Jack_Collins2;~Ingmar_Posner1",
        "gender": "M;;M;",
        "homepage": "http://junjungoal.github.io;https://www.linkedin.com/in/shaohong-z-982435a2/;https://jacktcollins.com/;",
        "dblp": ";;222/3913;59/542",
        "google_scholar": "ESeyBEEAAAAJ;;https://scholar.google.com/citations?hl=en;dPk-iwsAAAAJ",
        "orcid": ";;0000-0002-5970-1624;0000-0001-6270-700X",
        "linkedin": ";;jacktcollins/;ingmar-posner-20b49a",
        "or_profile": "~Jun_Yamada1;~Shaohong_Zhong1;~Jack_Collins2;~Ingmar_Posner1",
        "aff": "University of Oxford;University of Oxford;University of Oxford;Amazon+University of Oxford",
        "aff_domain": "ox.ac.uk;ox.ac.uk;oxford.ac.uk;amazon.com+ox.ac.uk",
        "position": "PhD student;PhD student;Postdoc;Principal Researcher+Full Professor",
        "bibtex": "@inproceedings{\nyamada2025dcubed,\ntitle={D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous Deformable Manipulation},\nauthor={Jun Yamada and Shaohong Zhong and Jack Collins and Ingmar Posner},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=5htQM8jqOe}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=5htQM8jqOe",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1+0",
        "aff_unique_norm": "University of Oxford;Amazon",
        "aff_unique_dep": ";Amazon.com, Inc.",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.amazon.com",
        "aff_unique_abbr": "Oxford;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1+0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "5ySSVlJBOn",
        "title": "FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Generalizable object fetching in cluttered scenes remains a fundamental and application-critical challenge in embodied AI. Closely packed objects cause inevitable occlusions, making safe action generation particularly difficult. Under such partial observability, effective policies must not only generalize across diverse objects and layouts but also reason about occlusion to avoid collisions. However, collecting large-scale real-world data for this task remains prohibitively expensive, leaving this problem largely unsolved. In this paper, we introduce FetchBot, a sim-to-real framework for this challenge. We first curate a large-scale synthetic dataset featuring 1M diverse scenes and 500k representative demonstrations. Based on this dataset, FetchBot employs a depth-conditioned method for action generation, which leverages structural cues to enable robust obstacle-aware action planning. However, depth is perfect in simulation but noisy in real-world environments. To address this sim-to-real gap, FetchBot predicts depth from RGB inputs using a foundation model and integrates local occupancy prediction as a co-training task, providing a generalizable latent representation for sim-to-real transfer. Extensive experiments in simulation and real-world environments demonstrate FetchBot\u2019s strong zero-shot sim-to-real transfer, effective clutter handling, and adaptability to novel scenarios. In cluttered environments, it achieves an average success rate of 89.95%, significantly outperforming prior methods. Moreover, FetchBot demonstrates excellent robustness in challenging cases, such as fetching transparent, reflective, and irregular objects, highlighting its practical value.",
        "keywords": "Generalizable Fetching;Sim2Real;Occlusion Handling",
        "primary_area": "",
        "supplementary_material": "/attachment/173481ec1e3a4777a581f2cbbc5c78c5d22b8ea5.zip",
        "author": "Weiheng Liu;Yuxuan Wan;Jilong Wang;Yuxuan Kuang;Xuesong Shi;Haoran Li;Dongbin Zhao;Zhizheng Zhang;He Wang",
        "authorids": "~Weiheng_Liu2;~Yuxuan_Wan2;~Jilong_Wang1;~Yuxuan_Kuang1;~Xuesong_Shi1;~Haoran_Li7;~Dongbin_Zhao1;~Zhizheng_Zhang1;~He_Wang5",
        "gender": "M;M;M;M;M;M;M;M;M",
        "homepage": "https://weiheng-liu.github.io/;https://github.com/Yaser-wyx;https://42jaylonw.github.io/;https://yuxuank.com/;;;http://people.ucas.ac.cn/~zhaodongbin?language=en;;https://hughw19.github.io",
        "dblp": ";;;358/9222;;;40/255;67/4758;01/6368-10",
        "google_scholar": ";;0sym0VkAAAAJ;8HBT4ocAAAAJ;https://scholar.google.com/citations?hl=en;kalE5UIAAAAJ;;X7M0I8kAAAAJ;roCAWkoAAAAJ",
        "orcid": ";;0000-0002-7511-1333;;0000-0002-3880-4501;0000-0003-2559-9585;0000-0001-8218-9633;;",
        "linkedin": ";;jilong-jaylon-wang-530963172/;;shixuesong/;;;;",
        "or_profile": "~Weiheng_Liu2;~Yuxuan_Wan2;~Jilong_Wang1;~Yuxuan_Kuang1;~Xuesong_Shi1;~Haoran_Li7;~Dongbin_Zhao1;~Zhizheng_Zhang1;~He_Wang5",
        "aff": "Institute of Automation, Chinese Academy of Sciences;Peking University;Galbot Co. Ltd.;School of Computer Science, Carnegie Mellon University+Peking University;Galbot;Institute of Automation, Chinese Academy of Sciences;Institute of Automation, Chinese Academy of Sciences;Beijing Galbot Co., Ltd;Galbot+Peking University",
        "aff_domain": "ia.ac.cn;pku.edu.cn;galbot.com;cs.cmu.edu+pku.edu.cn;galbot.com;ia.ac.cn;ia.ac.cn;galbot.com;galbot.com+pku.edu.cn",
        "position": "PhD student;PhD student;Researcher;PhD student+Undergrad student;Engineer;Associate Professor;Full Professor;Principal Researcher;CTO+Assistant Professor",
        "bibtex": "@inproceedings{\nliu2025fetchbot,\ntitle={FetchBot: Learning Generalizable Object Fetching in Cluttered Scenes via Zero-Shot Sim2Real},\nauthor={Weiheng Liu and Yuxuan Wan and Jilong Wang and Yuxuan Kuang and Xuesong Shi and Haoran Li and Dongbin Zhao and Zhizheng Zhang and He Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=5ySSVlJBOn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=5ySSVlJBOn",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3+1;4;0;0;4;4+1",
        "aff_unique_norm": "Chinese Academy of Sciences;Peking University;Galbot Co. Ltd.;Carnegie Mellon University;Galbot",
        "aff_unique_dep": "Institute of Automation;;;School of Computer Science;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.pku.edu.cn;;https://www.cmu.edu;",
        "aff_unique_abbr": "CAS;Peking U;;CMU;",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0;2+0;0;0;0;0",
        "aff_country_unique": "China;;United States"
    },
    {
        "id": "6AASPlloSt",
        "title": "RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Multi-task ``vision-language-action'' (VLA) models have recently demonstrated increasing promise as generalist foundation models for robotics, achieving non-trivial performance out of the box on new tasks in new environments. However, for such models to be truly useful, an end user must have easy means to teach them to improve. For language and vision models, the emergent ability to perform in-context learning (ICL) has proven to be a versatile and highly useful interface to easily teach new tasks with no parameter finetuning. Unfortunately, VLAs pre-trained with imitation learning objectives do not naturally acquire ICL abilities. In this paper, we demonstrate that, with the right finetuning recipe and a small robot demonstration dataset, it is possible to inject in-context adaptability post hoc into such a VLA. After retraining for in-context learning (RICL), our system permits an end user to provide a small number (10-20) of demonstrations for a new task. RICL then fetches the most relevant portions of those demonstrations into the VLA context to exploit ICL, performing the new task and boosting task performance. We apply RICL to inject ICL into the $\\pi_0$-FAST VLA, and show that it permits large in-context improvements for a variety of new manipulation tasks with only 20 demonstrations per task, without any parameter updates. When parameter updates on the target task demonstrations is possible, RICL finetuning further boosts performance. We release code and model weights for RICL-$\\pi_0$-FAST alongside the paper to enable, for the first time, a simple in-context learning interface for new manipulation tasks",
        "keywords": "Vision-Language-Action (VLA) models;In-Context Learning (ICL);Retrieval-Augmented Generation (RAG)",
        "primary_area": "",
        "supplementary_material": "/attachment/f95f7a23e04a81202d98a8dbe70152630813a602.zip",
        "author": "Kaustubh Sridhar;Souradeep Dutta;Dinesh Jayaraman;Insup Lee",
        "authorids": "~Kaustubh_Sridhar1;~Souradeep_Dutta2;~Dinesh_Jayaraman2;~Insup_Lee1",
        "gender": "M;M;M;",
        "homepage": "https://kaustubhsridhar.github.io/;https://sites.google.com/site/duttasouradeep39/;https://www.seas.upenn.edu/~dineshj/;https://www.cis.upenn.edu/~lee/",
        "dblp": "289/5808;;145/3870;l/InsupLee.html",
        "google_scholar": "V-HiOnUAAAAJ;;QxLpghAAAAAJ;qPlUgrgAAAAJ",
        "orcid": ";;0000-0002-6888-3095;0000-0003-2672-1132",
        "linkedin": "kaustubh-sridhar-8636797a/;;dinesh-jayaraman-44b31539/;",
        "or_profile": "~Kaustubh_Sridhar1;~Souradeep_Dutta2;~Dinesh_Jayaraman2;~Insup_Lee1",
        "aff": "Google Deepmind+University of Pennsylvania;University of British Columbia;University of Pennsylvania;University of Pennsylvania",
        "aff_domain": "google.com+seas.upenn.edu;ubc.ca;upenn.edu;upenn.edu",
        "position": "Researcher+PhD student;Assistant Professor;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nsridhar2025ricl,\ntitle={{RICL}:  Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models},\nauthor={Kaustubh Sridhar and Souradeep Dutta and Dinesh Jayaraman and Insup Lee},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=6AASPlloSt}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=6AASPlloSt",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;2;1;1",
        "aff_unique_norm": "DeepMind;University of Pennsylvania;University of British Columbia",
        "aff_unique_dep": "DeepMind;;",
        "aff_unique_url": "https://deepmind.com;https://www.upenn.edu;https://www.ubc.ca",
        "aff_unique_abbr": "DeepMind;UPenn;UBC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+1;2;1;1",
        "aff_country_unique": "United Kingdom;United States;Canada"
    },
    {
        "id": "6yB6AX8aSU",
        "title": "LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Developing robotic systems capable of robustly executing long-horizon manipulation tasks with human-level dexterity is challenging, as such tasks require both physical dexterity and seamless sequencing of manipulation skills while robustly handling environment variations. While imitation learning offers a promising approach, acquiring comprehensive datasets is resource-intensive. In this work, we propose a learning framework and system LodeStar that automatically decomposes task demonstrations into semantically meaningful skills using off-the-shelf foundation models, and generates diverse synthetic demonstration datasets from a few human demos through reinforcement learning. These sim-augmented datasets enable robust skill training, with a Skill Routing Transformer (SRT) policy effectively chaining the learned skills together to execute complex long-horizon manipulation tasks. Experimental evaluations on three challenging real-world long-horizon dexterous manipulation tasks demonstrate that our approach significantly improves task performance and robustness compared to previous baselines. Videos are available at lodestar-robot.github.io.",
        "keywords": "Dexterous Manipulation;Imitation Learning;Sim-to-Real",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Weikang Wan;Jiawei Fu;Xiaodi Yuan;Yifeng Zhu;Hao Su",
        "authorids": "~Weikang_Wan1;~Jiawei_Fu1;~Xiaodi_Yuan1;~Yifeng_Zhu2;~Hao_Su1",
        "gender": "M;M;F;M;M",
        "homepage": "https://wkwan7.github.io/;;https://rabbit-hu.github.io/;https://cs.utexas.edu/~yifengz;http://ai.ucsd.edu/~haosu",
        "dblp": "314/9770;;;;09/4945-1",
        "google_scholar": "MVE-fyQAAAAJ;;i-QiNPIAAAAJ;;1P8Zu04AAAAJ",
        "orcid": ";;0009-0003-2320-2297;;",
        "linkedin": ";jiawei-fu-b6687b200/;xiaodi-yuan-117b681aa/;;",
        "or_profile": "~Weikang_Wan1;~Jiawei_Fu1;~Xiaodi_Yuan1;~Yifeng_Zhu2;~Hao_Su1",
        "aff": "University of California, San Diego;University of California, San Diego;University of California, San Diego;The University of Texas at Austin;University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu;utexas.edu;ucsd.edu",
        "position": "PhD student;PhD student;PhD student;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nwan2025lodestar,\ntitle={LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations},\nauthor={Weikang Wan and Jiawei Fu and Xiaodi Yuan and Yifeng Zhu and Hao Su},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=6yB6AX8aSU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=6yB6AX8aSU",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "University of California, San Diego;University of Texas at Austin",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucsd.edu;https://www.utexas.edu",
        "aff_unique_abbr": "UCSD;UT Austin",
        "aff_campus_unique_index": "0;0;0;1;0",
        "aff_campus_unique": "San Diego;Austin",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "7OOMC7pzaw",
        "title": "TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Dexterous teleoperation plays a crucial role in robotic manipulation for real-world data collection and remote robot control. Previous dexterous teleoperation mostly relies on hand retargeting to closely mimic human hand postures. However, these approaches may fail to fully leverage the inherent dexterity of dexterous hands, which can execute unique actions through their structural advantages compared to human hands. To address this limitation, we propose TypeTele, a type-guided dexterous teleoperation system, which enables dexterous hands to perform actions that are not constrained by human motion patterns. This is achieved by introducing dexterous manipulation types into the teleoperation system, allowing operators to employ appropriate types to complete specific tasks. To support this system, we build an extensible dexterous manipulation type library to cover comprehensive dexterous postures used in manipulation tasks. During teleoperation, we employ a MLLM (Multi-modality Large Language Model)-assisted type retrieval module to identify the most suitable manipulation type based on the specific task and operator commands. Extensive experiments of real-world teleoperation and imitation learning demonstrate that the incorporation of manipulation types significantly takes full advantage of the dexterous robot's ability to perform diverse and complex tasks with higher success rates.",
        "keywords": "Teleoperation;Dexterous;Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/e79ad958e953bf87a8c9e5e17375b0931aa500a4.zip",
        "author": "Yuhao Lin;Yi-Lin Wei;Haoran Liao;Mu Lin;Chengyi Xing;Hao Li;Dandan Zhang;Mark Cutkosky;Wei-Shi Zheng",
        "authorids": "~Yuhao_Lin9;~Yi-Lin_Wei1;~Haoran_Liao6;~Mu_Lin1;~Chengyi_Xing1;~Hao_Li23;~Dandan_Zhang6;~Mark_Cutkosky1;~Wei-Shi_Zheng3",
        "gender": "M;;;M;M;Not Specified;F;M;M",
        "homepage": "https://github.com/lycopene123;;;https://github.com/frenkielm;https://chengyi-xing.com;https://haolirobo.github.io;https://www.intelligentrobotics-acrossscales.com/;http://bdml.stanford.edu;http://www.isee-ai.cn/~zhwshi",
        "dblp": ";;;;348/9124;;;;30/8399",
        "google_scholar": ";;;;BglGZXEAAAAJ;IDmUyTEAAAAJ;;https://scholar.google.com/citations?hl=en;AwqDDGoAAAAJ",
        "orcid": ";;;;;0000-0001-5030-457X;;0000-0003-4730-0900;",
        "linkedin": ";;;;xcyhbp/;hao-li-sjtu-pu/;;;",
        "or_profile": "~Yuhao_Lin9;~Yi-Lin_Wei1;~Haoran_Liao6;~Mu_Lin1;~Chengyi_Xing1;~Hao_Li23;~Dandan_Zhang6;~Mark_Cutkosky1;~Wei-Shi_Zheng3",
        "aff": "SUN YAT-SEN UNIVERSITY;;;SUN YAT-SEN UNIVERSITY;Stanford University;Stanford University;Imperial College London;Stanford University;SUN YAT-SEN UNIVERSITY",
        "aff_domain": "sysu.edu.cn;;;mail2.sysu.edu.cn;stanford.edu;stanford.edu;imperial.ac.uk;stanford.edu;sysu.edu.cn",
        "position": "Undergrad student;;;Undergrad student;MS student;PhD student;Lecturer;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nlin2025typetele,\ntitle={TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types},\nauthor={Yuhao Lin and Yi-Lin Wei and Haoran Liao and Mu Lin and Chengyi Xing and Hao Li and Dandan Zhang and Mark Cutkosky and Wei-Shi Zheng},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=7OOMC7pzaw}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7OOMC7pzaw",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;0;2;2;3;2;0",
        "aff_unique_norm": "Sun Yat-sen University;;Stanford University;Imperial College London",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "http://www.sysu.edu.cn;;https://www.stanford.edu;https://www.imperial.ac.uk",
        "aff_unique_abbr": "SYSU;;Stanford;ICL",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0;2;2;3;2;0",
        "aff_country_unique": "China;;United States;United Kingdom"
    },
    {
        "id": "7XyO9Y1hI1",
        "title": "EndoVLA: Dual-Phase Vision-Language-Action for Precise Autonomous Tracking in Endoscopy",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In endoscopic procedures, autonomous tracking of abnormal regions and following of circumferential cutting markers can significantly reduce the cognitive burden on endoscopists. However, conventional model-based pipelines are fragile\u2014each component (e.g., detection, motion planning) requires manual tuning and struggles to incorporate high-level endoscopic intent, resulting in poor generalization across variable scenes. Vision\u2013Language\u2013Action (VLA) models, which integrate visual perception, language grounding, and motion planning within an end-to-end framework, offer a promising alternative to semantically adapt to surgeon prompts, without the need for manual recalibration. Despite their potential, applying VLA models to robotic endoscopy presents unique challenges due to the inherently complex and dynamic anatomical environments of the gastrointestinal (GI) tract. To this end, we introduce EndoVLA, designed specifically for continuum robots in GI interventions. Provided endoscopic images and surgeon-issued tracking prompts, EndoVLA performs three core tasks: (1) polyp tracking, (2) delineation and following of abnormal mucosal regions, and (3) adherence to predefined circular markers during circumferential cutting. To address the unique challenges posed by data scarcity and domain shifts, we propose a dual-phase strategy, with supervised fine-tuning on our EndoVLA-Motion dataset and reinforcement fine-tuning using task-aware rewards. Our approach significantly enhances the tracking performance in endoscopy, and zero-shot generalization of tracking in general scenes and more challenging sequential tasks.",
        "keywords": "Vision\u2013Language\u2013Action;Continuum Robots;Autonomous Endoscopic Tracking;Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "CHI KIT NG;Long Bai;Guankun Wang;Yupeng Wang;Huxin Gao;Kun yuan;Chenhan Jin;Tieyong Zeng;Hongliang Ren",
        "authorids": "~CHI_KIT_NG1;~Long_Bai2;~Guankun_Wang1;~Yupeng_Wang3;~Huxin_Gao1;~Kun_yuan6;~Chenhan_Jin2;~Tieyong_Zeng2;~Hongliang_Ren3",
        "gender": "M;;M;;M;M;;M;M",
        "homepage": "https://ngchikit.github.io/;;https://gkw0010.github.io/;;https://ghx-0228.github.io/;https://flaick.github.io/;;https://www.math.cuhk.edu.hk/~zeng/;http://www.labren.org/mm/",
        "dblp": ";;;;;;;63/2745.html;44/3343.html",
        "google_scholar": "SCHOLAR_ID;;Z27F8gkAAAAJ;;https://scholar.google.com/citations?hl=en;zId4EqoAAAAJ;;https://scholar.google.com.hk/citations?user=2yyTgRwAAAAJ;https://scholar.google.com.sg/citations?user=rcF7N44AAAAJ",
        "orcid": ";;0000-0003-2440-4950;;;;;0000-0002-0688-202X;0000-0002-6488-1551",
        "linkedin": "chi-kit-ng-91994230a;;;%E5%AE%87%E9%B9%8F-%E7%8E%8B-778170293/;;;;;",
        "or_profile": "~CHI_KIT_NG1;~Long_Bai2;~Guankun_Wang1;~Yupeng_Wang3;~Huxin_Gao1;~Kun_yuan6;~Chenhan_Jin2;~Tieyong_Zeng2;~Hongliang_Ren3",
        "aff": "Chinese University of Hong Kong;;The Chinese University of Hong Kong;The Chinese University of Hong Kong;The Chinese University of Hong Kong;Universit\u00e9 de Strasbourg+Technische Universit\u00e4t M\u00fcnchen;;The Chinese University of Hong Kong;The Chinese University of Hong Kong",
        "aff_domain": "cuhk.hk;;link.cuhk.edu.hk;link.cuhk.edu.hk;cuhk.edu.hk;unistra.fr+tum.de;;cuhk.edu.hk;cuhk.edu.hk",
        "position": "PhD student;;PhD student;PhD student;Postdoc;PhD student+PhD student;;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nkit2025endovla,\ntitle={Endo{VLA}: Dual-Phase Vision-Language-Action for Precise Autonomous Tracking in Endoscopy},\nauthor={CHI KIT NG and Long Bai and Guankun Wang and Yupeng Wang and Huxin Gao and Kun yuan and Chenhan Jin and Tieyong Zeng and Hongliang Ren},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=7XyO9Y1hI1}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7XyO9Y1hI1",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0;0;2+3;1;0;0",
        "aff_unique_norm": "Chinese University of Hong Kong;;Universit\u00e9 de Strasbourg;Technische Universit\u00e4t M\u00fcnchen",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.cuhk.edu.hk;;https://www.unistra.fr;https://www.tum.de",
        "aff_unique_abbr": "CUHK;;Unistra;TUM",
        "aff_campus_unique_index": "0;0;0;0;;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;2+3;0;0",
        "aff_country_unique": "China;;France;Germany"
    },
    {
        "id": "7iaYcss56y",
        "title": "ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Learning robot manipulation from abundant human videos offers a scalable alternative to costly robot-specific data collection. However, domain gaps across visual, morphological, and physical aspects hinder direct imitation. To effectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic co-training framework that leverages both human videos and a small amount of teleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with either action- or visual-based mapping to map retargeted human hand poses to robot joints, followed by MixUp interpolation between paired human and robot trajectories. Our key insights are (1) retargeted human hand trajectories provide informative action labels, and (2) interpolation over the mapped data creates intermediate domains that facilitate smooth domain adaptation during co-training. Evaluations on four real-world manipulation tasks (Pick and Place, Push, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro, Ability) show that ImMimic improves task success rates and execution smoothness, highlighting its efficacy to bridge the domain gap for robust robot manipulation. The project website can be found at https://sites.google.com/view/immimic.",
        "keywords": "Learning from Human;Imitation learning;Dexterous Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/8bcc08b2c34e9f52ae3cebd63adfb2fa2d087c53.zip",
        "author": "Yangcen Liu;Woo Chul Shin;Yunhai Han;Zhenyang Chen;Harish Ravichandar;Danfei Xu",
        "authorids": "~Yangcen_Liu1;~Woo_Chul_Shin1;~Yunhai_Han1;~Zhenyang_Chen1;~Harish_Ravichandar1;~Danfei_Xu1",
        "gender": "M;;M;M;;M",
        "homepage": "https://github.com/Randle-Github;https://swc0620.github.io/profile/;https://y8han.github.io/;;http://harishravichandar.com/;https://cs.stanford.edu/~danfei/",
        "dblp": ";;276/6126;;237/9959;135/8443",
        "google_scholar": "Xq-r3dIAAAAJ;;lsN3nY8AAAAJ;9jF7qBkAAAAJ;d2HP6SMAAAAJ;J5D4kcoAAAAJ",
        "orcid": "0009-0005-1720-2199;;;;0000-0002-6635-2637;",
        "linkedin": "yangcenliu/;woochulshin/;;;;",
        "or_profile": "~Yangcen_Liu1;~Woo_Chul_Shin1;~Yunhai_Han1;~Zhenyang_Chen1;~Harish_Ravichandar1;~Danfei_Xu1",
        "aff": "Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;;Georgia Institute of Technology;Georgia Institute of Technology+NVIDIA",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;;gatech.edu;gatech.edu+nvidia.com",
        "position": "MS student;MS student;PhD student;;Assistant Professor;Assistant Professor+Research Scientist",
        "bibtex": "@inproceedings{\nliu2025immimic,\ntitle={ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation},\nauthor={Yangcen Liu and Woo Chul Shin and Yunhai Han and Zhenyang Chen and Harish Ravichandar and Danfei Xu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=7iaYcss56y}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7iaYcss56y",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1;0;0+2",
        "aff_unique_norm": "Georgia Institute of Technology;;NVIDIA",
        "aff_unique_dep": ";;NVIDIA Corporation",
        "aff_unique_url": "https://www.gatech.edu;;https://www.nvidia.com",
        "aff_unique_abbr": "Georgia Tech;;NVIDIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "7wGYX11BJB",
        "title": "ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "3D world models (i.e., learning-based 3D dynamics models) offer a promising approach to generalizable robotic manipulation by capturing the underlying physics of environment evolution conditioned on robot actions. However, existing 3D world models are primarily limited to single-material dynamics using a particle-based Graph Neural Network model, and often require time-consuming 3D scene reconstruction to obtain 3D particle tracks for training. In this work, we present ParticleFormer, a Transformer-based point cloud world model trained with a hybrid point cloud reconstruction loss, supervising both global and local dynamics features in multi-material, multi-object robot interactions. ParticleFormer captures fine-grained multi-object interactions between rigid, deformable, and flexible materials, trained directly from real-world robot perception data without an elaborate scene reconstruction. We demonstrate the model's effectiveness both in 3D scene forecasting tasks, and in downstream manipulation tasks using a Model Predictive Control (MPC) policy.  In addition, we extend existing dynamics learning benchmarks to include diverse multi-material, multi-object interaction scenarios. We validate our method on six simulation and three real-world experiments, where it consistently outperforms leading baselines by achieving superior dynamics prediction accuracy and less rollout error in downstream visuomotor tasks. Experimental videos are available at https://particleformer.github.io/.",
        "keywords": "Learning-based Dynamics Modeling;Model-based Planning",
        "primary_area": "",
        "supplementary_material": "/attachment/8ff363fff37c5929276352ccb8807de09c322c71.zip",
        "author": "Suning Huang;Qianzhong Chen;Xiaohan Zhang;Jiankai Sun;Mac Schwager",
        "authorids": "~Suning_Huang1;~Qianzhong_Chen2;~Xiaohan_Zhang7;~Jiankai_Sun6;~Mac_Schwager1",
        "gender": ";M;M;;M",
        "homepage": ";https://qianzhong-chen.github.io/;https://keke-220.github.io/;;https://msl.stanford.edu/",
        "dblp": ";;;121/4211;22/7012",
        "google_scholar": ";MqU82XsAAAAJ;uWfcPkkAAAAJ;726MCb8AAAAJ;-EqbTXoAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";qianzhong-chen-9bab01209/;;;",
        "or_profile": "~Suning_Huang1;~Qianzhong_Chen2;~Xiaohan_Zhang7;~Jiankai_Sun6;~Mac_Schwager1",
        "aff": ";Stanford University;Boston Dynamics AI Institute;Stanford University;Stanford University",
        "aff_domain": ";stanford.edu;theaiinstitute.com;stanford.edu;stanford.edu",
        "position": ";MS student;Roboticist;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nhuang2025particleformer,\ntitle={ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation},\nauthor={Suning Huang and Qianzhong Chen and Xiaohan Zhang and Jiankai Sun and Mac Schwager},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=7wGYX11BJB}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=7wGYX11BJB",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;1;1",
        "aff_unique_norm": ";Stanford University;Boston Dynamics AI Institute",
        "aff_unique_dep": ";;AI Institute",
        "aff_unique_url": ";https://www.stanford.edu;https://www.bostondynamics.com/",
        "aff_unique_abbr": ";Stanford;BD AI",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "1;1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "8DHSyMFLbB",
        "title": "Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Learning generalizable robot manipulation policies, especially for complex multi-fingered humanoids, remains a significant challenge. Existing approaches primarily rely on extensive data collection and imitation learning, which are expensive, labor-intensive, and difficult to scale. Sim-to-real reinforcement learning (RL) offers a promising alternative, but has mostly succeeded in simpler state-based or single-hand setups. How to effectively extend this to vision-based, contact-rich bimanual manipulation tasks remains an open question. In this paper, we introduce a practical sim-to-real RL recipe that trains a humanoid robot to perform three challenging dexterous manipulation tasks: grasp-and-reach, box lift and bimanual handover. Our method features an automated real-to-sim tuning module, a generalized reward formulation based on contact and object goals, a divide-and-conquer policy distillation framework, and a hybrid object representation strategy with modality-specific augmentation. We demonstrate high success rates on unseen objects and robust, adaptive policy behaviors -- highlighting that vision-based dexterous manipulation via sim-to-real RL is not only viable, but also scalable and broadly applicable to real-world humanoid manipulation tasks.",
        "keywords": "Humanoids;Vision-Based Dexterous Manipulation;Reinforcement Learning;Sim-to-Real",
        "primary_area": "",
        "supplementary_material": "/attachment/e61af1a1cea4559fc2ef4ad4ff63cb708604eaf2.zip",
        "author": "Toru Lin;Kartik Sachdev;Linxi Fan;Jitendra Malik;Yuke Zhu",
        "authorids": "~Toru_Lin1;~Kartik_Sachdev1;~Linxi_Fan2;~Jitendra_Malik2;~Yuke_Zhu1",
        "gender": ";M;;M;M",
        "homepage": ";;;https://people.eecs.berkeley.edu/~malik/;https://yukezhu.me/",
        "dblp": ";;154/6778;58/2944;133/1772",
        "google_scholar": ";;sljtWIUAAAAJ;oY9R5YQAAAAJ;mWGyYMsAAAAJ",
        "orcid": ";;;0000-0003-3695-1580;",
        "linkedin": ";https://de.linkedin.com/in/kartik-sachdev-7bb581ab;;;",
        "or_profile": "~Toru_Lin1;~Kartik_Sachdev1;~Linxi_Fan2;~Jitendra_Malik2;~Yuke_Zhu1",
        "aff": ";NVIDIA;NVIDIA;Meta Facebook+University of California, Berkeley;Computer Science Department, University of Texas, Austin",
        "aff_domain": ";nvidia.com;nvidia.com;fb.com+berkeley.edu;cs.utexas.edu",
        "position": ";Researcher;Researcher;Director of Research+Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nlin2025simtoreal,\ntitle={Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids},\nauthor={Toru Lin and Kartik Sachdev and Linxi Fan and Jitendra Malik and Yuke Zhu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=8DHSyMFLbB}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8DHSyMFLbB",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;2+3;4",
        "aff_unique_norm": ";NVIDIA;Meta;University of California, Berkeley;University of Texas at Austin",
        "aff_unique_dep": ";NVIDIA Corporation;Meta Platforms, Inc.;;Computer Science Department",
        "aff_unique_url": ";https://www.nvidia.com;https://meta.com;https://www.berkeley.edu;https://www.utexas.edu",
        "aff_unique_abbr": ";NVIDIA;Meta;UC Berkeley;UT Austin",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Berkeley;Austin",
        "aff_country_unique_index": "1;1;1+1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "8RdxHk9hpr",
        "title": "Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Nominal payload ratings for articulated robots are typically derived from worst-case configurations, resulting in uniform payload constraints across the entire workspace. This conservative approach severely underutilizes the robot's inherent capabilities---our analysis demonstrates that manipulators can safely handle payloads well above nominal capacity across broad regions of their workspace while staying within joint angle, velocity, acceleration, and torque limits. To address this gap between assumed and actual capability, we propose a novel trajectory generation approach using denoising diffusion models that explicitly incorporates payload constraints into the planning process. Unlike traditional sampling-based methods that rely on inefficient trial-and-error, optimization-based methods that are prohibitively slow, or kinodynamic planners that struggle with problem dimensionality, our approach generates dynamically feasible joint-space trajectories in constant time that can be directly executed on physical hardware without post-processing. Experimental validation on a 7 DoF Franka Emika Panda robot demonstrates that up to 67.6% of the workspace remains accessible even with payloads exceeding 3 times the nominal capacity. This expanded operational envelope highlights the importance of a more nuanced consideration of payload dynamics in motion planning algorithms.",
        "keywords": "Robot Planning;Grasping & Manipulation;Robot Modeling & Simulation;diffusion models;dynamics-constrained planning;payload transport",
        "primary_area": "",
        "supplementary_material": "/attachment/5756274f5d29dd872e481b80ec8c1dc34973b778.zip",
        "author": "Anuj Pasricha;Joewie J. Koh;Jay Vakil;Alessandro Roncone",
        "authorids": "~Anuj_Pasricha1;~Joewie_J._Koh1;~Jay_Vakil1;~Alessandro_Roncone1",
        "gender": ";;M;M",
        "homepage": ";;https://jdvakil.github.io;https://hiro-group.ronc.one/",
        "dblp": ";;345/8174;151/9736",
        "google_scholar": "zu18jdAAAAAJ;;https://scholar.google.com/citations?hl=en;nV-XGVIAAAAJ",
        "orcid": ";;;0000-0001-7385-1875",
        "linkedin": ";;jdvakil;alessandroroncone",
        "or_profile": "~Anuj_Pasricha1;~Joewie_J._Koh1;~Jay_Vakil1;~Alessandro_Roncone1",
        "aff": "University of Colorado at Boulder;;University of Colorado at Boulder;University of Colorado at Boulder",
        "aff_domain": "colorado.edu;;colorado.edu;colorado.edu",
        "position": "PhD student;;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\npasricha2025dynamicscompliant,\ntitle={Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation},\nauthor={Anuj Pasricha and Joewie J. Koh and Jay Vakil and Alessandro Roncone},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=8RdxHk9hpr}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8RdxHk9hpr",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Colorado;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.colorado.edu;",
        "aff_unique_abbr": "CU;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Boulder;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "8v0mlyKk5q",
        "title": "Beyond Constant Parameters: Hyper Prediction Models and HyperMPC",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Model Predictive Control (MPC) is among the most widely adopted and reliable methods for robot control, relying critically on an accurate dynamics model. However, existing dynamics models used in the gradient-based MPC are limited by computational complexity and state representation. To address this limitation, we propose the Hyper Prediction Model (HyperPM) - a novel approach in which we project the unmodeled dynamics onto a time-dependent dynamics model. This time-dependency is captured through time-varying model parameters, whose evolution over the MPC prediction horizon is learned using a neural network. Such formulation preserves the computational efficiency and robustness of the base model while equipping it with the capacity to anticipate previously unmodeled phenomena. We evaluated the proposed approach on several challenging systems, including real-world F1TENTH autonomous racing, and demonstrated that it significantly reduces long-horizon prediction errors. Moreover, when integrated within the MPC framework (HyperMPC), our method consistently outperforms existing state-of-the-art techniques.",
        "keywords": "Model Learning for Robot Control;Dynamics Model Learning;Model Predictive Control;MPC",
        "primary_area": "",
        "supplementary_material": "/attachment/42c824124d793f6b3de9290c90fc297a5fa32337.zip",
        "author": "Jan W\u0119grzynowski;Piotr Kicki;Grzegorz Czechmanowski;Maciej Piotr Krupka;Krzysztof Walas",
        "authorids": "~Jan_W\u0119grzynowski1;~Piotr_Kicki1;~Grzegorz_Czechmanowski1;~Maciej_Piotr_Krupka1;~Krzysztof_Walas2",
        "gender": "M;M;M;M;M",
        "homepage": ";;;;https://ideas-ncbr.pl/en/osoby/krzysztof-walas/",
        "dblp": ";234/2595;;;05/9858.html",
        "google_scholar": ";tilnVjMAAAAJ;;;0FZ0cZQAAAAJ",
        "orcid": "0009-0001-7901-7387;;0009-0002-7199-2492;;0000-0002-2800-2716",
        "linkedin": ";;;maciej-krupka-91a41a212/;krzysztof-walas-850492a7/",
        "or_profile": "~Jan_W\u0119grzynowski1;~Piotr_Kicki1;~Grzegorz_Czechmanowski1;~Maciej_Piotr_Krupka1;~Krzysztof_Walas2",
        "aff": "Technical University of Poznan;Technical University of Poznan+IDEAS NCBR Sp.;;;Technical University of Poznan",
        "aff_domain": "put.poznan.pl;put.poznan.pl+ideas-ncbr.pl;;;put.poznan.pl",
        "position": "Lecturer;Assistant Professor+Postdoc;;;Assistant Professor",
        "bibtex": "@inproceedings{\nwegrzynowski2025beyond,\ntitle={Beyond Constant Parameters: Hyper Prediction Models and Hyper{MPC}},\nauthor={Jan W{\\k{e}}grzynowski and Piotr Kicki and Grzegorz Czechmanowski and Maciej Piotr Krupka and Krzysztof Walas},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=8v0mlyKk5q}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=8v0mlyKk5q",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0+1;2;2;0",
        "aff_unique_norm": "Technical University of Poznan;IDEAS NCBR;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.put.poznan.pl/;;",
        "aff_unique_abbr": "PUT;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "Poland;"
    },
    {
        "id": "93bWCbhXJR",
        "title": "Distilling On-device Language Models for Robot Planning with Minimal Human Intervention",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Large language models (LLMs) provide robots with powerful contextual reasoning abilities and a natural human interface. Yet, current LLM-enabled robots typically depend on cloud-hosted models, limiting their usability in environments with unreliable communication infrastructure, such as outdoor or industrial settings. We present PRISM, a framework for distilling small language model (SLM)-enabled robot planners that run on-device with minimal human supervision. Starting from an existing LLM-enabled planner, PRISM automatically synthesizes diverse tasks and environments, elicits plans from the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in replacement of the source model. We apply PRISM to three LLM-enabled planners for mapping and exploration, manipulation, and household assistance, and we demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20\\% of GPT-4o's performance to over 93% - using only synthetic data. We further demonstrate that the distilled planners generalize across heterogeneous robotic platforms (ground and aerial) and diverse environments (indoor and outdoor). We release all software, trained models, and datasets to promote reproducibility and follow-up work.",
        "keywords": "LLM-enabled Robots;LLM Distillation",
        "primary_area": "",
        "supplementary_material": "/attachment/ddb9c952393a7a0b1e3c0313a4f20364b48e5b6f.zip",
        "author": "Zachary Ravichandran;Ignacio Hounie;Fernando Cladera;Alejandro Ribeiro;George J. Pappas;Vijay Kumar",
        "authorids": "~Zachary_Ravichandran1;~Ignacio_Hounie1;~Fernando_Cladera1;~Alejandro_Ribeiro1;~George_J._Pappas1;~Vijay_Kumar2",
        "gender": ";;M;M;;",
        "homepage": "https://zacravichandran.github.io/;;http://fcladera.com;https://alelab.seas.upenn.edu;;http://kumarrobotics.org",
        "dblp": ";;;32/15;;",
        "google_scholar": "pwAI24gAAAAJ;V0h3OSYAAAAJ;5kvESDsAAAAJ;7mrPM4kAAAAJ;;FUOEBDUAAAAJ",
        "orcid": ";;;0000-0003-4230-9906;;",
        "linkedin": "zachary-ravichandran;;;;;",
        "or_profile": "~Zachary_Ravichandran1;~Ignacio_Hounie1;~Fernando_Cladera1;~Alejandro_Ribeiro1;~George_J._Pappas1;~Vijay_Kumar2",
        "aff": "University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;University of Pennsylvania;;",
        "aff_domain": "seas.upenn.edu;upenn.edu;upenn.edu;upenn.edu;;",
        "position": "PhD student;PhD student;PhD student;Full Professor;;",
        "bibtex": "@inproceedings{\nravichandran2025distilling,\ntitle={Distilling On-device Language Models for Robot Planning with Minimal Human Intervention},\nauthor={Zachary Ravichandran and Ignacio Hounie and Fernando Cladera and Alejandro Ribeiro and George J. Pappas and Vijay Kumar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=93bWCbhXJR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=93bWCbhXJR",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;1;1",
        "aff_unique_norm": "University of Pennsylvania;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upenn.edu;",
        "aff_unique_abbr": "UPenn;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "9AHjtHLlIe",
        "title": "Imagine, Verify, Execute: Memory-guided Agentic Exploration with Vision-Language Models",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Exploration is key for general-purpose robotic learning, particularly in open-ended environments where explicit guidance or task-specific feedback is limited. Vision-language models (VLMs), which can reason about object semantics, spatial relations, and potential outcomes, offer a promising foundation for guiding exploratory behavior by generating high-level goals or transitions. However, their outputs lack grounding, making it difficult to determine whether imagined transitions are physically feasible or informative. To bridge the gap between imagination and execution, we present IVE (Imagine, Verify, Execute), an agentic exploration framework inspired by human curiosity.  Human exploration often emerges from the drive to discover novel scene configurations and to understand the environment.\nInspired by this, IVE leverages VLMs to abstract RGB-D observations into semantic scene graphs, imagine novel scenes, predict their physical plausibility, and generate executable skill sequences through action tools. We evaluate IVE in both simulated and real-world tabletop environments. The results show that IVE produces more diverse and meaningful exploration than RL baselines. The collected data facilitates learning downstream tasks that closely match those of policies trained on human-collected demonstrations.",
        "keywords": "Exploration;Agentic System;Vision-Language Model",
        "primary_area": "",
        "supplementary_material": "/attachment/2b73bb234439bf8433d2cde76593db8660ba9815.zip",
        "author": "Seungjae Lee;Daniel Ekpo;Haowen Liu;Furong Huang;Abhinav Shrivastava;Jia-Bin Huang",
        "authorids": "~Seungjae_Lee2;~Daniel_Ekpo1;~Haowen_Liu2;~Furong_Huang1;~Abhinav_Shrivastava2;~Jia-Bin_Huang1",
        "gender": ";M;;F;M;M",
        "homepage": "https://sjlee.cc;https://danielekpo.com;;https://furong-huang.com;http://abhinavsh.info;https://jbhuang0604.github.io/",
        "dblp": ";;;72/8513;65/10572;51/1815-1.html",
        "google_scholar": "hpR9h74AAAAJ;WI977pYAAAAJ;;13yyuCcAAAAJ;mIF9BowAAAAJ;pp848fYAAAAJ",
        "orcid": ";;;;0000-0001-8928-8554;",
        "linkedin": ";https://linkedin.com/in/danielekpo;;;;jia-bin-huang-070a7418/",
        "or_profile": "~Seungjae_Lee2;~Daniel_Ekpo1;~Haowen_Liu2;~Furong_Huang1;~Abhinav_Shrivastava2;~Jia-Bin_Huang1",
        "aff": "University of Maryland, College Park;University of Maryland, College Park;;University of Maryland;Department of Computer Science, University of Maryland, College Park;University of Maryland, College Park",
        "aff_domain": "umd.edu;umd.edu;;umd.edu;cs.umd.edu;umd.edu",
        "position": "PhD student;PhD student;;Associate Professor;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\nlee2025imagine,\ntitle={Imagine, Verify, Execute: Memory-guided Agentic Exploration with Vision-Language Models},\nauthor={Seungjae Lee and Daniel Ekpo and Haowen Liu and Furong Huang and Abhinav Shrivastava and Jia-Bin Huang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=9AHjtHLlIe}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9AHjtHLlIe",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;2;0",
        "aff_unique_norm": "University of Maryland;;University of Maryland, College Park",
        "aff_unique_dep": ";;Department of Computer Science",
        "aff_unique_url": "https://www/umd.edu;;https://www/umd.edu",
        "aff_unique_abbr": "UMD;;UMD",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "College Park;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "9FpccnRarn",
        "title": "BEVCalib: LiDAR-Camera Calibration via Geometry-Guided Bird\u2019s-Eye View Representation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Accurate LiDAR-camera calibration is the foundation of accurate multimodal fusion environmental perception for autonomous driving and robotic systems. Traditional calibration methods require extensive data collection in controlled environments and cannot compensate for the transformation changes during the vehicle/robot movement. In this paper, we propose the first model that uses bird's-eye view (BEV) features to perform LiDAR camera calibration from raw data, termed BEVCalib. To achieve this, we extract camera BEV features and LiDAR BEV features separately and fuse them into a shared BEV feature space. To fully utilize the geometry information from the BEV feature, we introduce a novel feature selector to choose the most important feature in the transformation decoder, which reduces memory consumption and enables efficient training. Extensive evaluations in various datasets demonstrate that BEVCalib establishes a new state-of-the-art; improving the best open-source baseline by two orders of magnitude on KITTI, Nuscenes, and our dynamic extrinsic dataset, respectively, and outperforming the best baseline in literature by 72% on KITTI dataset, and 69% on Nuscenes dataset. All source code and checkpoints will be released.",
        "keywords": "LiDAR-Camera Calibration;Autonomous Driving;BEV Features",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Weiduo Yuan;Jerry Li;Justin Yue;Divyank Shah;Konstantinos Karydis;Hang Qiu",
        "authorids": "~Weiduo_Yuan1;~Jerry_Li5;~Justin_Yue1;~Divyank_Shah1;~Konstantinos_Karydis2;~Hang_Qiu1",
        "gender": "M;M;M;M;;",
        "homepage": ";https://jeli04.github.io;https://github.com/jyue86;;;https://hangqiu.github.io/",
        "dblp": ";;;;;20/1303",
        "google_scholar": ";;faplVG4AAAAJ;;;9i_MgykAAAAJ",
        "orcid": ";;;;;0000-0003-1206-9032",
        "linkedin": "weiduo-yuan-903832305/;;jyue86;divyank-shah/;;",
        "or_profile": "~Weiduo_Yuan1;~Jerry_Li5;~Justin_Yue1;~Divyank_Shah1;~Konstantinos_Karydis2;~Hang_Qiu1",
        "aff": "University of Southern California;University of Southern California+University of California, Riverside;University of California, Riverside;University of California, Riverside;;University of California, Riverside",
        "aff_domain": "usc.edu;usc.edu+ucr.edu;ucr.edu;ucr.edu;;ucr.edu",
        "position": "MS student;MS student+Undergrad student;PhD student;MS student;;Assistant Professor",
        "bibtex": "@inproceedings{\nyuan2025bevcalib,\ntitle={{BEVC}alib: Li{DAR}-Camera Calibration via Geometry-Guided Bird{\\textquoteright}s-Eye View Representation},\nauthor={Weiduo Yuan and Jerry Li and Justin Yue and Divyank Shah and Konstantinos Karydis and Hang Qiu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=9FpccnRarn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9FpccnRarn",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0+1;1;1;2;1",
        "aff_unique_norm": "University of Southern California;University of California, Riverside;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.usc.edu;https://www.ucr.edu;",
        "aff_unique_abbr": "USC;UCR;",
        "aff_campus_unique_index": "0;0+1;1;1;1",
        "aff_campus_unique": "Los Angeles;Riverside;",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "9HwVDqndnv",
        "title": "Generating Robot Constitutions & Benchmarks for Semantic Safety",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Large vision and language models are being increasingly deployed on real robots, leading to an immediate need for ensuring robot safety under AI-control. In this paper, we develop the ASIMOV Benchmark \u2014 a collection of large-scale semantic safety datasets grounded in real-world visual scenes and human injury reports from hospitals (500k situations, 3M instructions). We propose a scalable recipe for data generation leveraging text and image generation techniques to synthesize safety-relevant scenarios. As a second contribution, we develop a framework to automatically generate robot constitutions from real-world data to steer a robot\u2019s behavior using Constitutional AI mechanisms. We report a top alignment rate of 84.3% on the ASIMOV Benchmark using generated constitutions, outperforming no-constitution baselines and human-written constitutions. We argue that human interpretability and modifiability of constitutions inferred from data make them an ideal medium for behavior governance of AI-controlled robots.",
        "keywords": "constitutional ai;constitution;safety;benchmark;semantic safety",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Pierre Sermanet;Anirudha Majumdar;Alex Irpan;Dmitry Kalashnikov;Vikas Sindhwani",
        "authorids": "~Pierre_Sermanet1;~Anirudha_Majumdar1;~Alex_Irpan1;~Dmitry_Kalashnikov1;~Vikas_Sindhwani1",
        "gender": ";M;M;;M",
        "homepage": "https://sermanet.github.io/;https://irom-lab.princeton.edu/majumdar/;http://www.alexirpan.com;;http://vikas.sindhwani.org",
        "dblp": "28/6457;116/6436;202/2063;222/2882;26/4825",
        "google_scholar": "0nPi5YYAAAAJ;ibu3FwsAAAAJ;;;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;;",
        "linkedin": "sermanet/;;;;vikassindhwani",
        "or_profile": "~Pierre_Sermanet1;~Anirudha_Majumdar1;~Alex_Irpan1;~Dmitry_Kalashnikov1;~Vikas_Sindhwani1",
        "aff": "Google;Google+Princeton University;Google DeepMind;Google;Google",
        "aff_domain": "google.com;google.com+princeton.edu;google.com;google.com;google.com",
        "position": "Research Scientist;Researcher+Associate Professor;Researcher;Researcher;Senior Staff Research Scientist",
        "bibtex": "@inproceedings{\nsermanet2025generating,\ntitle={Generating Robot Constitutions \\& Benchmarks for Semantic Safety},\nauthor={Pierre Sermanet and Anirudha Majumdar and Alex Irpan and Dmitry Kalashnikov and Vikas Sindhwani},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=9HwVDqndnv}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9HwVDqndnv",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "Google;Princeton University",
        "aff_unique_dep": "Google;",
        "aff_unique_url": "https://www.google.com;https://www.princeton.edu",
        "aff_unique_abbr": "Google;Princeton",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0;0+0;1;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "9f3klkpa4y",
        "title": "CLASS: Contrastive Learning via Action Sequence Supervision for Robot Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Recent advances in Behavior Cloning (BC) have led to strong performance in robotic manipulation, driven by expressive models, sequence modeling of actions, and large-scale demonstration data. However, BC faces significant challenges when applied to heterogeneous datasets, such as visual shift with different camera poses or object appearances, where performance degrades despite the benefits of learning at scale. This stems from BC's tendency to overfit individual demonstrations rather than capture shared structure, limiting generalization. To address this, we introduce Contrastive Learning via Action Sequence Supervision (CLASS), a method for learning behavioral representations from demonstrations using supervised contrastive learning. CLASS leverages weak supervision from similar action sequences identified via Dynamic Time Warping (DTW) and optimizes a soft InfoNCE loss with similarity-weighted positive pairs. We evaluate CLASS on 5 simulation benchmarks and 3 real-world tasks to achieve competitive results using retrieval-based control with representations only. Most notably, for downstream policy learning under significant visual shifts, CLASS achieves an average success rate of 70% with Diffusion Policy, while all other baseline methods fail to perform competitively.",
        "keywords": "Supervised Contrastive Learning;Imitation Learning;Robot Manipulation;Action Chunking",
        "primary_area": "",
        "supplementary_material": "/attachment/e2eaabefe9cb350c33f47c635c2ec395a9b3711b.zip",
        "author": "Sung-Wook Lee;Xuhui Kang;Brandon Y. Yang;Yen-Ling Kuo",
        "authorids": "~Sung-Wook_Lee1;~Xuhui_Kang1;~Brandon_Y._Yang1;~Yen-Ling_Kuo1",
        "gender": "M;M;M;F",
        "homepage": "https://seansungwooklee.github.io/;;https://www.brandonyifanyang.com/;http://yenlingkuo.com",
        "dblp": ";354/8655.html;;120/3172",
        "google_scholar": "Z9oFHVsAAAAJ;sIY8FtgAAAAJ;;pNkyRs4AAAAJ",
        "orcid": "0009-0005-5309-480X;;;",
        "linkedin": ";xuhui-joshua-kang-44314317b/;;",
        "or_profile": "~Sung-Wook_Lee1;~Xuhui_Kang1;~Brandon_Y._Yang1;~Yen-Ling_Kuo1",
        "aff": "University of Virginia, Charlottesville;University of Virginia, Charlottesville;University of Virginia, Charlottesville;University of Virginia, Charlottesville",
        "aff_domain": "virginia.edu;virginia.edu;virginia.edu;virginia.edu",
        "position": "PhD student;PhD student;Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\nlee2025class,\ntitle={{CLASS}: Contrastive Learning via Action Sequence Supervision for Robot Manipulation},\nauthor={Sung-Wook Lee and Xuhui Kang and Brandon Y. Yang and Yen-Ling Kuo},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=9f3klkpa4y}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9f3klkpa4y",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Virginia",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.virginia.edu",
        "aff_unique_abbr": "UVA",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Charlottesville",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "9lCTcsmZMV",
        "title": "Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Achieving coordinated teamwork among legged robots requires both fine-grained locomotion control and long-horizon strategic decision-making. Robot soccer offers a compelling testbed for this challenge, combining dynamic, competitive, and multi-agent interactions. In this work, we present a hierarchical multi-agent reinforcement learning (MARL) framework that enables fully autonomous and decentralized quadruped robot soccer. First, a set of highly dynamic low-level skills is trained for legged locomotion and ball manipulation, such as walking, dribbling, and kicking. On top of these, a high-level strategic planning policy is trained with Multi-Agent Proximal Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning framework allows agents to adapt to diverse opponent strategies and gives rise to sophisticated team behaviors, including coordinated passing, interception, and dynamic role allocation. With an extensive ablation study, the proposed learning method shows significant advantages in the cooperative and competitive multi-agent soccer game. We deploy the learned policies to real quadruped robots relying solely on onboard proprioception and decentralized localization, with the resulting system supporting autonomous robot-robot and robot-human soccer matches on indoor and outdoor soccer courts.",
        "keywords": "Robot Soccer;Multi-Agent Reinforcement Learning;Legged Robots",
        "primary_area": "",
        "supplementary_material": "/attachment/c7e9dd28c008c5b2f0ce4590ec1c74125c95b669.zip",
        "author": "Zhi Su;Yuman Gao;Emily Lukas;Yunfei Li;Jiaze Cai;Faris Tulbah;Fei Gao;Chao Yu;Zhongyu Li;Yi Wu;Koushil Sreenath",
        "authorids": "~Zhi_Su1;~Yuman_Gao1;~Emily_Lukas1;~Yunfei_Li1;~Jiaze_Cai1;~Faris_Tulbah1;~Fei_Gao16;~Chao_Yu1;~Zhongyu_Li3;~Yi_Wu1;~Koushil_Sreenath1",
        "gender": "M;M;F;;M;;M;F;M;M;M",
        "homepage": "https://suz-tsinghua.github.io/;;https://lukasep.alphaxiv.io/;https://irisli17.github.io/;;;https://zju-fast.com/research-group/fei-gao/;http://zoeyuchao.github.io;;https://jxwuyi.weebly.com;",
        "dblp": ";;;;;;16/722-11;36/6789-5;;;",
        "google_scholar": "R5Y1xlUAAAAJ;nepkga0AAAAJ;;https://scholar.google.com/citations?hl=en;5AajeiYAAAAJ;https://scholar.google.com/citations?hl=en;4RObDv0AAAAJ;BYoq_bwAAAAJ;ouSpgSkAAAAJ;dusV5HMAAAAJ;o9aFV8cAAAAJ",
        "orcid": ";;;0000-0003-0988-9400;;;;0000-0001-6975-0158;;;",
        "linkedin": ";;emily-lukas-eng/;;;;;;;;",
        "or_profile": "~Zhi_Su1;~Yuman_Gao1;~Emily_Lukas1;~Yunfei_Li1;~Jiaze_Cai1;~Faris_Tulbah1;~Fei_Gao16;~Chao_Yu1;~Zhongyu_Li3;~Yi_Wu1;~Koushil_Sreenath1",
        "aff": "Tsinghua University;Zhejiang University of Technology;University of California, Berkeley;ByteDance Inc.+Institute for Interdisciplinary Information Sciences, Tsinghua University;University of California, Berkeley;University of California, Berkeley;Zhejiang University;Tsinghua University;;Tsinghua University;University of California, Berkeley",
        "aff_domain": "mails.tsinghua.edu.cn;zjut.edu.cn;berkeley.edu;bytedance.com+tsinghua.edu.cn;berkeley.edu;berkeley.edu;zju.edu.cn;mail.tsinghua.edu.cn;;tsinghua.edu.cn;berkeley.edu",
        "position": "Undergrad student;PhD student;PhD student;Researcher+PhD student;Researcher;PhD student;Associate Professor;Postdoc;;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nsu2025toward,\ntitle={Toward Real-World Cooperative and Competitive Soccer with Quadrupedal Robot Teams},\nauthor={Zhi Su and Yuman Gao and Emily Lukas and Yunfei Li and Jiaze Cai and Faris Talubah and Fei Gao and Chao Yu and Zhongyu Li and Yi Wu and Koushil Sreenath},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=9lCTcsmZMV}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9lCTcsmZMV",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3+0;2;2;4;0;5;0;2",
        "aff_unique_norm": "Tsinghua University;Zhejiang University of Technology;University of California, Berkeley;ByteDance;Zhejiang University;",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.zjut.edu.cn;https://www.berkeley.edu;https://www.bytedance.com;https://www.zju.edu.cn;",
        "aff_unique_abbr": "THU;ZJUT;UC Berkeley;ByteDance;ZJU;",
        "aff_campus_unique_index": "1;;1;1;1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "0;0;1;0+0;1;1;0;0;0;1",
        "aff_country_unique": "China;United States;"
    },
    {
        "id": "9uKL9FJBiz",
        "title": "Pseudo-Simulation for Autonomous Driving",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical limitations. Real-world evaluation is often challenging due to safety concerns and a lack of reproducibility, whereas closed-loop simulation can face insufficient realism or high computational costs. Open-loop evaluation, while being efficient and data-driven, relies on metrics that generally overlook compounding errors. In this paper, we propose pseudo-simulation, a novel paradigm that addresses these limitations. Pseudo-simulation operates on real datasets, similar to open-loop evaluation, but augments them with synthetic observations generated prior to evaluation using 3D Gaussian Splatting. Our key idea is to approximate potential future states the AV might encounter by generating a diverse set of observations that vary in position, heading, and speed. Our method then assigns a higher importance to synthetic observations that best match the AV's likely behavior using a novel proximity-based weighting scheme. This enables evaluating error recovery and the mitigation of causal confusion, as in closed-loop benchmarks, without requiring sequential interactive simulation. We show that pseudo-simulation is better correlated with closed-loop simulations ($R^2=0.8$) than the best existing open-loop approach ($R^2=0.7$). We also establish a public leaderboard for the community to benchmark new methodologies with pseudo-simulation.",
        "keywords": "Simulation;Benchmarking;Autonomous Driving",
        "primary_area": "",
        "supplementary_material": "/attachment/8f62cec9556baca256fd283ec0c9b39a92cca704.pdf",
        "author": "Wei Cao;Marcel Hallgarten;Tianyu Li;Daniel Dauner;Xunjiang Gu;Caojun Wang;Yakov Miron;Marco Aiello;Hongyang Li;Igor Gilitschenski;Boris Ivanovic;Marco Pavone;Andreas Geiger;Kashyap Chitta",
        "authorids": "~Wei_Cao2;~Marcel_Hallgarten1;~Tianyu_Li5;~Daniel_Dauner1;~Xunjiang_Gu1;~Caojun_Wang1;~Yakov_Miron1;~Marco_Aiello2;~Hongyang_Li1;~Igor_Gilitschenski1;~Boris_Ivanovic1;~Marco_Pavone1;~Andreas_Geiger3;~Kashyap_Chitta1",
        "gender": "M;M;M;;M;M;M;M;M;M;;M;M;M",
        "homepage": "https://vveicao.github.io/;https://uni-tuebingen.de/fr/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/kognitive-systeme/the-chair/staff/marcel-hallgarten/;https://github.com/sephyli;https://danieldauner.github.io/;https://alfredgu001324.github.io/;;;http://aiellom.it;https://datascience.hku.hk/people/hongyang-li/;https://www.gilitschenski.org/igor;http://www.borisivanovic.com/;https://web.stanford.edu/~pavone/;http://www.cvlibs.net;https://kashyap7x.github.io/",
        "dblp": ";;;349/4864;;;;43/3220-1.html;95/8433-1;129/1281;203/8356;91/3382-1.html;40/5825-1;220/3765",
        "google_scholar": ";;X6vTmEMAAAAJ;tZqIYDcAAAAJ;https://scholar.google.ca/citations?user=5h3kvJsAAAAJ;35xHlDUAAAAJ;https://scholar.google.com/citations?hl=de;XvRUyU4AAAAJ;https://scholar.google.com.hk/citations?user=Hfrih1EAAAAJ;Nuw1Y4oAAAAJ;ey9AQcEAAAAJ;RhOpyXcAAAAJ;https://scholar.google.ca/citations?hl=en;vX5i2CcAAAAJ",
        "orcid": ";;0009-0008-3838-160X;;;0009-0006-0113-2373;;0000-0002-0764-2124;0000-0001-9110-5534;;0000-0002-8698-202X;;0000-0002-8151-3726;",
        "linkedin": "wei-cao-dave/;;sephy-li/;;alfred-gu-9902831a0?trk=contact-info;;;aiellom/;hongyangli2020/;igorgilitschenski/;boris-ivanovic-a3103064;;;",
        "or_profile": "~Wei_Cao2;~Marcel_Hallgarten1;~Tianyu_Li5;~Daniel_Dauner1;~Xunjiang_Gu1;~Caojun_Wang1;~Yakov_Miron1;~Marco_Aiello2;~Hongyang_Li1;~Igor_Gilitschenski1;~Boris_Ivanovic1;~Marco_Pavone1;~Andreas_Geiger3;~Kashyap_Chitta1",
        "aff": "University of Illinois, Urbana Champaign;Bosch+Eberhard-Karls-Universit\u00e4t T\u00fcbingen;Fudan University+Shanghai AI Laboratory;Eberhard-Karls-Universit\u00e4t T\u00fcbingen;University of Toronto;Tongji University;Bosch;Universit\u00e4t Stuttgart;University of Hong Kong;University of Toronto;NVIDIA;NVIDIA+Stanford University;University of Tuebingen;NVIDIA+University of T\u00fcbingen",
        "aff_domain": "illinois.edu;bosch.de+uni-tuebingen.de;fudan.edu.cn+pjlab.org.cn;uni-tuebingen.de;utoronto.ca;tongji.edu.cn;bosch.com;uni-stuttgart.de;hku.hk;toronto.edu;nvidia.com;nvidia.com+stanford.edu;uni-tuebingen.de;nvidia.com+uni-tuebingen.de",
        "position": "PhD student;Researcher+PhD student;PhD student+Intern;PhD student;MS student;PhD student;Researcher;Full Professor;Assistant Professor;Assistant Professor;Researcher;Director, Autonomous Vehicle Research+Associate Professor;Professor;Postdoc+PhD student",
        "bibtex": "@inproceedings{\ncao2025pseudosimulation,\ntitle={Pseudo-Simulation for Autonomous Driving},\nauthor={Wei Cao and Marcel Hallgarten and Tianyu Li and Daniel Dauner and Xunjiang Gu and Caojun Wang and Yakov Miron and Marco Aiello and Hongyang Li and Igor Gilitschenski and Boris Ivanovic and Marco Pavone and Andreas Geiger and Kashyap Chitta},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=9uKL9FJBiz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=9uKL9FJBiz",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            14,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1+2;3+4;2;5;6;1;7;8;5;9;9+10;11;9+12",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;Robert Bosch GmbH;Eberhard Karls University of T\u00fcbingen;Fudan University;Shanghai AI Laboratory;University of Toronto;Tongji University;University of Stuttgart;University of Hong Kong;NVIDIA;Stanford University;University of Tuebingen;University of T\u00fcbingen",
        "aff_unique_dep": ";;;;;;;;;NVIDIA Corporation;;;",
        "aff_unique_url": "https://illinois.edu;https://www.bosch.com;https://www.uni-tuebingen.de/;https://www.fudan.edu.cn;https://www.shanghai-ai-lab.com;https://www.utoronto.ca;https://www.tongji.edu.cn;https://www.uni-stuttgart.de;https://www.hku.hk;https://www.nvidia.com;https://www.stanford.edu;https://www.uni-tuebingen.de/;https://www.uni-tuebingen.de/",
        "aff_unique_abbr": "UIUC;Bosch;Uni T\u00fcbingen;Fudan;SAIL;U of T;Tongji;Uni Stuttgart;HKU;NVIDIA;Stanford;Uni T\u00fcbingen;Uni T\u00fcbingen",
        "aff_campus_unique_index": "0;2;;2;3;4;",
        "aff_campus_unique": "Urbana-Champaign;;T\u00fcbingen;Hong Kong SAR;Stanford",
        "aff_country_unique_index": "0;1+1;2+2;1;3;2;1;1;2;3;0;0+0;1;0+1",
        "aff_country_unique": "United States;Germany;China;Canada"
    },
    {
        "id": "AE299O0tph",
        "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harness VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, out performing the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs.",
        "keywords": "Vision-Language Models;Zero-shot UAV Navigation;2D-to-3D;Waypoint Prompting",
        "primary_area": "",
        "supplementary_material": "/attachment/e2b95c2db33e527d94ce0a4772a5b543324d50ff.zip",
        "author": "Chih Yao Hu;Yang-Sen Lin;Yuna Lee;Chih-Hai Su;Jie-Ying Lee;Shr-Ruei Tsai;Chin-Yang Lin;Kuan-Wen Chen;Tsung-Wei Ke;Yu-Lun Liu",
        "authorids": "~Chih_Yao_Hu1;~Yang-Sen_Lin1;~Yuna_Lee1;~Chih-Hai_Su1;~Jie-Ying_Lee1;~Shr-Ruei_Tsai1;~Chin-Yang_Lin1;~Kuan-Wen_Chen2;~Tsung-Wei_Ke2;~Yu-Lun_Liu2",
        "gender": ";M;;M;M;M;M;M;;",
        "homepage": ";;;https://su-terry.github.io/;https://jayinnn.dev;;https://linjohnss.github.io;http://covis.cs.nctu.edu.tw/advisor.php;https://twke18.github.io/;",
        "dblp": ";;;379/3787;380/5180;;;59/968;173/4984;",
        "google_scholar": ";;;glkS-7AAAAAJ;mKB6voEAAAAJ;;5hYgWcwAAAAJ;2kfjkmEAAAAJ;WTEFsHMAAAAJ;",
        "orcid": ";;;0009-0002-8500-9161;0009-0008-0826-4664;0009-0002-2707-0095;;0000-0002-4159-201X;;",
        "linkedin": ";yang-sen-lin-3242452b2/;;ohinic/;jayinnn/;;chin-yang-lin;;;",
        "or_profile": "~Chih_Yao_Hu1;~Yang-Sen_Lin1;~Yuna_Lee1;~Chih-Hai_Su1;~Jie-Ying_Lee1;~Shr-Ruei_Tsai1;~Chin-Yang_Lin1;~Kuan-Wen_Chen2;~Tsung-Wei_Ke2;~Yu-Lun_Liu2",
        "aff": ";National Yang Ming Chiao Tung University;;National Yang Ming Chiao Tung University+National Yang Ming Chiao Tung University;ETHZ - ETH Zurich+National Yang Ming Chiao Tung University;National Yang Ming Chiao Tung University;National Yang Ming Chiao Tung University;National Yang Ming Chiao Tung University;Department of computer science and informational engineering, National Taiwan University+Carnegie Mellon University;",
        "aff_domain": ";nycu.edu.tw;;nycu.edu.tw+nycu.edu.tw;ethz.ch+nycu.edu.tw;nycu.edu.tw;nycu.edu.tw;nycu.edu.tw;csie.ntu.edu.tw+andrew.cmu.edu;",
        "position": ";Undergrad student;;PhD student+Undergrad student;Undergrad student+Undergrad student;Undergrad student;PhD student;Associate Professor;Assistant Professor+Postdoc;",
        "bibtex": "@inproceedings{\nhu2025see,\ntitle={See, Point, Fly: A Learning-Free {VLM} Framework for Universal Unmanned Aerial Navigation},\nauthor={Chih Yao Hu and Yang-Sen Lin and Yuna Lee and Chih-Hai Su and Jie-Ying Lee and Shr-Ruei Tsai and Chin-Yang Lin and Kuan-Wen Chen and Tsung-Wei Ke and Yu-Lun Liu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=AE299O0tph}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=AE299O0tph",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;1+1;2+1;1;1;1;3+4;0",
        "aff_unique_norm": ";National Yang Ming Chiao Tung University;ETH Zurich;National Taiwan University;Carnegie Mellon University",
        "aff_unique_dep": ";;;Department of Computer Science and Informational Engineering;",
        "aff_unique_url": ";https://www.nycu.edu.tw;https://www.ethz.ch;https://www.ntu.edu.tw;https://www.cmu.edu",
        "aff_unique_abbr": ";NYCU;ETHZ;NTU;CMU",
        "aff_campus_unique_index": "1;1+1;1;1;1;1;1",
        "aff_campus_unique": ";Taiwan",
        "aff_country_unique_index": "1;1+1;2+1;1;1;1;1+3",
        "aff_country_unique": ";China;Switzerland;United States"
    },
    {
        "id": "AO0BKxf3ss",
        "title": "Diffusion-Guided Multi-Arm Motion Planning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Multi-arm motion planning is fundamental for enabling arms to complete collaborative tasks in shared spaces but current methods struggle with scalability due to exponential state-space growth and reliance on large training datasets for learned models. Inspired by Multi-Agent Path Finding (MAPF), which decomposes planning into single-agent problems coupled with collision resolution, we propose a novel diffusion-guided multi-arm planner (DG-MAP) that enhances scalability of learning-based models while reducing their reliance on massive multi-arm datasets. Recognizing that collisions are primarily pairwise, we train two conditional diffusion models, one to generate feasible single-arm trajectories, and a second, to model the dual-arm dynamics required for effective pairwise collision resolution. By integrating these specialized generative models within a MAPF-inspired structured decomposition, our planner efficiently scales to larger number of arms. Evaluations against alternative learning-based methods across various team sizes demonstrate our method's effectiveness and practical applicability. Code and data will be made publicly available. View video demonstrations in our supplementary material.",
        "keywords": "multi-agent;planning;diffusion",
        "primary_area": "",
        "supplementary_material": "/attachment/9d16a8adb3dd2ab219f455d243bec744a55fea01.zip",
        "author": "Viraj Parimi;Brian C. Williams",
        "authorids": "~Viraj_Parimi2;~Brian_C._Williams1",
        "gender": ";",
        "homepage": ";",
        "dblp": ";",
        "google_scholar": ";",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Viraj_Parimi2;~Brian_C._Williams1",
        "aff": ";",
        "aff_domain": ";",
        "position": ";",
        "bibtex": "@inproceedings{\nparimi2025diffusionguided,\ntitle={Diffusion-Guided Multi-Arm Motion Planning},\nauthor={Viraj Parimi and Brian C. Williams},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=AO0BKxf3ss}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=AO0BKxf3ss",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "AP7kM1xk2a",
        "title": "Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Recent breakthroughs in large language models (LLMs) have not only advanced natural language processing but also inspired their application in domains with structurally similar problems\u2014most notably, autonomous driving motion generation. Both domains involve autoregressive sequence modeling, token-based representations, and context-aware decision making, making the transfer of LLM components a natural and increasingly common practice. However, despite promising early attempts, a systematic understanding of which LLM modules are truly transferable remains lacking. In this paper, we present a comprehensive evaluation of five key LLM modules\u2014tokenizer design, positional embedding, pre-training paradigms, post-training strategies, and test-time computation\u2014within the context of motion generation for autonomous driving. Through extensive experiments on the Waymo Sim Agents benchmark, we demonstrate that, when appropriately adapted, these modules can significantly improve performance for autonomous driving motion generation. In addition, we identify which techniques can be effectively transferred, analyze the potential reasons for the failure of others, and discuss the specific adaptations needed for autonomous driving scenarios. We evaluate our method on the Sim Agents task and achieve competitive results.",
        "keywords": "Large Language Model;Autonomous Driving;Motion Generation;Sim Agent",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mingyi Wang;Jingke Wang;Tengju Ye;Kaicheng Yu",
        "authorids": "~Mingyi_Wang1;~Jingke_Wang1;~Tengju_Ye1;~Kaicheng_Yu1",
        "gender": ";;M;M",
        "homepage": ";;;https://www.yukaicheng.cn",
        "dblp": ";262/1535;160/9533;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;;j9OguiIAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Mingyi_Wang1;~Jingke_Wang1;~Tengju_Ye1;~Kaicheng_Yu1",
        "aff": "Zhejiang University;Udeer.AI;UDEER AI PTE.LTD;KMind.AI+Westlake University",
        "aff_domain": "zju.edu.cn;udeer.ai;udeer.ai;kmind.com+westlake.edu",
        "position": "MS student;Researcher;Researcher;Principal Researcher+Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025do,\ntitle={Do {LLM} Modules Generalize? A Study on Motion Generation for Autonomous Driving},\nauthor={Mingyi Wang and Jingke Wang and Tengju Ye and Kaicheng Yu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=AP7kM1xk2a}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=AP7kM1xk2a",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3+4",
        "aff_unique_norm": "Zhejiang University;Udeer.ai;UDEER AI PTE.LTD;KMind.AI;Westlake University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.zju.edu.cn;;;;https://www.westlake.edu.cn",
        "aff_unique_abbr": "ZJU;;;;WU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;2;0",
        "aff_country_unique": "China;;Singapore"
    },
    {
        "id": "AVDCwK1dek",
        "title": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Balance control is important for human and bipedal robotic systems. While dynamic balance during locomotion has received considerable attention, quantitative understanding of static balance and falling remains limited. This work presents a hierarchical control pipeline for simulating human balance via a comprehensive whole-body musculoskeletal system. We identified spatiotemporal dynamics of balancing during stable standing, revealed the impact of muscle injury on balancing behavior, and generated fall contact patterns that aligned with clinical data. Furthermore, our simulated hip exoskeleton assistance demonstrated improvement in balance maintenance and reduced muscle effort under perturbation. This work offers unique muscle-level insights into human balance dynamics that are challenging to capture experimentally. It could provide a foundation for developing targeted interventions for individuals with balance impairments and support the advancement of humanoid robotic systems.",
        "keywords": "balance control;bipedal standing and falling;musculoskeletal system",
        "primary_area": "",
        "supplementary_material": "/attachment/c75baca724e3d407d2b379e8dd902076d8d5c3a6.zip",
        "author": "Chengtian Ma;Yunyue Wei;Chenhui Zuo;Chen Zhang;Yanan Sui",
        "authorids": "~Chengtian_Ma1;~Yunyue_Wei1;~Chenhui_Zuo1;~Chen_Zhang14;~Yanan_Sui1",
        "gender": "F;M;M;F;M",
        "homepage": "https://github.com/chengtianma;https://github.com/yunyuewei;https://github.com/zchJo;;https://www.yanansui.com",
        "dblp": ";;;;151/6934",
        "google_scholar": ";7OUDlegAAAAJ;;;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;;",
        "linkedin": ";;;https://www.linkedin.cn/incareer/in/chen-zhang-02606270;",
        "or_profile": "~Chengtian_Ma1;~Yunyue_Wei1;~Chenhui_Zuo1;~Chen_Zhang14;~Yanan_Sui1",
        "aff": "Tsinghua University;Tsinghua University+Tsinghua University;;;Tsinghua University",
        "aff_domain": "mails.tsinghua.edu.cn;tsinghua.edu.cn+tsinghua.edu.cn;;;tsinghua.edu.cn",
        "position": "MS student;Postdoc+PhD student;;;Associate Professor",
        "bibtex": "@inproceedings{\nma2025bipedal,\ntitle={Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations},\nauthor={Chengtian Ma and Yunyue Wei and Chenhui Zuo and Chen Zhang and Yanan Sui},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=AVDCwK1dek}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=AVDCwK1dek",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0+0;1;1;0",
        "aff_unique_norm": "Tsinghua University;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tsinghua.edu.cn;",
        "aff_unique_abbr": "THU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "Af2RMaWRjm",
        "title": "Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In the field of robot learning, it is becoming possible to coordinate robot action through language instructions. On the other hand, it is still a difficult task to adjust the action based on human instructions because human instructions are often qualitative, and there are cases where there is no one-to-one correspondence between the behavior and the instructions. In this paper, we propose a motion generation model that can adjust actions in response to qualitative human instructions during task execution. The core of the proposed method is a learning architecture that maps qualitative human instructions to actions. Specifically, the demonstration is divided into short action sequences, and labels reflecting human qualitative senses are assigned to these sequences to realize learning that links human qualitative instructions and robot actions. In evaluation experiments, we verified the effectiveness of the method in two tasks: a pick-and-place task and a wiping task. Experimental results showed that the proposed method is able to generate motions in response to human qualitative instructions during task execution, whereas the conventional method generates trajectories all at once, making it impossible to adjust motions during task execution.",
        "keywords": "Imitation learning;Disentangled representation learning",
        "primary_area": "",
        "supplementary_material": "/attachment/37aa45869b773b06c1ec8b9c5fc0c1962661a6bc.zip",
        "author": "Ryoga Oishi;Sho Sakaino;Toshiaki Tsuji",
        "authorids": "~Ryoga_Oishi1;sakaino@iit.tsukuba.ac.jp;~Toshiaki_Tsuji1",
        "gender": "M;;M",
        "homepage": ";;",
        "dblp": ";;",
        "google_scholar": ";;https://scholar.google.co.jp/citations?user=hfQcAzMAAAAJ",
        "orcid": "0009-0003-2297-819X;;",
        "linkedin": ";;",
        "or_profile": "~Ryoga_Oishi1;sakaino@iit.tsukuba.ac.jp;~Toshiaki_Tsuji1",
        "aff": "Saitama University;;Saitama University",
        "aff_domain": "saitama-u.ac.jp;;saitama-u.ac.jp",
        "position": "PhD student;;Associate Professor",
        "bibtex": "@inproceedings{\noishi2025imitation,\ntitle={Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics},\nauthor={Ryoga Oishi and Sho Sakaino and Toshiaki Tsuji},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Af2RMaWRjm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Af2RMaWRjm",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Saitama University;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.saitama-u.ac.jp;",
        "aff_unique_abbr": "Saitama U;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Japan;"
    },
    {
        "id": "B6knAJsB9P",
        "title": "COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In this work, we study the problem of data retrieval for few-shot imitation learning: select data from a large dataset to train a performant policy for a specific task, given only a few target demonstrations. Prior methods retrieve data using a single-feature distance heuristic, assuming that the best demonstrations are those that most closely resemble the target examples in visual, semantic, or motion space. However, this approach captures only a subset of the relevant information and is prone to introducing detrimental demonstrations, e.g., retrieving data from unrelated tasks due to similar scene layouts, or selecting similar motions from tasks with divergent goals. We present COLLAGE, a method for COLLective data AGgrEgation in few-shot imitation learning that uses an adaptive late fusion mechanism to guide the selection of relevant demonstrations based on a task12 specific combination of multiple cues. COLLAGE follows a simple, but flexible and efficient data aggregation recipe: it assigns weights to subsets of the dataset that are pre-selected using a single feature (e.g., appearance, shape, or language similarity), based on their task relevance, measured by how well a policy trained on each subset predicts actions in the few target demonstrations. These weights are then used during policy training to perform importance sampling over the aggregated dataset, sampling data more densely or sparsely, according to their estimated relevance. This weighted aggregation strategy is general and feature-agnostic, allowing COLLAGE to combine and leverage any number of subsets selected by any retrieval heuristic or method, and to identify which subset provides the most benefit for the target task. In extensive experiments, COLLAGE outperforms state-of-the-art retrieval and multi-task learning approaches, achieving a 5.1% improvement over the best baseline in simulation across 10 tasks, and a 16.6% improvement in the real world across 6 tasks. For our real world experiments, we include data selection from the large-scale, real-world DROID dataset, significantly improving few-shot imitation policy training. More information at: https://collagecorl25.github.io/",
        "keywords": "Data Retrieval;Few-shot Learning;Imitation Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sateesh Kumar;Shivin Dass;Georgios Pavlakos;Roberto Mart\u00edn-Mart\u00edn",
        "authorids": "~Sateesh_Kumar2;~Shivin_Dass2;~Georgios_Pavlakos1;~Roberto_Mart\u00edn-Mart\u00edn1",
        "gender": "M;;M;M",
        "homepage": ";;https://geopavlakos.github.io/;https://robertomartinmartin.com/",
        "dblp": "253/0475;;145/3361;153/7670",
        "google_scholar": "6CWng3MAAAAJ;;iH2BZ8UAAAAJ;XOJE8OEAAAAJ",
        "orcid": ";;;0000-0002-9586-2759",
        "linkedin": ";;;",
        "or_profile": "~Sateesh_Kumar2;~Shivin_Dass2;~Georgios_Pavlakos1;~Roberto_Mart\u00edn-Mart\u00edn1",
        "aff": ", University of Texas at Austin;;University of Texas at Austin;Amazon+University of Texas at Austin",
        "aff_domain": "cs.utexas.edu;;cs.utexas.edu;amazon.com+utexas.edu",
        "position": "PhD student;;Assistant Professor;Researcher+Assistant Professor",
        "bibtex": "@inproceedings{\nkumar2025collage,\ntitle={{COLLAGE}: Adaptive Fusion-based Retrieval for Augmented Policy Learning},\nauthor={Sateesh Kumar and Shivin Dass and Georgios Pavlakos and Roberto Mart{\\'\\i}n-Mart{\\'\\i}n},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=B6knAJsB9P}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=B6knAJsB9P",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;2+0",
        "aff_unique_norm": "University of Texas at Austin;;Amazon",
        "aff_unique_dep": ";;Amazon.com, Inc.",
        "aff_unique_url": "https://www.utexas.edu;;https://www.amazon.com",
        "aff_unique_abbr": "UT Austin;;Amazon",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "BNCh3SS1Yl",
        "title": "Articulate AnyMesh: Open-vocabulary 3D Articulated Objects Modeling",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "3D articulated objects modeling has long been a challenging problem, since it requires to capture both accurate surface geometries and semantically meaningful and spatially precise structures, parts, and joints. Existing methods heavily depend on training data from a limited set of handcrafted articulated object categories (e.g., cabinets and drawers), which restricts their ability to model a wide range of articulated objects in an open-vocabulary context. To address these limitations, we propose Articulate AnyMesh, an automated framework that is able to convert any rigid 3D mesh into its articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our framework utilizes advanced Vision-Language Models and visual prompting techniques to extract semantic information, allowing for both the segmentation of object parts and the construction of functional joints. Our experiments show that Articulate AnyMesh can generate large-scale, high-quality 3D articulated objects, including tools, toys, mechanical devices, and vehicles, significantly expanding the coverage of existing 3D articulated object datasets. Additionally, we show that these generated assets can facilitate the acquisition of new articulated object manipulation skills in simulation, which can then be transferred to a real robotic system.",
        "keywords": "Articulated objects;visual prompting;URDF prediction",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xiaowen Qiu;Jincheng Yang;Yian Wang;Zhehuan Chen;Yufei Wang;Tsun-Hsuan Wang;Zhou Xian;Chuang Gan",
        "authorids": "~Xiaowen_Qiu1;~Jincheng_Yang2;~Yian_Wang1;~Zhehuan_Chen1;~Yufei_Wang4;~Tsun-Hsuan_Wang2;~Zhou_Xian1;~Chuang_Gan1",
        "gender": "M;M;;M;;M;M;M",
        "homepage": "http://None;https://github.com/Yang-Chincheng;;https://www.cnblogs.com/ACMLCZH;https://yufeiwang63.github.io/;https://zswang666.github.io/;;http://people.csail.mit.edu/ganchuang/",
        "dblp": ";;;;;217/1809.html;258/5020;139/6993",
        "google_scholar": ";;;LvNUzlEAAAAJ;HQl9718AAAAJ;xE3WSuYAAAAJ;;PTeSCbIAAAAJ",
        "orcid": ";;;;;;;",
        "linkedin": ";;;;;;;",
        "or_profile": "~Xiaowen_Qiu1;~Jincheng_Yang2;~Yian_Wang1;~Zhehuan_Chen1;~Yufei_Wang4;~Tsun-Hsuan_Wang2;~Zhou_Xian1;~Chuang_Gan1",
        "aff": "University of Massachusetts at Amherst;Shanghai Jiaotong University;;University of Massachusetts at Amherst;School of Computer Science, Carnegie Mellon University;Massachusetts Institute of Technology;Carnegie Mellon University;University of Massachusetts at Amherst",
        "aff_domain": "umass.edu;sjtu.edu.cn;;umass.edu;cs.cmu.edu;mit.edu;cmu.edu;umass.edu",
        "position": "MS student;Undergrad student;;MS student;PhD student;PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nqiu2025articulate,\ntitle={Articulate AnyMesh: Open-vocabulary 3D Articulated Objects Modeling},\nauthor={Xiaowen Qiu and Jincheng Yang and Yian Wang and Zhehuan Chen and Yufei Wang and Tsun-Hsuan Wang and Zhou Xian and Chuang Gan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=BNCh3SS1Yl}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BNCh3SS1Yl",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;0;3;4;3;0",
        "aff_unique_norm": "University of Massachusetts Amherst;Shanghai Jiao Tong University;;Carnegie Mellon University;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;;School of Computer Science;",
        "aff_unique_url": "https://www.umass.edu;https://www.sjtu.edu.cn;;https://www.cmu.edu;https://web.mit.edu",
        "aff_unique_abbr": "UMass Amherst;SJTU;;CMU;MIT",
        "aff_campus_unique_index": "0;0;2;0",
        "aff_campus_unique": "Amherst;;Pittsburgh",
        "aff_country_unique_index": "0;1;0;0;0;0;0",
        "aff_country_unique": "United States;China;"
    },
    {
        "id": "BNdgT6GeC6",
        "title": "Granular loco-manipulation: Repositioning rocks through strategic sand avalanche",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Legged robots have the potential to leverage obstacles to climb steep sand slopes. However, efficiently repositioning these obstacles to desired locations is challenging. Here we present DiffusiveGRAIN, a learning-based method that enables a multi-legged robot to strategically induce localized sand avalanches during locomotion and indirectly manipulate obstacles. We conducted 375 trials, systematically varying obstacle spacing, robot orientation, and leg actions in 75 of them. Results show that movement of closely-spaced obstacles exhibit significant interference, requiring joint modeling. In addition, different multi-leg excavation actions could cause distinct robot state changes, necessitating integrated planning of manipulation and locomotion. To address these challenges, DiffusiveGRAIN includes a diffusion-based environment predictor to capture multi-obstacle movements under granular flow interferences and a robot state predictor to estimate changes in robot state from multi-leg action patterns. Deployment experiments (90 trials) demonstrate that by integrating the environment and robot state predictors, the robot can autonomously plan its movements based on loco-manipulation goals, successfully shifting closely located rocks to desired locations in over 65% of trials. Our study showcases the potential for a locomoting robot to strategically manipulate obstacles to achieve improved mobility on challenging terrains.",
        "keywords": "Granular media;avalanche dynamics;diffusion models;legged robots",
        "primary_area": "",
        "supplementary_material": "/attachment/d6d9cd5157faeb334fa850288e6b57a94f646c55.zip",
        "author": "Haodi Hu;Yue Wu;Daniel Seita;Feifei Qian",
        "authorids": "~Haodi_Hu1;~Yue_Wu46;~Daniel_Seita1;~Feifei_Qian1",
        "gender": "M;M;;F",
        "homepage": "https://sites.google.com/view/haodihu;;;https://viterbi.usc.edu/directory/faculty/Qian/Feifei",
        "dblp": "227/7639;;;",
        "google_scholar": "Ez_AALsAAAAJ;;;SqYmRh0AAAAJ",
        "orcid": ";;;",
        "linkedin": ";yue-wu-jerry/;;",
        "or_profile": "~Haodi_Hu1;~Yue_Wu46;~Daniel_Seita1;~Feifei_Qian1",
        "aff": "University of Southern California;University of Southern California;;University of Southern California",
        "aff_domain": "usc.edu;usc.edu;;usc.edu",
        "position": "PhD student;Undergrad student;;Assistant Professor",
        "bibtex": "@inproceedings{\nhu2025granular,\ntitle={Granular loco-manipulation: Repositioning rocks through strategic sand avalanche},\nauthor={Haodi Hu and Yue Wu and Daniel Seita and Feifei Qian},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=BNdgT6GeC6}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BNdgT6GeC6",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "University of Southern California;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.usc.edu;",
        "aff_unique_abbr": "USC;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "BO7qo66YJ2",
        "title": "X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Sim introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30\\% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection, and (3) generalizes to new camera viewpoints and test-time changes.",
        "keywords": "Learning from Human Videos;Sim-to-Real;Representation Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/657abd2e638eebca34701098844a3757b77ab236.zip",
        "author": "Prithwish Dan;Kushal Kedia;Angela Chao;Edward Duan;Maximus Adrian Pace;Wei-Chiu Ma;Sanjiban Choudhury",
        "authorids": "~Prithwish_Dan1;~Kushal_Kedia1;~Angela_Chao1;~Edward_Duan1;~Maximus_Adrian_Pace1;~Wei-Chiu_Ma1;~Sanjiban_Choudhury3",
        "gender": "M;M;F;M;M;M;M",
        "homepage": "https://portfolio-pdan101.vercel.app/;https://kushal2000.github.io/;https://github.com/angelac345;;https://maxpace1.github.io;https://www.cs.cornell.edu/~weichiu/;https://www.sanjibanchoudhury.com/",
        "dblp": ";;;;;151/4277;135/8207",
        "google_scholar": ";;;;;SVIdh6AAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;;;;",
        "linkedin": "prithwish-dan/;;angela-chao-b347b217a;edward-duan/;maximuspace/;;",
        "or_profile": "~Prithwish_Dan1;~Kushal_Kedia1;~Angela_Chao1;~Edward_Duan1;~Maximus_Adrian_Pace1;~Wei-Chiu_Ma1;~Sanjiban_Choudhury3",
        "aff": "Department of Computer Science, Cornell University;Cornell University;;Department of Computer Science, Cornell University;;Cornell University;Cornell University",
        "aff_domain": "cs.cornell.edu;cornell.edu;;cs.cornell.edu;;cornell.edu;cornell.edu",
        "position": "MS student;PhD student;;Undergrad student;;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ndan2025xsim,\ntitle={X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real},\nauthor={Prithwish Dan and Kushal Kedia and Angela Chao and Edward Duan and Maximus Adrian Pace and Wei-Chiu Ma and Sanjiban Choudhury},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=BO7qo66YJ2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BO7qo66YJ2",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;1;0;0",
        "aff_unique_norm": "Cornell University;",
        "aff_unique_dep": "Department of Computer Science;",
        "aff_unique_url": "https://www.cornell.edu;",
        "aff_unique_abbr": "Cornell;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "BTUioBmCWo",
        "title": "Phantom: Training Robots Without Robots Using Only Human Videos",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Training general-purpose robots requires learning from large and diverse data sources. Current approaches rely heavily on teleoperated demonstrations which are difficult to scale. We present a scalable framework for training manipulation policies directly from human video demonstrations, requiring no robot data. Our method converts human demonstrations into robot-compatible observation-action pairs using hand pose estimation and visual data editing. We inpaint the human arm and overlay a rendered robot to align the visual domains. This enables zero-shot deployment on real hardware without any fine-tuning. We demonstrate strong success rates\u2014up to 92%\u2014on a range of tasks including deformable object manipulation, multi-object sweeping, and insertion. Our approach generalizes to novel environments and supports closed-loop execution. By demonstrating that effective policies can be trained using only human videos, our method broadens the path to scalable robot learning. Videos are available at https://phantom-training-robots.github.io.",
        "keywords": "Learning from Human Videos;Imitation Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Marion Lepert;Jiaying Fang;Jeannette Bohg",
        "authorids": "~Marion_Lepert1;~Jiaying_Fang1;~Jeannette_Bohg1",
        "gender": "F;F;",
        "homepage": "https://marionlepert.github.io/;https://jiayingfang.github.io/;https://web.stanford.edu/~bohg/",
        "dblp": "303/0652;;52/7377",
        "google_scholar": "Wp5ZuXgAAAAJ;r_s-btUAAAAJ;rjnJnEkAAAAJ",
        "orcid": ";;0000-0002-4921-7193",
        "linkedin": ";jiaying-fang-554b691b8/;",
        "or_profile": "~Marion_Lepert1;~Jiaying_Fang1;~Jeannette_Bohg1",
        "aff": "Stanford University;Cornell University+Stanford University;Stanford University",
        "aff_domain": "stanford.edu;cornell.edu+stanford.edu;stanford.edu",
        "position": "PhD student;PhD student+MS student;Assistant Professor",
        "bibtex": "@inproceedings{\nlepert2025phantom,\ntitle={Phantom: Training Robots Without Robots Using Only Human Videos},\nauthor={Marion Lepert and Jiaying Fang and Jeannette Bohg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=BTUioBmCWo}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BTUioBmCWo",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1+0;0",
        "aff_unique_norm": "Stanford University;Cornell University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.cornell.edu",
        "aff_unique_abbr": "Stanford;Cornell",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "BcJlmjF1vV",
        "title": "HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Effective policy learning for robotic manipulation requires scene representations that selectively capture task-relevant environmental features. Current approaches typically employ task-agnostic representation extraction, failing to emulate the dynamic perceptual adaptation observed in human cognition. We present HyperTASR, a hypernetwork-driven framework that modulates scene representations based on both task objectives and the execution phase. Our architecture dynamically generates representation transformation parameters conditioned on task specifications and progression state, enabling representations to evolve contextually throughout task execution. This approach maintains architectural compatibility with existing policy learning frameworks while fundamentally reconfiguring how visual features are processed. Unlike methods that simply concatenate or fuse task embeddings with task-agnostic representations, HyperTASR establishes computational separation between task-contextual and state-dependent processing paths, enhancing learning efficiency and representational quality. Comprehensive evaluations in both simulation and real-world environments demonstrate substantial performance improvements across different representation paradigms. Most notably, HyperTASR elevates success rates by over 27\\% when applied to GNFactor and achieves unprecedented single-view performance exceeding 80\\% success with 3D Diffuser Actor. Through ablation studies and attention visualization, we confirm that our approach selectively prioritizes task-relevant scene information, closely mirroring human adaptive perception during manipulation tasks.",
        "keywords": "Representation learning;manipulation;hypernetworks",
        "primary_area": "",
        "supplementary_material": "/attachment/388e4342cf89272542c73a2a296986d65bea7d41.zip",
        "author": "Li Sun;Jiefeng Wu;Feng Chen;Ruizhe Liu;Yanchao Yang",
        "authorids": "~Li_Sun8;~Jiefeng_Wu1;~Feng_Chen16;~Ruizhe_Liu1;~Yanchao_Yang1",
        "gender": "M;M;M;M;M",
        "homepage": "https://lisunphil.github.io/;;https://winniechen2002.github.io/;;https://yanchaoyang.github.io/",
        "dblp": ";;;;84/8637-1",
        "google_scholar": "FCuoSiMAAAAJ;;xuVkkKwAAAAJ;https://scholar.google.com/citations?view_op=list_works;r2tKnV4AAAAJ",
        "orcid": ";0009-0002-0469-9384;;;",
        "linkedin": ";;https://linkedin.com/in/\u67ab-\u9648-822809265;;",
        "or_profile": "~Li_Sun8;~Jiefeng_Wu1;~Feng_Chen16;~Ruizhe_Liu1;~Yanchao_Yang1",
        "aff": "University of Hong Kong;University of Hong Kong+Zhejiang University;University of Hong Kong;University of Hong Kong;University of Hong Kong",
        "aff_domain": "hku.hk;hku.hk+zju.edu.cn;hku.hk;hku.hk;hku.hk",
        "position": "PhD student;PhD student+Undergrad student;PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nsun2025hypertasr,\ntitle={Hyper{TASR}: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation},\nauthor={Li Sun and Jiefeng Wu and Feng Chen and Ruizhe Liu and Yanchao Yang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=BcJlmjF1vV}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=BcJlmjF1vV",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0+1;0;0;0",
        "aff_unique_norm": "University of Hong Kong;Zhejiang University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.hku.hk;https://www.zju.edu.cn",
        "aff_unique_abbr": "HKU;ZJU",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0+0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "Bl2VfU9NhF",
        "title": "Hold My Beer: Learning Gentle Humanoid Locomotion and End-Effector Stabilization Control",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Can your humanoid walk up and hand you a full cup of beer\u2014without spilling a drop? While humanoids are increasingly featured in flashy demos\u2014dancing, delivering packages, traversing rough terrain\u2014fine-grained control during locomotion remains a significant challenge. In particular, stabilizing a filled end-effector (EE) while walking is far from solved, due to a fundamental mismatch in task dynamics: locomotion demands slow-timescale, robust control, whereas EE stabilization requires rapid, high-precision corrections. To address this, we propose SoFTA, a Slow-Fast Two-Agent framework that decouples upper-body and lower-body control into separate agents operating at different frequencies and with distinct rewards. This temporal and objective separation mitigates policy interference mitagates objective conflict and enables coordinated whole-body behavior. SoFTA executes upper-body actions at 100 Hz for precise EE control and lower-body actions at 50 Hz for robust gait. It reduces EE acceleration by 2-5x to baselines and performs 2\u20133x closer to human-level stability, enabling delicate tasks such as carrying nearly full cups, capturing steady video during locomotion, and disturbance rejection with EE stability.",
        "keywords": "Humanoid Robots;Reinforcement Learning;Stable Locomotion",
        "primary_area": "",
        "supplementary_material": "/attachment/ecfb0d2c0ce190c0d910180dbf56b4502d3b4195.zip",
        "author": "Yitang Li;Yuanhang Zhang;Wenli Xiao;Chaoyi Pan;Haoyang Weng;Guanqi He;Tairan He;Guanya Shi",
        "authorids": "~Yitang_Li1;~Yuanhang_Zhang3;~Wenli_Xiao1;~Chaoyi_Pan1;~Haoyang_Weng1;~Guanqi_He1;~Tairan_He1;~Guanya_Shi1",
        "gender": "F;M;M;M;;M;M;M",
        "homepage": "https://liyitang22.github.io/;https://hang0610.github.io/;https://wenlixiao-cs.github.io/;https://www.panchaoyi.com;;https://guanqihe.github.io/;https://tairanhe.com;http://guanyashi.github.io",
        "dblp": "380/7586;;;331/7271;;;263/2891.html;230/4386",
        "google_scholar": "5nqArY8AAAAJ;;https://scholar.google.com/citations?hl=en;lJNKzEMAAAAJ;;DH04ikwAAAAJ;TVWH2U8AAAAJ;joR1Z4UAAAAJ",
        "orcid": ";;;;;;;0000-0002-9075-3705",
        "linkedin": ";yuanhang-zhang-7969372a2/;wenli-xiao/;;;;tairan-he-41a904294/;guanya-shi-b07b43126/",
        "or_profile": "~Yitang_Li1;~Yuanhang_Zhang3;~Wenli_Xiao1;~Chaoyi_Pan1;~Haoyang_Weng1;~Guanqi_He1;~Tairan_He1;~Guanya_Shi1",
        "aff": "Tsinghua University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;;Carnegie Mellon University;NVIDIA+Carnegie Mellon University;Carnegie Mellon University",
        "aff_domain": "mails.tsinghua.edu.cn;cmu.edu;cmu.edu;cmu.edu;;cmu.edu;nvidia.com+andrew.cmu.edu;andrew.cmu.edu",
        "position": "Undergrad student;MS student;MS student;PhD student;;MS student;Intern+PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nli2025hold,\ntitle={Hold My Beer: Learning Gentle Humanoid Locomotion and End-Effector Stabilization Control},\nauthor={Yitang Li and Yuanhang Zhang and Wenli Xiao and Chaoyi Pan and Haoyang Weng and Guanqi He and Tairan He and Guanya Shi},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Bl2VfU9NhF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Bl2VfU9NhF",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;1;2;1;3+1;1",
        "aff_unique_norm": "Tsinghua University;Carnegie Mellon University;;NVIDIA",
        "aff_unique_dep": ";;;NVIDIA Corporation",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.cmu.edu;;https://www.nvidia.com",
        "aff_unique_abbr": "THU;CMU;;NVIDIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1;1+1;1",
        "aff_country_unique": "China;United States;"
    },
    {
        "id": "Bw9NHYjDqR",
        "title": "CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Humanoid robot teleoperation plays a vital role in demonstrating and collecting data for complex interactions. Current methods suffer from two key limitations: (1) restricted controllability due to decoupled upper- and lower-body control, and (2) severe drift caused by open-loop execution. These issues prevent humanoid robots from performing coordinated whole-body motions required for long-horizon loco-manipulation tasks. We introduce CLONE, a whole-body teleoperation system that overcomes these challenges through three key contributions: (1) a Mixture-of-Experts (MoE) whole-body control policy that enables complex coordinated movements, such as \u201cpicking up an object from the ground\u201d and \u201cplacing it in a distant bin\u201d; (2) a closed-loop error correction mechanism using LiDAR odometry, reducing translational drift to 12cm over 8.9-meter trajectories; and (3) a systematic data augmentation strategy that ensures robust performance under diverse, previously unseen operator poses. In extensive experiments, CLONE demonstrates robust performance across diverse scenarios while maintaining stable whole-body control. These capabilities significantly advance humanoid robotics by enabling the collection of long-horizon interaction data and establishing a foundation for more sophisticated humanoid-environment interaction in both research and practical applications.",
        "keywords": "Humanoid;Whole-body Control;Humanoid-Environment Interaction",
        "primary_area": "",
        "supplementary_material": "/attachment/f53530f6cd7e8ba79b96b38bb5083710a52c8210.pdf",
        "author": "Yixuan Li;Yutang Lin;Jieming Cui;Tengyu Liu;Wei Liang;Yixin Zhu;Siyuan Huang",
        "authorids": "~Yixuan_Li7;~Yutang_Lin2;~Jieming_Cui1;~Tengyu_Liu1;~Wei_Liang1;~Yixin_Zhu1;~Siyuan_Huang2",
        "gender": ";M;F;M;F;M;M",
        "homepage": "https://github.com/lyx-777;;https://jiemingcui.github.io/;https://tengyu.ai;https://liangwei-bit.github.io/web/;https://yzhu.io/;https://siyuanhuang.com/",
        "dblp": ";165/0916;336/7638;257/1450;;91/1103-1.html;62/885-1",
        "google_scholar": "https://scholar.google.cz/citations?user=xZm0IygAAAAJ;;;;3p6YfBEAAAAJ;qG9l6JEAAAAJ;1NN7Ee8AAAAJ",
        "orcid": "0009-0001-0650-4133;0009-0004-4933-1203;0000-0001-5189-7266;0000-0003-4006-1740;0000-0002-7539-3107;0000-0001-7024-1545;",
        "linkedin": ";;;;;;",
        "or_profile": "~Yixuan_Li7;~Yutang_Lin2;~Jieming_Cui1;~Tengyu_Liu1;~Wei_Liang1;~Yixin_Zhu1;~Siyuan_Huang2",
        "aff": "Beijing Institute of Technology;Peiking University;Peking University;Beijing Institute of General Artificial Intelligence;Beijing Institute of Technology;Peking University;Beijing Institute for General Artificial Intelligence",
        "aff_domain": "bit.edu.cn;stu.pku;pku.edu.cn;bigai.ai;bit.edu.cn;pku.edu.cn;bigai.ai",
        "position": "PhD student;Undergrad student;PhD student;Researcher;Full Professor;Assistant Professor;Researcher",
        "bibtex": "@inproceedings{\nli2025clone,\ntitle={{CLONE}: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks},\nauthor={Yixuan Li and Yutang Lin and Jieming Cui and Tengyu Liu and Wei Liang and Yixin Zhu and Siyuan Huang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Bw9NHYjDqR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Bw9NHYjDqR",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3;0;2;4",
        "aff_unique_norm": "Beijing Institute of Technology;Peiking University;Peking University;Beijing Institute of General Artificial Intelligence;Beijing Institute for General Artificial Intelligence",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "http://www.bit.edu.cn/;;http://www.pku.edu.cn;http://www.bigaiai.cn;http://www.bigaiai.org/",
        "aff_unique_abbr": "BIT;;Peking U;BIGAI;BIGAI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "C6VxzSpjrv",
        "title": "Visual Imitation Enables Contextual Humanoid Control",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably the simplest way is to _just show them_\u2014casually capture a human motion video and feed it to humanoids. We introduce **VideoMimic**, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills all from a single policy, conditioned on the environment and global root commands. We hope our data and approach help enable a scalable path towards teaching humanoids to operate in diverse real-world environments.",
        "keywords": "Visual Imitation;Humanoids;Reinforcement Learning;Reconstruction;Real2Sim2Real",
        "primary_area": "",
        "supplementary_material": "/attachment/f779d0a0e2faf429a711b1b31d22140a170ab009.zip",
        "author": "Arthur Allshire;Hongsuk Choi;Junyi Zhang;David McAllister;Anthony Zhang;Chung Min Kim;Trevor Darrell;Pieter Abbeel;Jitendra Malik;Angjoo Kanazawa",
        "authorids": "~Arthur_Allshire1;~Hongsuk_Choi1;~Junyi_Zhang3;~David_McAllister2;~Anthony_Zhang2;~Chung_Min_Kim1;~Trevor_Darrell2;~Pieter_Abbeel2;~Jitendra_Malik2;~Angjoo_Kanazawa1",
        "gender": ";M;M;M;M;;;M;M;F",
        "homepage": "https://allshire.org;https://hongsukchoi.github.io/;https://www.junyi42.com/;;https://antoniomacaronio.github.io/personal-website/;https://chungmin99.github.io/;;https://people.eecs.berkeley.edu/~pabbeel/;https://people.eecs.berkeley.edu/~malik/;https://people.eecs.berkeley.edu/~kanazawa/",
        "dblp": ";229/3689;00/1627-4;;;305/3515;;;58/2944;119/1305",
        "google_scholar": "https://scholar.google.ca/citations?user=TqsW7qMAAAAJ;CZbowncAAAAJ;LTi1tYsAAAAJ;;;ODr5lMgAAAAJ;;https://scholar.google.com.tw/citations?user=vtwH6GkAAAAJ;oY9R5YQAAAAJ;Ci-_QYIAAAAJ",
        "orcid": ";;0000-0002-9291-3098;;;;;;0000-0003-3695-1580;",
        "linkedin": ";;;davidrmcallister;antzhang/;;;;;",
        "or_profile": "~Arthur_Allshire1;~Hongsuk_Choi1;~Junyi_Zhang3;~David_McAllister2;~Anthony_Zhang2;~Chung_Min_Kim1;~Trevor_Darrell2;~Pieter_Abbeel2;~Jitendra_Malik2;~Angjoo_Kanazawa1",
        "aff": "University of California, Berkeley;University of California, Berkeley;University of California, Berkeley;University of California, Berkeley;University of California, Berkeley;University of California, Berkeley;;Amazon+University of California, Berkeley;Meta Facebook+University of California, Berkeley;University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;berkeley.edu;;amazon.com+berkeley.edu;fb.com+berkeley.edu;berkeley.edu",
        "position": "PhD student;PhD student;PhD student;PhD student;Undergrad student;PhD student;;Amazon Scholar+Professor;Director of Research+Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nallshire2025visual,\ntitle={Visual Imitation Enables Contextual Humanoid Control},\nauthor={Arthur Allshire and Hongsuk Choi and Junyi Zhang and David McAllister and Anthony Zhang and Chung Min Kim and Trevor Darrell and Pieter Abbeel and Jitendra Malik and Angjoo Kanazawa},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=C6VxzSpjrv}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=C6VxzSpjrv",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;0;0;1;2+0;3+0;0",
        "aff_unique_norm": "University of California, Berkeley;;Amazon;Meta",
        "aff_unique_dep": ";;Amazon.com, Inc.;Meta Platforms, Inc.",
        "aff_unique_url": "https://www.berkeley.edu;;https://www.amazon.com;https://meta.com",
        "aff_unique_abbr": "UC Berkeley;;Amazon;Meta",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0;0+0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "CBYGhryESq",
        "title": "MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Imitating tool manipulation from human videos offers an intuitive approach to teaching robots, while also providing a promising and scalable alternative to labor-intensive teleoperation data collection for visuomotor policy learning. While humans can mimic tool manipulation behavior by observing others perform a task just once and effortlessly transfer the skill to diverse tools for functionally equivalent tasks, current robots struggle to achieve this level of generalization. A key challenge lies in establishing function-level correspondences, considering the significant geometric variations among functionally similar tools, referred to as intra-function variations. To address this challenge, we propose MimicFunc, a framework that establishes functional correspondences with function frame, a function-centric local coordinate frame constructed with 3D functional keypoints, for imitating tool manipulation skills. Experiments demonstrate that MimicFunc effectively enables the robot to generalize the skill from a single RGB-D human video to manipulating novel tools for functionally equivalent tasks. Furthermore, leveraging MimicFunc's one-shot generalization capability, the generated rollouts can be used to train visuomotor policies without requiring labor-intensive teleoperation data collection for novel objects.",
        "keywords": "Tool Manipulation;Imitation from Human Video",
        "primary_area": "",
        "supplementary_material": "/attachment/12adad065cfd32a2d6ff552c9b50ecb4ac7a3d10.zip",
        "author": "Chao Tang;Anxing Xiao;Yuhong Deng;Tianrun Hu;Wenlong Dong;Hanbo Zhang;David Hsu;Hong Zhang",
        "authorids": "~Chao_Tang6;~Anxing_Xiao1;~Yuhong_Deng1;~Tianrun_Hu1;~Wenlong_Dong1;~Hanbo_Zhang1;~David_Hsu1;~Hong_Zhang2",
        "gender": "M;M;M;M;M;M;M;M",
        "homepage": "https://mkt1412.github.io/;https://anxingxiao.com;https://www.yuhongdeng.com/;https://h-tr.github.io/;;;http://www.comp.nus.edu.sg/~dyhsu/;https://rcvlab.eee.sustech.edu.cn",
        "dblp": ";272/5104;;;;119/1807;29/331;",
        "google_scholar": "hXGhWsUAAAAJ;qrgIuiEAAAAJ;KdN7CjEAAAAJ;o53_uPYAAAAJ;NNafpNQAAAAJ;1qfEEwsAAAAJ;S9LHLKEAAAAJ;https://scholar.google.ca/citations?user=J7UkpAIAAAAJ",
        "orcid": ";;;;;;0000-0002-2309-4535;",
        "linkedin": ";;;;;;david-hsu-a86200a1/;",
        "or_profile": "~Chao_Tang6;~Anxing_Xiao1;~Yuhong_Deng1;~Tianrun_Hu1;~Wenlong_Dong1;~Hanbo_Zhang1;~David_Hsu1;~Hong_Zhang2",
        "aff": "Southern University of Science and Technology;National University of Singapore;National University of Singapore;National University of Singapore;Southern University of Science and Technology;National University of Singapore;National University of Singapore;Southern University of Science and Technology",
        "aff_domain": "mail.sustech.edu.cn;nus.edu.sg;nus.edu.sg;nus.edu.sg;mail.sustech.edu.cn;nus.edu.sg;nus.edu.sg;sustech.edu.cn",
        "position": "PhD student;PhD student;PhD student;Researcher;MS student;Postdoc;Professor;Full Professor",
        "bibtex": "@inproceedings{\ntang2025mimicfunc,\ntitle={MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence},\nauthor={Chao Tang and Anxing Xiao and Yuhong Deng and Tianrun Hu and Wenlong Dong and Hanbo Zhang and David Hsu and Hong Zhang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=CBYGhryESq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CBYGhryESq",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;1;0;1;1;0",
        "aff_unique_norm": "Southern University of Science and Technology;National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sustech.edu.cn;https://www.nus.edu.sg",
        "aff_unique_abbr": "SUSTech;NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;0;1;1;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "CNPCSuwxJw",
        "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Human skin provides a rich tactile sensing stream, localizing intentional and unintentional contact events over a large and contoured region. Replicating these tactile sensing capabilities for dexterous robotic manipulation systems remains a longstanding challenge. In this work, we take a step towards this goal by introducing DexSkin. DexSkin is a soft, conformable capacitive electronic skin that enables sensitive, localized, and calibratable tactile sensing, and can be tailored to varying geometries. We demonstrate its efficacy for learning downstream robotic manipulation by sensorizing a pair of parallel jaw gripper fingers, providing tactile coverage across almost the entire finger surfaces. We empirically evaluate DexSkin\u2019s capabilities in learning challenging manipulation tasks that require sensing coverage across the entire surface of the fingers, such as reorienting objects in hand and wrapping elastic bands around boxes, in a learning-from-demonstration framework. We then show that, critically for data-driven approaches, DexSkin can be calibrated to enable model transfer across sensor instances, and demonstrate its applicability to online reinforcement learning on real robots. Our results highlight DexSkin\u2019s suitability and practicality for learning real-world, contact-rich manipulation.",
        "keywords": "tactile sensing;contact-rich manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/b263baaa2cf6c2e1a0a3a3f2a8c5714e7e620917.zip",
        "author": "Suzannah Wistreich;Baiyu Shi;Stephen Tian;Samuel Clarke;Michael Nath;Chengyi Xu;Zhenan Bao;Jiajun Wu",
        "authorids": "~Suzannah_Wistreich1;~Baiyu_Shi1;~Stephen_Tian1;~Samuel_Clarke1;~Michael_Nath1;~Chengyi_Xu1;~Zhenan_Bao1;~Jiajun_Wu1",
        "gender": "F;M;M;;M;M;F;M",
        "homepage": ";https://baiyu-shi.github.io/;http://s-tian.github.io;;https://michael-nath.github.io;https://x-lab.one;https://baogroup.stanford.edu/;https://jiajunwu.com",
        "dblp": ";;237/9780;;;;;117/4768",
        "google_scholar": ";;l19pn2sAAAAJ;;;v2dF7jYAAAAJ;4OLlInEAAAAJ;2efgcS0AAAAJ",
        "orcid": ";;;;;;0000-0002-0972-1715;0000-0002-4176-343X",
        "linkedin": "suzannah-wistreich/;;;;;;zhenan-bao-75a14b7/;jiajunwu/",
        "or_profile": "~Suzannah_Wistreich1;~Baiyu_Shi1;~Stephen_Tian1;~Samuel_Clarke1;~Michael_Nath1;~Chengyi_Xu1;~Zhenan_Bao1;~Jiajun_Wu1",
        "aff": "Stanford University+Stanford University+Stanford University;Stanford University;Stanford University;;Stanford University;University of Alabama at Birmingham;Stanford University;Stanford University",
        "aff_domain": "stanford.edu+stanford.edu+stanford.edu;stanford.edu;stanford.edu;;stanford.edu;uab.edu;stanford.edu;stanford.edu",
        "position": "MS student+Researcher+Undergrad student;PhD student;PhD student;;Undergrad student;Assistant Professor;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nwistreich2025dexskin,\ntitle={DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation},\nauthor={Suzannah Wistreich and Baiyu Shi and Stephen Tian and Samuel Clarke and Michael Nath and Chengyi Xu and Zhenan Bao and Jiajun Wu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=CNPCSuwxJw}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CNPCSuwxJw",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+0+0;0;0;1;0;2;0;0",
        "aff_unique_norm": "Stanford University;;University of Alabama at Birmingham",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;;https://www.uab.edu",
        "aff_unique_abbr": "Stanford;;UAB",
        "aff_campus_unique_index": "0+0+0;0;0;0;2;0;0",
        "aff_campus_unique": "Stanford;;Birmingham",
        "aff_country_unique_index": "0+0+0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "CQKxhmLobo",
        "title": "Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Recent advances in generative world models have enabled classical safe control methods, such as Hamilton-Jacobi (HJ) reachability, to generalize to complex robotic systems operating directly from high-dimensional sensor observations. However, obtaining comprehensive coverage of all safety-critical scenarios during world model training is extremely challenging. As a result, latent safety filters built on top of these models may miss novel hazards and even fail to prevent known ones, overconfidently misclassifying risky out-of-distribution (OOD) situations as safe. To address this, we introduce an uncertainty-aware latent safety filter that proactively steers robots away from both known and unseen failures. Our key idea is to use the world model\u2019s epistemic uncertainty as a proxy for identifying unseen potential hazards. We propose a principled method to detect OOD world model predictions by calibrating an uncertainty threshold via conformal prediction. By performing reachability analysis in an augmented state space\u2014spanning both the latent representation and the epistemic uncertainty\u2014we synthesize a latent safety filter that can reliably safeguard arbitrary policies from both known and unseen safety hazards. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our uncertainty-aware safety filter preemptively detects potential unsafe scenarios and reliably proposes safe, in-distribution actions.",
        "keywords": "safe control;uncertainty quantification;world models",
        "primary_area": "",
        "supplementary_material": "/attachment/ca413d58a8b4c6c9c18d703ee8cc9648c0420e51.zip",
        "author": "Junwon Seo;Kensuke Nakamura;Andrea Bajcsy",
        "authorids": "~Junwon_Seo2;~Kensuke_Nakamura1;~Andrea_Bajcsy1",
        "gender": "M;;",
        "homepage": "https://junwon.me/;;",
        "dblp": "318/0330;;",
        "google_scholar": "dLHzusUAAAAJ;https://scholar.google.ca/citations?hl=en;",
        "orcid": "0000-0002-5741-6774;;",
        "linkedin": "seo-junwon;;",
        "or_profile": "~Junwon_Seo2;~Kensuke_Nakamura1;~Andrea_Bajcsy1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University;",
        "aff_domain": "andrew.cmu.edu;andrew.cmu.edu;",
        "position": "PhD student;PhD student;",
        "bibtex": "@inproceedings{\nseo2025uncertaintyaware,\ntitle={Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures},\nauthor={Junwon Seo and Kensuke Nakamura and Andrea Bajcsy},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=CQKxhmLobo}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CQKxhmLobo",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Carnegie Mellon University;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;",
        "aff_unique_abbr": "CMU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "CgGSFtjplI",
        "title": "Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Teaching robots dexterous manipulation skills often requires collecting hundreds of demonstrations using wearables or teleoperation, a process that is challenging to scale. Videos of human-object interactions are easier to collect and scale, but leveraging them directly for robot learning is difficult due to the lack of explicit action labels and human-robot embodiment differences. We propose Human2Sim2Robot, a novel real-to-sim-to-real framework for training dexterous manipulation policies using only one RGB-D video of a human demonstrating a task. Our method utilizes reinforcement learning (RL) in simulation to cross the embodiment gap without relying on wearables, teleoperation, or large-scale data collection. From the video, we extract: (1) the object pose trajectory to define an object-centric, embodiment-agnostic reward, and (2) the pre-manipulation hand pose to initialize and guide exploration during RL training. These components enable effective policy learning without any task-specific reward tuning. In the single human demo regime, Human2Sim2Robot outperforms object-aware replay by over 55% and imitation learning by over 68% on grasping, non-prehensile manipulation, and multi-step tasks. Website: https://human2sim2robot.github.io",
        "keywords": "Dexterous Manipulation;Reinforcement Learning;Sim-to-Real",
        "primary_area": "",
        "supplementary_material": "/attachment/033472e8b6df3bc9107c2c67987792f1e95ec65d.zip",
        "author": "Tyler Ga Wei Lum;Olivia Y. Lee;Karen Liu;Jeannette Bohg",
        "authorids": "~Tyler_Ga_Wei_Lum1;~Olivia_Y._Lee1;~Karen_Liu1;~Jeannette_Bohg1",
        "gender": "M;;;",
        "homepage": "https://tylerlum.github.io/;https://oliviaylee.github.io/;https://cs.stanford.edu/~karenliu;https://web.stanford.edu/~bohg/",
        "dblp": ";;;52/7377",
        "google_scholar": "kPq6-XIAAAAJ;https://scholar.google.com/citations?hl=en;i28fU0MAAAAJ;rjnJnEkAAAAJ",
        "orcid": ";;0000-0001-5926-0905;0000-0002-4921-7193",
        "linkedin": "tyler-lum/;oliviaylee/;;",
        "or_profile": "~Tyler_Ga_Wei_Lum1;~Olivia_Y._Lee1;~Karen_Liu1;~Jeannette_Bohg1",
        "aff": "Stanford University;Computer Science Department, Stanford University;Computer Science Department, Stanford University;Stanford University",
        "aff_domain": "stanford.edu;cs.stanford.edu;cs.stanford.edu;stanford.edu",
        "position": "PhD student;MS student;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nlum2025crossing,\ntitle={Crossing the Human-Robot Embodiment Gap with Sim-to-Real {RL} using One Human Demonstration},\nauthor={Tyler Ga Wei Lum and Olivia Y. Lee and Karen Liu and Jeannette Bohg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=CgGSFtjplI}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=CgGSFtjplI",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "Cv3ihZYkZf",
        "title": "Uncertainty-Aware Scene Understanding via Efficient Sampling-Free Confidence Estimation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Reliable scene understanding requires not only accurate predictions but also well-calibrated confidence estimates to ensure calibrated uncertainty estimation, especially in safety-critical domains like autonomous driving. In this context, semantic segmentation of LiDAR points supports real-time 3D scene understanding, where reliable uncertainty estimates help identify potentially erroneous predictions. While most existing calibration approaches focus on modeling epistemic uncertainty, they often overlook aleatoric uncertainty arising from measurement inaccuracies, which is especially prevalent in LiDAR data and essential for real-world deployment.\nIn this work, we introduce a sampling-free approach for estimating well-calibrated confidence values by explicitly modeling aleatoric uncertainty in semantic segmentation, achieving alignment with true classification accuracy and reducing inference time compared to sampling-based methods. Evaluated on the real-world SemanticKITTI benchmark, our approach achieves 1.70\\% and 1.33\\% Adaptive Calibration Error (ACE) in semantic segmentation of LiDAR data using RangeViT and SalsaNext models, and is more than one order of magnitude faster than the comparable baseline. Furthermore, reliability diagrams reveal that our method produces underconfident rather than overconfident predictions \u2014 an advantageous property in safety-critical systems.",
        "keywords": "Confidence Calibration in Deep Learning;Aleatoric Uncertainty Estimation;Reliable Semantic Segmentation of LiDAR Point Clouds",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hanieh Shojaei Miandashti;Qianqian Zou;Claus Brenner",
        "authorids": "~Hanieh_Shojaei_Miandashti1;~Qianqian_Zou1;~Claus_Brenner2",
        "gender": "F;F;",
        "homepage": "https://www.ikg.uni-hannover.de/de/shojaei;;https://www.ikg.uni-hannover.de/",
        "dblp": ";;50/262",
        "google_scholar": "https://scholar.google.com/citations?hl=en;TUxCYgIAAAAJ;https://scholar.google.de/citations?user=VK5xKS4AAAAJ",
        "orcid": "0000-0003-0968-0420;0000-0001-7673-0630;0000-0002-6459-0682",
        "linkedin": "hanie-shojaei/;qianqian-zou-a04a0b173/;",
        "or_profile": "~Hanieh_Shojaei_Miandashti1;~Qianqian_Zou1;~Claus_Brenner2",
        "aff": "Institute of Cartography and Geoinformatics ;Leibniz Universit\u00e4t Hannover;Leibniz University Hannover",
        "aff_domain": "ikg.uni-hannover.de;uni-hannover.de;ikg.uni-hannover.de",
        "position": "PhD student;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nmiandashti2025uncertaintyaware,\ntitle={Uncertainty-Aware Scene Understanding via Efficient Sampling-Free Confidence Estimation},\nauthor={Hanieh Shojaei Miandashti and Qianqian Zou and Claus Brenner},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Cv3ihZYkZf}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Cv3ihZYkZf",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Institute of Cartography and Geoinformatics;Leibniz Universit\u00e4t Hannover;Leibniz University Hannover",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.leibniz.uni-hannover.de/;https://www.leibniz.uni-hannover.de",
        "aff_unique_abbr": ";LUH;LUH",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";Germany"
    },
    {
        "id": "DKXx17oaUf",
        "title": "OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "LiDAR place recognition is a critical capability for autonomous navigation and cross-modal localization in large-scale outdoor environments. Existing approaches predominantly depend on pre-built 3D dense maps or aerial imagery, which impose significant storage overhead and lack real-time adaptability. In this paper, we propose OPAL, a novel network for LiDAR place recognition that leverages OpenStreetMap (OSM) as a lightweight and up-to-date prior. Our key innovation lies in bridging the domain disparity between sparse LiDAR scans and structured OSM data through two carefully designed components. First, a cross-modal visibility mask that identifies maximal observable regions from both modalities to guide feature learning. Second, an adaptive radial fusion module that dynamically consolidates radial features into discriminative global descriptors. Extensive experiments on the KITTI and KITTI-360 datasets demonstrate OPAL\u2019s superiority, achieving 15.98% higher recall at  @1m threshold for top-1 retrieved matches, along with 12x faster inference speed compared to the state-of-the-art approach. Code and datasets will be publicly available.",
        "keywords": "Place Recognition;OpenStreetMap;Point Cloud",
        "primary_area": "",
        "supplementary_material": "/attachment/ad85af308b995247afcfeb0efc911049e4904070.zip",
        "author": "Shuhao Kang;Youqi Liao;Yan Xia;Olaf Wysocki;Boris Jutzi;Daniel Cremers",
        "authorids": "~Shuhao_Kang1;~Youqi_Liao1;~Yan_Xia5;~Olaf_Wysocki1;~Boris_Jutzi1;~Daniel_Cremers1",
        "gender": "M;;M;M;;M",
        "homepage": ";https://martin-liao.github.io/;https://yan-xia.github.io/;https://olafwysocki.github.io/;https://www.professoren.tum.de/jutzi-boris;https://vision.in.tum.de/members/cremers",
        "dblp": ";;17/6518-3;323/0406;54/9001.html;c/DanielCremers",
        "google_scholar": "qB6B7lkAAAAJ;Q7N06fkAAAAJ;xkBn4mMAAAAJ;9xQhtFcAAAAJ;ZpB02CwAAAAJ;cXQciMEAAAAJ",
        "orcid": ";;;0000-0002-0016-0229;0000-0002-4322-3074;",
        "linkedin": ";;;olaf-wysocki;boris-jutzi-91ab2035/;",
        "or_profile": "~Shuhao_Kang1;~Youqi_Liao1;~Yan_Xia5;~Olaf_Wysocki1;~Boris_Jutzi1;~Daniel_Cremers1",
        "aff": "Technische Universit\u00e4t M\u00fcnchen;Wuhan University;University of Science and Technology of China+Technische Universit\u00e4t M\u00fcnchen;University of Cambridge+Technical University of Munich;Karlsruher Institut f\u00fcr Technologie+Technische Universit\u00e4t M\u00fcnchen;Technical University Munich",
        "aff_domain": "tum.de;whu.edu.cn;ustc.edu.cn+tum.de;cam.ac.uk+tum.de;kit.edu+tum.de;tum.de",
        "position": "MS student;MS student;Associate Professor+Researcher;Principal Researcher+Researcher;Full Professor+Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nkang2025opal,\ntitle={{OPAL}: Visibility-aware Li{DAR}-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion},\nauthor={Shuhao Kang and Youqi Liao and Yan Xia and Olaf Wysocki and Boris Jutzi and Daniel Cremers},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=DKXx17oaUf}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DKXx17oaUf",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2+0;3+4;5+0;4",
        "aff_unique_norm": "Technische Universit\u00e4t M\u00fcnchen;Wuhan University;University of Science and Technology of China;University of Cambridge;Technical University of Munich;Karlsruher Institut f\u00fcr Technologie",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.tum.de;http://www.whu.edu.cn/;http://www.ustc.edu.cn;https://www.cam.ac.uk;https://www.tum.de;https://www.kit.edu",
        "aff_unique_abbr": "TUM;WHU;USTC;Cambridge;TUM;KIT",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Cambridge",
        "aff_country_unique_index": "0;1;1+0;2+0;0+0;0",
        "aff_country_unique": "Germany;China;United Kingdom"
    },
    {
        "id": "Ddb8w8FVV9",
        "title": "Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Unsupervised Skill Discovery (USD) allows agents to autonomously learn diverse behaviors without task-specific rewards. While recent USD methods have shown promise, their application to real-world robotics remains underexplored.\nIn this paper, we propose a modular USD framework to address the challenges in safety, interpretability, and deployability of the learned skills.\nOur approach factorizes the state space to learn disentangled skill representations and assigns different skill discovery algorithms to each factor based on the desired intrinsic reward function.\nTo encourage structured morphology-aware skills, we introduce symmetry-based inductive biases tailored to individual factors. We also incorporate a style factor and regularization penalties to promote safe and robust behaviors.\nWe evaluate our framework in simulation using a quadrupedal robot and demonstrate zero-shot transfer of the learned skills to real hardware. Our results show that factorization and symmetry lead to the discovery of structured, human-interpretable behaviors, while the style factor and penalties enhance safety and diversity. Additionally, we show that the learned skills can be used for downstream tasks and perform on par with oracle policies trained with hand-crafted rewards.\nTo facilitate future research, we will release our code upon publication.",
        "keywords": "unsupervised skill discovery;reinforcement learning;legged robots",
        "primary_area": "",
        "supplementary_material": "/attachment/03b040533a446a9ea1481ebba4b0ae110557be5b.zip",
        "author": "Rafael Cathomen;Mayank Mittal;Marin Vlastelica;Marco Hutter",
        "authorids": "~Rafael_Cathomen1;~Mayank_Mittal1;~Marin_Vlastelica1;~Marco_Hutter1",
        "gender": "M;M;;M",
        "homepage": ";https://mayankm96.github.io;;http://www.rsl.ethz.ch",
        "dblp": ";;;04/2753",
        "google_scholar": ";iVXG-IkAAAAJ;;https://scholar.google.ch/citations?user=DO3quJYAAAAJ",
        "orcid": "0009-0005-7593-1402;;;0000-0002-4285-4990",
        "linkedin": "rafael-cathomen-6b4b5b287;mayankm-0096/;;",
        "or_profile": "~Rafael_Cathomen1;~Mayank_Mittal1;~Marin_Vlastelica1;~Marco_Hutter1",
        "aff": "ETHZ - ETH Zurich;ETHZ - ETH Zurich+NVIDIA;;ETHZ - ETH Zurich",
        "aff_domain": "ethz.ch;ethz.ch+nvidia.com;;ethz.ch",
        "position": "MS student;PhD student+Researcher;;Associate Professor",
        "bibtex": "@inproceedings{\ncathomen2025divide,\ntitle={Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors},\nauthor={Rafael Cathomen and Mayank Mittal and Marin Vlastelica and Marco Hutter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Ddb8w8FVV9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Ddb8w8FVV9",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0+1;2;0",
        "aff_unique_norm": "ETH Zurich;NVIDIA;",
        "aff_unique_dep": ";NVIDIA Corporation;",
        "aff_unique_url": "https://www.ethz.ch;https://www.nvidia.com;",
        "aff_unique_abbr": "ETHZ;NVIDIA;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+1;0",
        "aff_country_unique": "Switzerland;United States;"
    },
    {
        "id": "DiyLz91PKF",
        "title": "UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Accurate estimation of the in-hand pose of an object based on its CAD model is crucial in both industrial applications and everyday tasks\u2014ranging from positioning workpieces and assembling components to seamlessly inserting devices like USB connectors. While existing methods often rely on regression, feature matching, or registration techniques, achieving high precision and generalizability to unseen CAD models remains a significant challenge. In this paper, we propose a novel three-stage framework for in-hand pose estimation. The first stage involves sampling and pre-ranking pose candidates, followed by iterative refinement of these candidates in the second stage. In the final stage, post-ranking is applied to identify the most likely pose candidates. These stages are governed by a unified energy-based diffusion model, which is trained solely on simulated data. This energy model simultaneously generates gradients to refine pose estimates and produces an energy scalar that quantifies the quality of the pose estimates. Additionally, inspired by the computer vision domain, we incorporate a render-compare architecture within the energy-based score network to significantly enhance sim-to-real performance, as demonstrated by our ablation studies. Extensive experimental evaluations show that our method outperforms conventional baselines based on regression, matching, and registration techniques, while also exhibiting strong generalization to previously unseen CAD models. Moreover, our approach integrates tactile object pose estimation, pose tracking, and uncertainty estimation into a unified system, enabling robust performance across a variety of real-world conditions.",
        "keywords": "Tactile Object Pose Estimation;Diffusion Model;Precise Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/2bceec51ee53c5fc40880b342daa1c4e0b3bdfad.zip",
        "author": "Mingdong Wu;Long Yang;Jin Liu;Weiyao Huang;Lehong Wu;Zelin Chen;Daolin Ma;Hao Dong",
        "authorids": "~Mingdong_Wu1;~Long_Yang7;~Jin_Liu23;~Weiyao_Huang1;~Lehong_Wu1;~Zelin_Chen2;~Daolin_Ma1;~Hao_Dong3",
        "gender": "M;M;M;M;M;M;M;M",
        "homepage": "https://aaronanima.github.io/;;;https://sshwy.github.io;https://lehongwu.github.io/;;;https://zsdonghao.github.io",
        "dblp": "315/5136;;;;387/3841;;;14/1525-3.html",
        "google_scholar": "https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?view_op=list_works;;;;;lfQRWLkAAAAJ;xLFL4sMAAAAJ",
        "orcid": ";;0009-0008-1418-2542;;;0009-0004-6645-7457;;0000-0003-2261-9122",
        "linkedin": ";;;;;;;",
        "or_profile": "~Mingdong_Wu1;~Long_Yang7;~Jin_Liu23;~Weiyao_Huang1;~Lehong_Wu1;~Zelin_Chen2;~Daolin_Ma1;~Hao_Dong3",
        "aff": "Center on Frontiers of Computing Studies,Peking University;Peking University;Shanghai Jiaotong University;Peking University;Peking University;Shanghai Jiaotong University;Shanghai Jiaotong University;Peking University+Peking University",
        "aff_domain": "pku.edu.cn;stu.pku.edu.cn;sjtu.edu.cn;pku.edu.cn;stu.pku.edu.cn;sjtu.edu.cn;sjtu.edu.cn;pku.edu.cn+pku.edu.cn",
        "position": "PhD student;MS student;PhD student;Undergrad student;Undergrad student;PhD student;Associate Professor;Associate Professor+Assistant Professor",
        "bibtex": "@inproceedings{\nwu2025unitacpose,\ntitle={UniTac2Pose: A Unified Approach Learned in Simulation for Category-level Visuotactile In-hand Pose Estimation},\nauthor={Mingdong Wu and Long Yang and Jin Liu and Weiyao Huang and Lehong Wu and Zelin Chen and Daolin Ma and Hao Dong},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=DiyLz91PKF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DiyLz91PKF",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;0;1;1;0+0",
        "aff_unique_norm": "Peking University;Shanghai Jiao Tong University",
        "aff_unique_dep": "Center on Frontiers of Computing Studies;",
        "aff_unique_url": "http://www.pku.edu.cn;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "Peking U;SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "China"
    },
    {
        "id": "DzRNBBCP4R",
        "title": "Action-Free Reasoning for Policy Generalization",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "End-to-end imitation learning offers a promising approach for training robot policies. However, generalizing to new settings\u2014such as unseen scenes, tasks, and object instances\u2014remains a significant challenge. Although large-scale robot demonstration datasets have shown potential for inducing generalization, they are resource-intensive to scale. In contrast, human video data is abundant and diverse, presenting an attractive alternative. Yet, these human-video datasets lack action labels, complicating their use in imitation learning. Existing methods attempt to extract grounded action representations (e.g., hand poses), but resulting policies struggle to bridge the embodiment gap between human and robot actions. We propose an alternative approach: leveraging language-based reasoning from human videos\u2014essential for guiding robot actions\u2014to train generalizable robot policies. Building on recent advances in reasoning-based policy architectures, we introduce Reasoning through Action-free Data (RAD). RAD learns from both robot demonstration data (with reasoning and action labels) and action-free human video data (with only reasoning labels). The robot data teaches the model to map reasoning to low-level actions, while the action-free data enhances reasoning capabilities. Additionally, we release a new dataset of 3,377 human-hand demonstrations compatible with the Bridge V2 benchmark. This dataset includes chain-of-thought reasoning annotations and hand-tracking data, and is aimed at facilitating future research on reasoning-driven robot learning. Our experiments demonstrate that RAD enables effective transfer across the embodiment gap, allowing robots to perform tasks seen only in action-free data. Furthermore, scaling up action-free reasoning data significantly improves policy performance and generalization to novel tasks. These results highlight the promise of reasoning-driven learning from action-free datasets for advancing generalizable robot control. See website with videos: https://rad-generalization-s.github.io/",
        "keywords": "Foundation Models;Robot Learning;Imitation Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jaden Clark;Suvir Mirchandani;Dorsa Sadigh;Suneel Belkhale",
        "authorids": "~Jaden_Clark1;~Suvir_Mirchandani1;~Dorsa_Sadigh1;~Suneel_Belkhale1",
        "gender": "M;M;F;M",
        "homepage": "https://jadenvc.github.io/;http://suvirpmirchandani.com;https://dorsa.fyi/;https://github.com/suneelbelkhale",
        "dblp": ";287/4981;117/3174;236/5069",
        "google_scholar": "https://scholar.google.com/citations?hl=en;fz7LJPIAAAAJ;ZaJEZpYAAAAJ;",
        "orcid": "0009-0007-3875-2792;;;0000-0002-3963-7987",
        "linkedin": ";;;suneel-b-032b1a101/",
        "or_profile": "~Jaden_Clark1;~Suvir_Mirchandani1;~Dorsa_Sadigh1;~Suneel_Belkhale1",
        "aff": "Computer Science Department, Stanford University;Stanford University;Google+Stanford University;Stanford University",
        "aff_domain": "cs.stanford.edu;stanford.edu;google.com+stanford.edu;stanford.edu",
        "position": "MS student;PhD student;Researcher+Assistant Professor;PhD student",
        "bibtex": "@inproceedings{\nclark2025actionfree,\ntitle={Action-Free Reasoning for Policy Generalization},\nauthor={Jaden Clark and Suvir Mirchandani and Dorsa Sadigh and Suneel Belkhale},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=DzRNBBCP4R}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=DzRNBBCP4R",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1+0;0",
        "aff_unique_norm": "Stanford University;Google",
        "aff_unique_dep": "Computer Science Department;Google",
        "aff_unique_url": "https://www.stanford.edu;https://www.google.com",
        "aff_unique_abbr": "Stanford;Google",
        "aff_campus_unique_index": "0;0;1+0;0",
        "aff_campus_unique": "Stanford;Mountain View",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "E9t1ekt6W9",
        "title": "Joint Model-based Model-free Diffusion for Planning with Constraints",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Model-free diffusion planners have shown great promise for robot motion planning, but practical robotic systems often require combining them with model-based optimization modules to enforce constraints, such as safety. Na\\\"ively integrating these modules presents compatibility challenges when diffusion's multi-modal outputs behave adversarially to optimization-based modules. To address this, we introduce Joint Model-based Model-free Diffusion (JM2D), a novel generative modeling framework. JM2D formulates module integration as a joint sampling problem to maximize compatibility via an interaction potential, without additional training. Using importance sampling, JM2D guides modules outputs based only on evaluations of the interaction potential, thus handling non-differentiable objectives commonly arising from non-convex optimization modules. We evaluate JM2D via application to aligning diffusion planners with safety modules on offline RL and robot manipulation. JM2D significantly improves task performance compared to conventional safety filters without sacrificing safety. Further, we show that conditional generation is a special case of JM2D and elucidate key design choices by comparing with SOTA gradient-based and projection-based diffusion planners. More details at: \\url{https://sites.google.com/view/joint-mbmf-diffusion}",
        "keywords": "Diffusion Model;Safety;Constrained Generation",
        "primary_area": "",
        "supplementary_material": "/attachment/ef82bdb695dc85be2bb4c1deed71b82896e0b96b.zip",
        "author": "Wonsuhk Jung;Utkarsh Aashu Mishra;Nadun Ranawaka Arachchige;Yongxin Chen;Danfei Xu;Shreyas Kousik",
        "authorids": "~Wonsuhk_Jung1;~Utkarsh_Aashu_Mishra2;~Nadun_Ranawaka_Arachchige1;~Yongxin_Chen1;~Danfei_Xu1;~Shreyas_Kousik1",
        "gender": "M;M;;M;M;",
        "homepage": ";http://utkarshmishra04.github.io/;;https://yongxin.ae.gatech.edu/;https://cs.stanford.edu/~danfei/;",
        "dblp": ";274/2706;;;135/8443;",
        "google_scholar": "https://scholar.google.co.kr/citations?user=CK-lldAAAAAJ;10HbT44AAAAJ;;X8BYiV4AAAAJ;J5D4kcoAAAAJ;",
        "orcid": ";0000-0002-4977-5187;;;;",
        "linkedin": ";utkarshamishra/;https://linkedin.com/in/nadun-ranawaka-arachchige-87701b137;;;",
        "or_profile": "~Wonsuhk_Jung1;~Utkarsh_Aashu_Mishra2;~Nadun_Ranawaka_Arachchige1;~Yongxin_Chen1;~Danfei_Xu1;~Shreyas_Kousik1",
        "aff": "Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;NVIDIA+Georgia Institute of Technology;Georgia Institute of Technology+NVIDIA;",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;nvidia.com+gatech.edu;gatech.edu+nvidia.com;",
        "position": "PhD student;PhD student;PhD student;Principal Researcher+Associate Professor;Assistant Professor+Research Scientist;",
        "bibtex": "@inproceedings{\njung2025joint,\ntitle={Joint Model-based Model-free Diffusion for Planning with Constraints},\nauthor={Wonsuhk Jung and Utkarsh Aashu Mishra and Nadun Ranawaka Arachchige and Yongxin Chen and Danfei Xu and Shreyas Kousik},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=E9t1ekt6W9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=E9t1ekt6W9",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1+0;0+1;2",
        "aff_unique_norm": "Georgia Institute of Technology;NVIDIA;",
        "aff_unique_dep": ";NVIDIA Corporation;",
        "aff_unique_url": "https://www.gatech.edu;https://www.nvidia.com;",
        "aff_unique_abbr": "Georgia Tech;NVIDIA;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "EBcb4LemZZ",
        "title": "Co-Design of Soft Gripper with Neural Physics",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "For robot manipulation, both the controller and end-effector design are crucial. Compared with rigid grippers, soft grippers are more generalizable by deforming to different geometries, but designing such a gripper and finding its grasp pose remains challenging. In this paper, we propose a co-design framework that generates an optimized soft gripper\u2019s block-wise stiffness distribution and its grasping pose, using a neural physics model trained in simulation. We adopt a uniform-pressure tendon model, then generate a diverse dataset by randomizing both gripper pose and design parameters. A neural network is trained to approximate this forward simulation, yielding a fast, differentiable surrogate. We embed that surrogate in an end-to-end optimization loop to recover the ideal stiffness configuration and best grasp pose. Finally, we 3D-print the optimized grippers of various stiffness by changing the printing infills and parameters. We demonstrate that our co-designed grippers significantly outperform baseline designs in terms of force closure and success rate.",
        "keywords": "soft robot;manipulation;design optimization",
        "primary_area": "",
        "supplementary_material": "/attachment/ef8e99fcbd764b06ebaf226bac70b2105c3bf188.zip",
        "author": "Sha Yi;Xueqian Bai;Adabhav Singh;Jianglong Ye;Michael T. Tolley;Xiaolong Wang",
        "authorids": "~Sha_Yi1;~Xueqian_Bai1;~Adabhav_Singh1;~Jianglong_Ye1;~Michael_T._Tolley1;~Xiaolong_Wang3",
        "gender": "F;F;M;M;M;M",
        "homepage": "https://yswhynot.github.io;https://hakuna25.github.io/;;https://jianglongye.com;https://bioinspired.eng.ucsd.edu/;https://xiaolonw.github.io/",
        "dblp": ";;;307/5025;;91/952-4",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;;nkEGpKsAAAAJ;0kOHVOkAAAAJ;Y8O9N_0AAAAJ",
        "orcid": ";;;0000-0003-1347-9199;0000-0001-7821-7777;",
        "linkedin": ";;adabhav/;jianglongye/;;",
        "or_profile": "~Sha_Yi1;~Xueqian_Bai1;~Adabhav_Singh1;~Jianglong_Ye1;~Michael_T._Tolley1;~Xiaolong_Wang3",
        "aff": "University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego;University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu;ucsd.edu",
        "position": "Postdoc;MS student;Undergrad student;PhD student;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nyi2025codesign,\ntitle={Co-Design of Soft Gripper with Neural Physics},\nauthor={Sha Yi and Xueqian Bai and Adabhav Singh and Jianglong Ye and Michael T. Tolley and Xiaolong Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=EBcb4LemZZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=EBcb4LemZZ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "University of California, San Diego",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ucsd.edu",
        "aff_unique_abbr": "UCSD",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "EGSSHukI05",
        "title": "SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Offline Imitation Learning (IL) methods such as Behavior Cloning are effective at acquiring complex robotic manipulation skills. \n     However, existing IL-trained policies are confined to execute the task at the same speed as shown in demonstration data. This limits the task throughput of a robotic system, a critical requirement for applications such as industrial automation. In this paper, we introduce and formalize the novel problem of enabling faster-than-demonstration execution of visuomotor policies and identify fundamental challenges in robot dynamics and state-action distribution shifts. We instantiate the key insights as SAIL (Speed Adaptation for Imitation Learning), a full-stack system integrating four tightly-connected components: (1) a consistency-preserving action inference algorithm for smooth motion at high speed, (2) high-fidelity tracking of controller-invariant motion targets, (3) adaptive speed modulation that dynamically adjusts execution speed based on motion complexity, and (4) action scheduling to handle real-world system latencies. \n      Experiments on 12 tasks across simulation and two real, distinct robot platforms shows that SAIL achieves up to a {4$\\times$ speedup} over demonstration speed in simulation and up to {3.2$\\times$ speedup} in the real world. Additional detail is available at https://sail-robot.github.io",
        "keywords": "Visuomotor Imitation;Robot Learning Systems;Robotic Manipulation;Speed Adaptive Execution",
        "primary_area": "",
        "supplementary_material": "/attachment/52fadd6511316bb043174b8b4371bfed74c6dde5.zip",
        "author": "Nadun Ranawaka Arachchige;Zhenyang Chen;Wonsuhk Jung;Woo Chul Shin;Rohan Bansal;Pierre Barroso;Yu Hang He;Yingyan Celine Lin;Benjamin Joffe;Shreyas Kousik;Danfei Xu",
        "authorids": "~Nadun_Ranawaka_Arachchige1;~Zhenyang_Chen1;~Wonsuhk_Jung1;~Woo_Chul_Shin1;~Rohan_Bansal1;~Pierre_Barroso1;~Yu_Hang_He1;~Yingyan_Celine_Lin1;~Benjamin_Joffe1;~Shreyas_Kousik1;~Danfei_Xu1",
        "gender": ";M;M;;M;M;M;;;;M",
        "homepage": ";;;https://swc0620.github.io/profile/;;;;;https://research.gatech.edu/people/benjamin-joffe;;https://cs.stanford.edu/~danfei/",
        "dblp": ";;;;;;;;126/2732;;135/8443",
        "google_scholar": ";9jF7qBkAAAAJ;https://scholar.google.co.kr/citations?user=CK-lldAAAAAJ;;;;;;IaNhZ9AAAAAJ;;J5D4kcoAAAAJ",
        "orcid": ";;;;0009-0003-9533-4950;;0000-0001-9357-5118;;0000-0001-8574-0136;;",
        "linkedin": "https://linkedin.com/in/nadun-ranawaka-arachchige-87701b137;;;woochulshin/;https://linkedin.com/in/bansal-rohan;pierre-barroso;yu-hang-he;;bpjoffe;;",
        "or_profile": "~Nadun_Ranawaka_Arachchige1;~Zhenyang_Chen1;~Wonsuhk_Jung1;~Woo_Chul_Shin1;~Rohan_Bansal1;~Pierre_Barroso1;~Yu_Hang_He1;~Yingyan_Celine_Lin1;~Benjamin_Joffe1;~Shreyas_Kousik1;~Danfei_Xu1",
        "aff": "Georgia Institute of Technology;;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology+Georgia Tech Research Institute;;Georgia Institute of Technology;;Georgia Institute of Technology+NVIDIA",
        "aff_domain": "gatech.edu;;gatech.edu;gatech.edu;gatech.edu;gatech.edu;gatech.edu+gatech.edu;;gatech.edu;;gatech.edu+nvidia.com",
        "position": "PhD student;;PhD student;MS student;Undergrad student;Researcher;MS student+Researcher;;Researcher;;Assistant Professor+Research Scientist",
        "bibtex": "@inproceedings{\narachchige2025sail,\ntitle={{SAIL}: Faster-than-Demonstration Execution of Imitation Learning Policies},\nauthor={Nadun Ranawaka Arachchige and Zhenyang Chen and Wonsuhk Jung and Woo Chul Shin and Rohan Bansal and Pierre Barroso and Yu Hang He and Yingyan Celine Lin and Benjamin Joffe and Shreyas Kousik and Danfei Xu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=EGSSHukI05}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=EGSSHukI05",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0;0;0;0+2;1;0;1;0+3",
        "aff_unique_norm": "Georgia Institute of Technology;;Georgia Tech Research Institute;NVIDIA",
        "aff_unique_dep": ";;;NVIDIA Corporation",
        "aff_unique_url": "https://www.gatech.edu;;https://www.gtri.gatech.edu;https://www.nvidia.com",
        "aff_unique_abbr": "Georgia Tech;;GTRI;NVIDIA",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0+0;0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "EXgckdYESp",
        "title": "PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel Object Pose Estimation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "RGB-based novel object pose estimation is critical for rapid deployment in robotic applications, yet zero-shot generalization remains a key challenge. In this paper, we introduce PicoPose, a novel framework designed to tackle this task using a three-stage pixel-to-pixel correspondence learning process. Firstly, PicoPose matches features from the RGB observation with those from rendered object templates, identifying the best-matched template and establishing coarse correspondences. Secondly, PicoPose smooths the correspondences by globally regressing a 2D affine transformation, including in-plane rotation, scale, and 2D translation, from the coarse correspondence map. Thirdly, PicoPose applies the affine transformation to the feature map of the best-matched template and learns correspondence offsets within local regions to achieve fine-grained correspondences. By progressively refining the correspondences, PicoPose significantly improves the accuracy of object poses computed via PnP/RANSAC. PicoPose achieves state-of-the-art performance on the seven core datasets of the BOP benchmark, demonstrating exceptional generalization to novel objects. Our code and models will be made publicly available.",
        "keywords": "Novel Object Pose Estimation;Robotic Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/32bbd7571bfd4def9fd36e5e6a2a3a758b285194.zip",
        "author": "Lihua Liu;Jiehong Lin;ZhenXin Liu;Kui Jia",
        "authorids": "~Lihua_Liu1;~Jiehong_Lin1;~ZhenXin_Liu1;~Kui_Jia1",
        "gender": "M;M;M;M",
        "homepage": "https://foollh.github.io/LiHua.github.io/;;https://github.com/Liu-Zhen-Xin;http://kuijia.site/",
        "dblp": ";239/8762;;60/3834",
        "google_scholar": ";eSkDBYcAAAAJ;;Mf9VHRcAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Lihua_Liu1;~Jiehong_Lin1;~ZhenXin_Liu1;~Kui_Jia1",
        "aff": "South China University of Technology;University of Hong Kong;South China University of Technology;The Chinese University of Hong Kong, Shenzhen",
        "aff_domain": "scut.edu.cn;hku.hk;scut.edu.cn;cuhk.edu.cn",
        "position": "MS student;Postdoc;MS student;Full Professor",
        "bibtex": "@inproceedings{\nliu2025picopose,\ntitle={PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel Object Pose Estimation},\nauthor={Lihua Liu and Jiehong Lin and ZhenXin Liu and Kui Jia},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=EXgckdYESp}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=EXgckdYESp",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;2",
        "aff_unique_norm": "South China University of Technology;University of Hong Kong;Chinese University of Hong Kong",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.scut.edu.cn;https://www.hku.hk;https://www.cuhk.edu.cn",
        "aff_unique_abbr": "SCUT;HKU;CUHK",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Hong Kong SAR;Shenzhen",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "EcOGafgvuC",
        "title": "FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Reinforcement learning (RL) has made significant strides in legged robot control, enabling locomotion across diverse terrains and complex loco-manipulation capabilities.\nHowever, the commonly used position or velocity tracking-based objectives are agnostic to forces experienced by the robot, leading to stiff and potentially dangerous behaviors and poor control during forceful interactions.\nTo address this limitation, we present Force-Adaptive Control via Impedance Reference Tracking (FACET). Inspired by impedance control, we use RL to train a control policy to imitate a virtual mass-spring-damper system, allowing fine-grained control under external forces by manipulating the virtual spring. \nIn simulation, we demonstrate that our quadruped robot achieves improved robustness to large impulses\n(up to 200 Ns)\nand exhibits controllable compliance, achieving an 80\\% reduction in collision impulse. The policy is deployed to a physical robot, demonstrating both compliant behavior, such as initiation/cessation of movement with finger tip, and the ability to pull payloads up to 10kg. Further extension to a legged loco-manipulator and a humanoid shows the applicability of our method to more complex settings to enable whole-body compliance control.",
        "keywords": "Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/f74133c817e210084c69b9ded9f22857c5484e21.zip",
        "author": "Botian Xu;Haoyang Weng;Qingzhou Lu;Yang Gao;Huazhe Xu",
        "authorids": "~Botian_Xu1;~Haoyang_Weng1;~Qingzhou_Lu1;~Yang_Gao1;~Huazhe_Xu1",
        "gender": "M;;M;M;M",
        "homepage": "https://btx0424.github.io/;;https://me.axell.top/;http://yang-gao.weebly.com;http://hxu.rocks",
        "dblp": "220/1400;;;89/4402-29;164/9006",
        "google_scholar": "OLLsgKIAAAAJ;;;https://scholar.google.com/citations?hl=en;t9HPFawAAAAJ",
        "orcid": ";;0009-0009-9022-1846;;",
        "linkedin": ";;;yang-gao-45245348/;",
        "or_profile": "~Botian_Xu1;~Haoyang_Weng1;~Qingzhou_Lu1;~Yang_Gao1;~Huazhe_Xu1",
        "aff": "Tsinghua University;;Tsinghua University;Tsinghua University;Tsinghua University",
        "aff_domain": "mail.tsinghua.edu.cn;;mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "position": "Researcher;;Undergrad student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nxu2025facet,\ntitle={{FACET}: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots},\nauthor={Botian Xu and Haoyang Weng and Qingzhou Lu and Yang Gao and Huazhe Xu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=EcOGafgvuC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=EcOGafgvuC",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0;0",
        "aff_unique_norm": "Tsinghua University;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tsinghua.edu.cn;",
        "aff_unique_abbr": "THU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "EgSDP6AOF1",
        "title": "UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts.",
        "keywords": "Robot Learning;Imitation Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/f1f3bcf42a21ee4f522ed05df02f255ebd889862.zip",
        "author": "Hanjung Kim;Jaehyun Kang;Hyolim Kang;Meedeum Cho;Seon Joo Kim;Youngwoon Lee",
        "authorids": "~Hanjung_Kim1;~Jaehyun_Kang1;~Hyolim_Kang1;~Meedeum_Cho1;~Seon_Joo_Kim2;~Youngwoon_Lee1",
        "gender": "M;M;M;M;;M",
        "homepage": "https://kimhanjung.github.io;https://github.com/kang-jaehyun;;https://chomeed.github.io;;https://youngwoon.github.io",
        "dblp": "333/1291;187/4280;188/2259;;;117/4767",
        "google_scholar": "https://scholar.google.com/citations?hl=en;3d31SzwAAAAJ;mALQD0sAAAAJ;;;CDPa3AgAAAAJ",
        "orcid": ";0000-0001-5922-1770;;;;0000-0001-9918-1056",
        "linkedin": "hanjung-kim-060436191/;;hyolim-kang-03760b169/;;;",
        "or_profile": "~Hanjung_Kim1;~Jaehyun_Kang1;~Hyolim_Kang1;~Meedeum_Cho1;~Seon_Joo_Kim2;~Youngwoon_Lee1",
        "aff": "Yonsei University;Yonsei University;Yonsei University;Yonsei University;;Yonsei University",
        "aff_domain": "yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;yonsei.ac.kr;;yonsei.ac.kr",
        "position": "PhD student;MS student;PhD student;Undergrad student;;Assistant Professor",
        "bibtex": "@inproceedings{\nkim2025uniskill,\ntitle={UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations},\nauthor={Hanjung Kim and Jaehyun Kang and Hyolim Kang and Meedeum Cho and Seon Joo Kim and Youngwoon Lee},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=EgSDP6AOF1}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=EgSDP6AOF1",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "Yonsei University;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.yonsei.ac.kr;",
        "aff_unique_abbr": "Yonsei;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea;"
    },
    {
        "id": "FBsawSyYBM",
        "title": "ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "This paper considers the problem of enabling robots to navigate dynamic environments while following instructions. \nThe challenge lies in the combinatorial nature of instruction specifications: each instruction can include multiple specifications, and the number of possible specification combinations grows exponentially as the robot\u2019s skill set expands. For example, \u201covertake the pedestrian while staying on the right side of the road\u201d consists of two specifications: *\"overtake the pedestrian\"* and *\"walk on the right side of the road.\"*\nTo tackle this challenge, we propose ComposableNav, based on the intuition that following an instruction involves independently satisfying its constituent specifications, each corresponding to a distinct motion primitive. \nUsing diffusion models, ComposableNav learns each primitive separately, then composes them in parallel at deployment time to satisfy novel combinations of specifications unseen in training. \nAdditionally, to avoid the onerous need for demonstrations of individual motion primitives, we propose a two-stage training procedure: (1) supervised pre-training to learn a base diffusion model for dynamic navigation, and (2) reinforcement learning fine-tuning that molds the base model into different motion primitives.\nThrough simulation and real-world experiments, we show that ComposableNav enables robots to follow instructions by generating trajectories that satisfy diverse and unseen combinations of specifications, significantly outperforming both non-compositional VLM-based policies and costmap composing baselines.",
        "keywords": "Diffusion Motion Planning;Diffusion Model;Compositionality;Instruction-following Navigation in Dynamic Environments",
        "primary_area": "",
        "supplementary_material": "/attachment/5e4559bc27b1cc6bdbb98b461f3d99ad6ff07541.zip",
        "author": "Zichao Hu;Chen Tang;Michael Joseph Munje;Yifeng Zhu;Alex Liu;Shuijing Liu;Garrett Warnell;Peter Stone;Joydeep Biswas",
        "authorids": "~Zichao_Hu1;~Chen_Tang2;~Michael_Joseph_Munje1;~Yifeng_Zhu2;~Alex_Liu4;~Shuijing_Liu1;~Garrett_Warnell1;~Peter_Stone1;~Joydeep_Biswas1",
        "gender": "M;M;;M;M;F;M;M;M",
        "homepage": ";https://chentangmark.github.io;https://michaelmunje.com/about/;https://cs.utexas.edu/~yifengz;;https://shuijing725.github.io;;http://www.cs.utexas.edu/~pstone;https://www.joydeepb.com/",
        "dblp": ";;;;;211/7210;173/5902;s/PeterStone;84/73",
        "google_scholar": "Qk-v-okAAAAJ;x78TL58AAAAJ;;;;I4k7ukgAAAAJ;Ndp8dmgAAAAJ;qnwjcfAAAAAJ;https://scholar.google.com.tw/citations?user=f28F1YUAAAAJ",
        "orcid": "0009-0007-6433-8878;;;;;;;0000-0002-6795-420X;0000-0002-1211-1731",
        "linkedin": ";chen-tang-08377b5b/;;;alex-c-liu/;shuijing-liu-4089b3123;;;",
        "or_profile": "~Zichao_Hu1;~Chen_Tang2;~Michael_Joseph_Munje1;~Yifeng_Zhu2;~Alex_Liu4;~Shuijing_Liu1;~Garrett_Warnell1;~Peter_Stone1;~Joydeep_Biswas1",
        "aff": "University of Texas at Austin;University of Texas at Austin;University of Texas at Austin;The University of Texas at Austin;University of Texas at Austin;, University of Texas at Austin;University of Texas, Austin+Army Research Laboratory;Sony AI+University of Texas, Austin;NVIDIA+The University of Texas at Austin",
        "aff_domain": "utexas.edu;utexas.edu;utexas.edu;utexas.edu;utexas.edu;cs.utexas.edu;utexas.edu+army.mil;sony.com+utexas.edu;nvidia.com+cs.utexas.edu",
        "position": "PhD student;Postdoc;PhD student;PhD student;Undergrad student;Postdoc;Visiting Researcher+Research Scientist;Principal Researcher+Full Professor;Visiting Professor+Associate Professor",
        "bibtex": "@inproceedings{\nhu2025composablenav,\ntitle={ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion},\nauthor={Zichao Hu and Chen Tang and Michael Joseph Munje and Yifeng Zhu and Alex Liu and Shuijing Liu and Garrett Warnell and Peter Stone and Joydeep Biswas},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=FBsawSyYBM}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FBsawSyYBM",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;0;0;0+1;2+0;3+0",
        "aff_unique_norm": "University of Texas at Austin;Army Research Laboratory;Sony;NVIDIA",
        "aff_unique_dep": ";;Sony AI;NVIDIA Corporation",
        "aff_unique_url": "https://www.utexas.edu;https://www.arl.army.mil;https://www.sony.com;https://www.nvidia.com",
        "aff_unique_abbr": "UT Austin;ARL;Sony AI;NVIDIA",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0;1+0;0+0",
        "aff_country_unique": "United States;Japan"
    },
    {
        "id": "FCHl2ybdAm",
        "title": "Robot Learning from Any Images",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We introduce RoLA, a framework that transforms any in\u2011the\u2011wild image into an interactive, physics\u2011enabled robotic environment.  Unlike previous methods, RoLA operates directly on a single image without requiring additional hardware or digital assets. Our framework democratizes robotic data generation by producing massive visuomotor robotic demonstrations within minutes from a wide range of image sources, including camera captures, robotic datasets, and Internet images. At its core, our approach combines a novel method for single-view physical scene recovery with an efficient visual blending strategy for photorealistic data collection. We demonstrate RoLA's versatility across applications like scalable robotic data generation and augmentation, robot learning from internet images, and single-image real-to-sim-to-real systems for manipulators and humanoids. Video results are available at our \\href{https://rola-2025.github.io/}{project page}.",
        "keywords": "Robotic Manipulation;Real-to-Sim-to-Real;Robotic Data Collection",
        "primary_area": "",
        "supplementary_material": "/attachment/80bf88752f883119cd91d4d5ab830af219b5dc5f.zip",
        "author": "Siheng Zhao;Jiageng Mao;Wei Chow;Zeyu Shangguan;Tianheng Shi;Rong Xue;Yuxi Zheng;Yijia Weng;Yang You;Daniel Seita;Leonidas Guibas;Sergey Zakharov;Vitor Campagnolo Guizilini;Yue Wang",
        "authorids": "~Siheng_Zhao1;~Jiageng_Mao1;~Wei_Chow1;~Zeyu_Shangguan1;~Tianheng_Shi1;~Rong_Xue1;~Yuxi_Zheng4;~Yijia_Weng1;~Yang_You2;~Daniel_Seita1;~Leonidas_Guibas1;~Sergey_Zakharov1;~Vitor_Campagnolo_Guizilini2;~Yue_Wang2",
        "gender": ";;M;M;M;F;F;F;M;;M;M;M;M",
        "homepage": "https://sihengz02.github.io/;;http://none.com;;;;https://linktr.ee/asukaredpanda;https://yijiaweng.github.io/;https://qq456cvb.github.io;;http://geometry.stanford.edu/;https://zakharos.github.io/;;https://yuewang.xyz",
        "dblp": "341/1176;;;334/1715;;;;264/9900;33/8167;;g/LeonidasJGuibas;195/5832;;33/4822-41",
        "google_scholar": "l7EAauYAAAAJ;;;;;;;yeuv8L4AAAAJ;1YV1_KUAAAAJ;;https://scholar.google.com.tw/citations?user=5JlEyTAAAAAJ;https://scholar.google.de/citations?user=3DK3I-8AAAAJ;UH9tP6QAAAAJ;v-AEFIEAAAAJ",
        "orcid": ";;;0000-0003-1435-6959;;0009-0009-8734-8440;;;;;;;;",
        "linkedin": ";;;;tianheng-shi-5244b8201/;;;;;;;;vitorguizilini/;",
        "or_profile": "~Siheng_Zhao1;~Jiageng_Mao1;~Wei_Chow1;~Zeyu_Shangguan1;~Tianheng_Shi1;~Rong_Xue1;~Yuxi_Zheng4;~Yijia_Weng1;~Yang_You2;~Daniel_Seita1;~Leonidas_Guibas1;~Sergey_Zakharov1;~Vitor_Campagnolo_Guizilini2;~Yue_Wang2",
        "aff": "University of Southern California+Amazon;;Zhejiang University;University of Southern California;University of Southern California;Shanghai Jiaotong University;University of Southern California;Stanford University;Stanford University;;Stanford University;Toyota Research Institute;Toyota Research Institute;University of Southern California+NVIDIA",
        "aff_domain": "usc.edu+amazon.com;;zju.edu.cn;usc.edu;usc.edu;sjtu.edu.cn;usc.edu;stanford.edu;stanford.edu;;stanford.edu;tri.global;tri.global;usc.edu+nvidia.com",
        "position": "PhD student+Applied Scientist Intern;;Undergrad student;PhD student;MS student;MS student;Intern;PhD student;Postdoc;;Full Professor;Researcher;Staff Research Scientist;Assistant Professor+Researcher",
        "bibtex": "@inproceedings{\nzhao2025robot,\ntitle={Robot Learning from Any Images},\nauthor={Siheng Zhao and Jiageng Mao and Wei Chow and Zeyu Shangguan and Tianheng Shi and Rong Xue and Yuxi Zheng and Yijia Weng and Yang You and Daniel Seita and Leonidas Guibas and Sergey Zakharov and Vitor Campagnolo Guizilini and Yue Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=FCHl2ybdAm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FCHl2ybdAm",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            14,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;2;3;0;0;4;0;5;5;2;5;6;6;0+7",
        "aff_unique_norm": "University of Southern California;Amazon;;Zhejiang University;Shanghai Jiao Tong University;Stanford University;Toyota Research Institute;NVIDIA",
        "aff_unique_dep": ";Amazon.com, Inc.;;;;;;NVIDIA Corporation",
        "aff_unique_url": "https://www.usc.edu;https://www.amazon.com;;https://www.zju.edu.cn;https://www.sjtu.edu.cn;https://www.stanford.edu;https://www.tri.global;https://www.nvidia.com",
        "aff_unique_abbr": "USC;Amazon;;ZJU;SJTU;Stanford;TRI;NVIDIA",
        "aff_campus_unique_index": "0;0;0;0;2;2;2;0",
        "aff_campus_unique": "Los Angeles;;Stanford",
        "aff_country_unique_index": "0+0;2;0;0;2;0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States;;China"
    },
    {
        "id": "FCpYuGtN4j",
        "title": "HuB: Learning Extreme Humanoid Balance",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "The human body demonstrates exceptional motor capabilities\u2014such as standing steadily on one foot or performing a high kick with the leg raised over 1.5 meters\u2014both requiring precise balance control. While recent research on humanoid control has leveraged reinforcement learning to track human motions for skill acquisition, applying this paradigm to balance-intensive tasks remains challenging. In this work, we identify three key obstacles: instability from reference motion errors, learning difficulties due to morphological mismatch, and the sim-to-real gap caused by sensor noise and unmodeled dynamics. To address these challenges, we propose $\\textbf{HuB}$ ($\\textbf{Hu}$manoid $\\textbf{B}$alance), a unified framework that integrates $\\textit{reference motion refinement}$, $\\textit{balance-aware policy learning}$, and $\\textit{sim-to-real robustness training}$, with each component targeting a specific challenge. We validate our approach on the Unitree G1 humanoid robot across challenging quasi-static balance tasks, including extreme single-legged poses such as $\\texttt{Swallow Balance}$ and $\\texttt{Bruce Lee\u2019s Kick}$. Our policy remains stable even under strong physical disturbances\u2014such as a forceful soccer strike\u2014while baseline methods consistently fail to complete these tasks.",
        "keywords": "Humanoid Whole-body Control;Balance Control;Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/2b04c4462e801c80bb6162c407a4112fe60cf489.zip",
        "author": "Tong Zhang;Boyuan Zheng;Ruiqian Nai;Yingdong Hu;Yen-Jen Wang;Geng Chen;Fanqi Lin;Jiongye Li;Chuye Hong;Koushil Sreenath;Yang Gao",
        "authorids": "~Tong_Zhang23;~Boyuan_Zheng3;~Ruiqian_Nai1;~Yingdong_Hu1;~Yen-Jen_Wang1;~Geng_Chen3;~Fanqi_Lin2;~Jiongye_Li1;~Chuye_Hong1;~Koushil_Sreenath1;~Yang_Gao1",
        "gender": ";M;M;M;M;M;M;M;M;M;M",
        "homepage": "https://tongzhangthu.github.io/;https://github.com/ZhengBryan;;;https://wangyenjen.github.io;https://jc043.github.io/;https://fanqi-lin.github.io/;https://github.com/Cata1ysttt;;;http://yang-gao.weebly.com",
        "dblp": ";;304/3166;219/8916;164/2206;;;;;;89/4402-29",
        "google_scholar": "https://scholar.google.com/citations?hl=en;ifGjX54AAAAJ;https://scholar.google.com/citations?hl=en;HhotyAoAAAAJ;_U-HwfkAAAAJ;;https://scholar.google.com/citations?hl=en;;;o9aFV8cAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;;;;;;0009-0005-4679-2212;;",
        "linkedin": ";;;;wangyenjen/;;;;;;yang-gao-45245348/",
        "or_profile": "~Tong_Zhang23;~Boyuan_Zheng3;~Ruiqian_Nai1;~Yingdong_Hu1;~Yen-Jen_Wang1;~Geng_Chen3;~Fanqi_Lin2;~Jiongye_Li1;~Chuye_Hong1;~Koushil_Sreenath1;~Yang_Gao1",
        "aff": "Tsinghua University;Tsinghua University+Tongji University;Tsinghua University;Tsinghua University;University of California, Berkeley;University of California, San Diego;Tsinghua University;Tsinghua University;Tsinghua University;University of California, Berkeley;Tsinghua University",
        "aff_domain": "tsinghua.edu.cn;mails.tsinghua.edu.cn+tongji.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;berkeley.edu;ucsd.edu;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;mails.tsinghua.edu.cn;berkeley.edu;tsinghua.edu.cn",
        "position": "PhD student;PhD student+Undergrad student;PhD student;PhD student;PhD student;MS student;PhD student;Undergrad student;Undergrad student;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025hub,\ntitle={HuB: Learning Extreme Humanoid Balance},\nauthor={Tong Zhang and Boyuan Zheng and Ruiqian Nai and Yingdong Hu and Yen-Jen Wang and Geng Chen and Fanqi Lin and Jiongye Li and Chuye Hong and Koushil Sreenath and Yang Gao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=FCpYuGtN4j}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FCpYuGtN4j",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0+1;0;0;2;3;0;0;0;2;0",
        "aff_unique_norm": "Tsinghua University;Tongji University;University of California, Berkeley;University of California, San Diego",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.tongji.edu.cn;https://www.berkeley.edu;https://www.ucsd.edu",
        "aff_unique_abbr": "THU;Tongji;UC Berkeley;UCSD",
        "aff_campus_unique_index": ";1;2;1",
        "aff_campus_unique": ";Berkeley;San Diego",
        "aff_country_unique_index": "0;0+0;0;0;1;1;0;0;0;1;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "FdgtV9mO6j",
        "title": "Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent\u2019s behavior through constrained reinforcement learning. The system helps regulate the agent\u2019s actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds.",
        "keywords": "Crowd Navigation;Reinforcement Learning;Distribution Shift",
        "primary_area": "",
        "supplementary_material": "/attachment/fd7d322679424cfff98b60de9eb570fd4ec3afc4.zip",
        "author": "Jianpeng Yao;Xiaopan Zhang;Yu Xia;Zejin Wang;Amit Roy-Chowdhury;Jiachen Li",
        "authorids": "~Jianpeng_Yao1;~Xiaopan_Zhang1;~Yu_Xia17;~Zejin_Wang2;~Amit_Roy-Chowdhury2;~Jiachen_Li1",
        "gender": "M;M;F;M;;M",
        "homepage": "https://jyao97.github.io/;https://xiaopanz.github.io;;;;https://jiachenli94.github.io/",
        "dblp": ";;;;;137/8316-1.html",
        "google_scholar": "P6I5HF8AAAAJ;iLrGkU4AAAAJ;;;;1_f79vUAAAAJ",
        "orcid": ";;0009-0008-5557-6150;;;",
        "linkedin": ";;;zejing-wang-530575190/;;jiachen-li/",
        "or_profile": "~Jianpeng_Yao1;~Xiaopan_Zhang1;~Yu_Xia17;~Zejin_Wang2;~Amit_Roy-Chowdhury2;~Jiachen_Li1",
        "aff": "University of California, Riverside;University of California, Riverside;University of California, Riverside;University of California, Riverside;;University of California, Riverside",
        "aff_domain": "ucr.edu;ucr.edu;ucr.edu;ucr.edu;;ucr.edu",
        "position": "PhD student;PhD student;MS student;Intern;;Assistant Professor",
        "bibtex": "@inproceedings{\nyao2025towards,\ntitle={Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling},\nauthor={Jianpeng Yao and Xiaopan Zhang and Yu Xia and Zejin Wang and Amit Roy-Chowdhury and Jiachen Li},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=FdgtV9mO6j}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FdgtV9mO6j",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "University of California, Riverside;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucr.edu;",
        "aff_unique_abbr": "UCR;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Riverside;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "FwLMCbs47K",
        "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution.\nTo address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning.\nIn practice, to further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution.\nExtensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.",
        "keywords": "Autoregressive;Causal Diffusion Policy",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jiahua Ma;Yiran Qin;Yixiong Li;Xuanqi Liao;Yulan Guo;Ruimao Zhang",
        "authorids": "~Jiahua_Ma2;~Yiran_Qin1;~Yixiong_Li1;~Xuanqi_Liao1;~Yulan_Guo3;~Ruimao_Zhang1",
        "gender": "M;M;M;M;;M",
        "homepage": ";https://iranqin.github.io/;https://lyxichigoichie.github.io/;https://github.com/Coffee-Paper;;http://zhangruimao.site/#",
        "dblp": ";;;;;54/10697",
        "google_scholar": ";;;;;ZJwZdtgAAAAJ",
        "orcid": "0009-0006-6357-5284;;;;;",
        "linkedin": ";yiran-qin-b508871b2/;;;;",
        "or_profile": "~Jiahua_Ma2;~Yiran_Qin1;~Yixiong_Li1;~Xuanqi_Liao1;~Yulan_Guo3;~Ruimao_Zhang1",
        "aff": "SUN YAT-SEN UNIVERSITY;The Chinese University of Hong Kong(Shenzhen);SUN YAT-SEN UNIVERSITY;SUN YAT-SEN UNIVERSITY;;SUN YAT-SEN UNIVERSITY",
        "aff_domain": "sysu.edu.cn;cuhk.edu.cn;mail2.sysu.edu.cn;mail2.sysu.edu.cn;;sysu.edu.cn",
        "position": "PhD student;PhD student;MS student;MS student;;Associate Professor",
        "bibtex": "@inproceedings{\nma2025cdp,\ntitle={{CDP}: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion},\nauthor={Jiahua Ma and Yiran Qin and Yixiong Li and Xuanqi Liao and Yulan Guo and Ruimao Zhang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=FwLMCbs47K}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=FwLMCbs47K",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0;2;0",
        "aff_unique_norm": "Sun Yat-sen University;Chinese University of Hong Kong;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.sysu.edu.cn;https://www.cuhk.edu.cn;",
        "aff_unique_abbr": "SYSU;CUHK;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "GH9kURIRlx",
        "title": "ToddlerBot: Open-Source ML-Compatible Humanoid Platform for Loco-Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Learning-based robotics research driven by data demands a new approach to robot hardware design\u2014one that serves as both a platform for policy execution and a tool for embodied data collection. We introduce ToddlerBot, a low-cost, open-source humanoid robot platform designed for robotics and AI research. ToddlerBot enables seamless acquisition of high-quality simulation and real-world data. The plug-and-play zero-point calibration and transferable motor system identification ensure a high-fidelity digital twin and zero-shot sim-to-real policy transfer. A user-friendly teleoperation interface streamlines real-world data collection from human demonstrations. With its data collection ability and anthropomorphic design, ToddlerBot is ideal for whole-body loco-manipulation research. Additionally, ToddlerBot's compact size (0.56 m, 3.4 kg) ensures safe operation in real-world environments. Reproducibility is achieved with entirely 3D-printed, open-source design and off-the-shelf components, keeping the total cost under 6,000 USD. This allows assembly and maintenance with basic technical expertise, as validated by successful independent replications of the system. We demonstrate ToddlerBot's capabilities through arm span, payload, endurance tests, loco-manipulation tasks, and a collaborative long-horizon scenario where two robots tidy a toy session together. By advancing ML-compatibility, capability, and reproducibility, ToddlerBot provides a robust and scalable platform for policy learning and execution in robotics research.",
        "keywords": "Humanoid Robots;Mechanisms & Design;Robot Modeling & Simulation",
        "primary_area": "",
        "supplementary_material": "/attachment/e2e188d7f17f2d74bd9466a89acba82b18214418.zip",
        "author": "Haochen Shi;Weizhuo Wang;Shuran Song;Karen Liu",
        "authorids": "~Haochen_Shi2;~Weizhuo_Wang2;~Shuran_Song3;~Karen_Liu1",
        "gender": "M;M;F;",
        "homepage": "https://hshi74.github.io/;https://me.weizhuowang.com;https://shurans.github.io/;https://cs.stanford.edu/~karenliu",
        "dblp": ";161/7224;;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;gx_A620AAAAJ;https://scholar.google.com/citations?hl=en;i28fU0MAAAAJ",
        "orcid": "0000-0002-3604-465X;0009-0007-1153-1324;;0000-0001-5926-0905",
        "linkedin": ";weizhuo-ken-wang-ba4921119/;;",
        "or_profile": "~Haochen_Shi2;~Weizhuo_Wang2;~Shuran_Song3;~Karen_Liu1",
        "aff": "Stanford University;Stanford University;Stanford University;Computer Science Department, Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;cs.stanford.edu",
        "position": "PhD student;PhD student;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nshi2025toddlerbot,\ntitle={ToddlerBot: Open-Source {ML}-Compatible Humanoid Platform for Loco-Manipulation},\nauthor={Haochen Shi and Weizhuo Wang and Shuran Song and Karen Liu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=GH9kURIRlx}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GH9kURIRlx",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "GIHDBntBu9",
        "title": "TrackVLA: Embodied Visual Tracking in the Wild",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Embodied visual tracking is a fundamental skill in Embodied AI, enabling an agent to follow a specific target in dynamic environments using only egocentric vision. This task is inherently challenging as it requires both accurate target recognition and effective trajectory planning under conditions of severe occlusion and high scene dynamics. Existing approaches typically address this challenge through a modular separation of recognition and planning. In this work, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the synergy between object recognition and trajectory planning. Leveraging a shared LLM backbone, we employ a language modeling head for recognition and an anchor-based diffusion model for trajectory planning. To train TrackVLA, we construct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse difficulty levels of recognition samples, resulting in a dataset of 1.7 million samples. Through extensive experiments in both synthetic and real-world environments, TrackVLA demonstrates SOTA performance and strong generalizability. It significantly outperforms existing methods on public benchmarks in a zero-shot manner while remaining robust to high dynamics and occlusion in real-world scenarios at 10 FPS inference speed.",
        "keywords": "Embodied Visual Tracking;Vision-Language-Action Model",
        "primary_area": "",
        "supplementary_material": "/attachment/f20e503a062fa25f083fb433e71fc2028aff8b2a.zip",
        "author": "Shaoan Wang;Jiazhao Zhang;Minghan Li;Jiahang Liu;Anqi Li;Kui Wu;Fangwei Zhong;Junzhi Yu;Zhizheng Zhang;He Wang",
        "authorids": "~Shaoan_Wang1;~Jiazhao_Zhang2;~Minghan_Li5;~Jiahang_Liu2;~Anqi_Li7;~Kui_Wu5;~Fangwei_Zhong3;~Junzhi_Yu1;~Zhizheng_Zhang1;~He_Wang5",
        "gender": "M;M;;M;M;M;;M;M;M",
        "homepage": "https://wsakobe.github.io/;https://jzhzhang.github.io/;;https://orcid.org/0009-0002-3334-6692;https://andyhandsom6.github.io/;https://github.com/wukui-muc;;;;https://hughw19.github.io",
        "dblp": "326/4452;243/2982;;;;;;;67/4758;01/6368-10",
        "google_scholar": "m3FCQrkAAAAJ;zOZgpXwAAAAJ;;;;;;Gudfky4AAAAJ;X7M0I8kAAAAJ;roCAWkoAAAAJ",
        "orcid": "0000-0001-8175-1567;;;;;;;;;",
        "linkedin": ";;;;;;;;;",
        "or_profile": "~Shaoan_Wang1;~Jiazhao_Zhang2;~Minghan_Li5;~Jiahang_Liu2;~Anqi_Li7;~Kui_Wu5;~Fangwei_Zhong3;~Junzhi_Yu1;~Zhizheng_Zhang1;~He_Wang5",
        "aff": "Peking University;Peking University;;Harbin Institute of Technology;Peking University;Beihang University+Southern University of Science and Technology;;Peking University;Beijing Galbot Co., Ltd;Galbot+Peking University",
        "aff_domain": "stu.pku.edu.cn;pku.edu.cn;;stu.hit.edu.cn;stu.pku.edu.cn;buaa.edu.cn+sustech.edu.cn;;pku.edu.cn;galbot.com;galbot.com+pku.edu.cn",
        "position": "PhD student;PhD student;;Undergrad student;Undergrad student;PhD student+Intern;;Full Professor;Principal Researcher;CTO+Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025trackvla,\ntitle={Track{VLA}: Embodied Visual Tracking in the Wild},\nauthor={Shaoan Wang and Jiazhao Zhang and Minghan Li and Jiahang Liu and Anqi Li and Kui Wu and Fangwei Zhong and Junzhi Yu and Zhizheng Zhang and He Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=GIHDBntBu9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GIHDBntBu9",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;2;0;3+4;1;0;5;5+0",
        "aff_unique_norm": "Peking University;;Harbin Institute of Technology;Beihang University;Southern University of Science and Technology;Galbot",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "http://www.pku.edu.cn;;http://www.hit.edu.cn/;http://www.buaa.edu.cn/;https://www.sustech.edu.cn;",
        "aff_unique_abbr": "Peking U;;HIT;BUAA;SUSTech;",
        "aff_campus_unique_index": "1;;",
        "aff_campus_unique": ";Harbin",
        "aff_country_unique_index": "0;0;0;0;0+0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "GKueYvjqSS",
        "title": "KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Collecting demonstrations enriched with fine-grained tactile information is critical for dexterous manipulation, particularly in contact-rich tasks that require precise force control and physical interaction. While prior works primarily focus on teleoperation or video-based retargeting, they often suffer from kinematic mismatches and the absence of real-time tactile feedback, hindering the acquisition of high-fidelity tactile data. To mitigate this issue, we propose KineDex, a hand-over-hand kinesthetic teaching paradigm in which the operator\u2019s motion is directly transferred to the dexterous hand, enabling the collection of physically grounded demonstrations enriched with accurate tactile feedback. To resolve occlusions from human hand, we apply inpainting technique to preprocess the visual observations. Based on these demonstrations, we then train a visuomotor policy using tactile-augmented inputs and implement force control during deployment for precise contact-rich manipulation. We evaluate KineDex on a suite of challenging contact-rich manipulation tasks, including particularly difficult scenarios such as squeezing toothpaste onto a toothbrush, which require precise multi-finger coordination and stable force regulation. Across these tasks, KineDex achieves an average success rate of 74.4%, representing a 57.7% improvement over the variant without force control. Comparative experiments with teleoperation and user studies further validate the advantages of KineDex in data collection efficiency and operability. Specifically, KineDex collects data over twice as fast as teleoperation across two tasks of varying difficulty, while maintaining a near-100% success rate, compared to under 50% for teleoperation.",
        "keywords": "Dexterous Manipulation; Kinesthetic Teaching;Tactile Sensing",
        "primary_area": "",
        "supplementary_material": "/attachment/6b1e7891c78fd3fbc7b8626fc77fc55438edbe9e.zip",
        "author": "Di Zhang;Chengbo Yuan;Chuan Wen;Hai Zhang;Junqiao Zhao;Yang Gao",
        "authorids": "~Di_Zhang5;~Chengbo_Yuan2;~Chuan_Wen1;~Hai_Zhang2;~Junqiao_Zhao1;~Yang_Gao1",
        "gender": "M;;M;M;M;M",
        "homepage": "https://github.com/DinoMax00;;https://alvinwen428.github.io/;https://betray12138.github.io/resume/;http://cs1.tongji.edu.cn/~junqiao;http://yang-gao.weebly.com",
        "dblp": ";;239/8286;;;89/4402-29",
        "google_scholar": ";;G5M9nYwAAAAJ;YHqAzxUAAAAJ;;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;;;",
        "linkedin": ";;;;;yang-gao-45245348/",
        "or_profile": "~Di_Zhang5;~Chengbo_Yuan2;~Chuan_Wen1;~Hai_Zhang2;~Junqiao_Zhao1;~Yang_Gao1",
        "aff": "Tongji University;;Tsinghua University;University of Hong Kong+Tongji University;Tongji University;Tsinghua University",
        "aff_domain": "tongji.edu.cn;;tsinghua.edu.cn;hku.hk+tongji.edu.cn;tongji.edu.cn;tsinghua.edu.cn",
        "position": "Undergrad student;;PhD student;PhD student+MS student;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025kinedex,\ntitle={KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation},\nauthor={Di Zhang and Chengbo Yuan and Chuan Wen and Hai Zhang and Junqiao Zhao and Yang Gao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=GKueYvjqSS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GKueYvjqSS",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3+0;0;2",
        "aff_unique_norm": "Tongji University;;Tsinghua University;University of Hong Kong",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.tongji.edu.cn;;https://www.tsinghua.edu.cn;https://www.hku.hk",
        "aff_unique_abbr": "Tongji;;THU;HKU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "GUgIpVTC1T",
        "title": "From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Safe operation is essential for autonomous systems in safety-critical environments such as urban air mobility. \nValue function-based safety filters provide formal guarantees on safety, wrapping learned or planning-based controllers with a layer of protection. \nRecent approaches leverage offline learned value functions to scale these safety filters to high-dimensional systems. \nYet these methods assume detailed prior knowledge of all possible sources of model mismatch, in the form of disturbances, in the environment -- information that is typically unavailable in real world settings. \nEven in well-mapped environments like urban canyons or industrial sites, drones encounter complex, spatially-varying disturbances arising from payload-drone interaction, turbulent airflow, and other environmental factors. \nWe introduce Space2Time, which enables safe and adaptive deployment of offline-learned safety filters under unknown, spatially-varying disturbances. \nThe key idea is to reparameterize spatial disturbances as a time-varying formulation, allowing the use of temporally varying precomputed value functions during online operation. \nWe validate Space2Time through extensive simulations on diverse quadcopter models and real-world hardware experiments, demonstrating significantly improved safety performance over worst-case and naive baselines.",
        "keywords": "Disturbance-aware Safety;Reachability Analysis",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sander Tonkens;Nikhil Uday Shinde;Azra Begzadi\u0107;Michael C. Yip;Jorge Cortes;Sylvia Lee Herbert",
        "authorids": "~Sander_Tonkens1;~Nikhil_Uday_Shinde1;abegzadic@ucsd.edu;~Michael_C._Yip1;~Jorge_Cortes1;~Sylvia_Lee_Herbert1",
        "gender": "M;M;;;M;F",
        "homepage": "https://stonkens.github.io;;;http://www.ucsdarclab.com;http://terrano.ucsd.edu/jorge;https://sylviaherbert.com",
        "dblp": "277/9184;183/4792;;;35/4734;192/3242",
        "google_scholar": "RcDqKtMAAAAJ;;;gSYxbCYAAAAJ;rKO1QZoAAAAJ;",
        "orcid": ";;;;0000-0001-9582-5184;0000-0002-3863-8945",
        "linkedin": "https://linkedin.com/in/sander-tonkens;nikhil-uday-shinde/;;michael-yip-43913421/;;",
        "or_profile": "~Sander_Tonkens1;~Nikhil_Uday_Shinde1;abegzadic@ucsd.edu;~Michael_C._Yip1;~Jorge_Cortes1;~Sylvia_Lee_Herbert1",
        "aff": "University of California, San Diego;University of California, San Diego;;University of California, San Diego;University of California, San Diego;University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;;ucsd.edu;ucsd.edu;ucsd.edu",
        "position": "PhD student;PhD student;;Associate Professor;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ntonkens2025from,\ntitle={From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting},\nauthor={Sander Tonkens and Nikhil Uday Shinde and Azra Begzadi{\\'c} and Michael C. Yip and Jorge Cortes and Sylvia Lee Herbert},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=GUgIpVTC1T}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GUgIpVTC1T",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "University of California, San Diego;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucsd.edu;",
        "aff_unique_abbr": "UCSD;",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "GfEHkeEU73",
        "title": "Off Policy Lyapunov Stability in Reinforcement Learning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Traditional reinforcement learning lacks the ability to provide stability guarantees. More recent algorithms learn Lyapunov functions alongside the control policies to ensure stable learning. However, the current self-learned Lyapunov functions are sample inefficient due to their on-policy nature. This paper introduces a method for learning Lyapunov functions off-policy and incorporates the proposed off-policy Lyapunov function into the Soft Actor Critic and Proximal Policy Optimization algorithms to provide them with a data efficient stability certificate. Simulations of an inverted pendulum and a quadrotor illustrate the improved performance of the two algorithms when endowed with the proposed off-policy Lyapunov function.",
        "keywords": "Reinforcement Learning;Control;Stability;Lyapunov",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sarvan Gill;Daniela Constantinescu",
        "authorids": "~Sarvan_Gill1;~Daniela_Constantinescu1",
        "gender": "M;F",
        "homepage": ";",
        "dblp": ";",
        "google_scholar": ";",
        "orcid": "0009-0003-7491-5414;0000-0001-9151-0943",
        "linkedin": ";",
        "or_profile": "~Sarvan_Gill1;~Daniela_Constantinescu1",
        "aff": "University of Victoria;University of Victoria",
        "aff_domain": "uvic.ca;uvic.ca",
        "position": "MS student;Full Professor",
        "bibtex": "@inproceedings{\ngill2025off,\ntitle={Off Policy Lyapunov Stability in Reinforcement Learning},\nauthor={Sarvan Gill and Daniela Constantinescu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=GfEHkeEU73}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=GfEHkeEU73",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "University of Victoria",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.uvic.ca",
        "aff_unique_abbr": "UVic",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Canada"
    },
    {
        "id": "Ggu7Hh2xnn",
        "title": "RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Future robotic systems operating in real-world environments require\non-board embodied intelligence without continuous cloud connection, balancing\ncapabilities with constraints on computational power and memory. This work\npresents an extension of the R1-zero approach, which enables the usage of small\nparameter-count Large Language Models (LLMs) in the robotic domain. The\nR1-Zero approach was originally developed to enable mathematical reasoning in\nLLMs using static datasets. We extend it to the robotics domain through integration with a closed-loop Reinforcement Learning (RL) framework. This extension\nallows reasoning in Embodied Artificial Intelligence (EmbodiedAI) settings without relying solely on distillation of large models through Supervised Fine-Tuning\n(SFT). We show that small-scale LLMs can achieve effective reasoning performance by learning through closed-loop interaction with their environment, which\nenables tasks that previously required significantly larger models. A performance\ngain of 20.2% points over the SFT-based baseline is observed with a Qwen2.5-1.5B\nmodel. Using the proposed training procedure, Qwen2.5-3B achieves a 63.3%\ncontrol adaptability score, surpassing the 58.5% obtained by the much larger,\ncloud-bound GPT-4o. These results highlight that practical, on-board deployment\nof small LLMs is not only feasible but can outperform larger models when trained\nthrough environmental interaction, underscoring the importance of an interactive,\nembodied learning framework for robotic EmbodiedAI \u2014 one grounded in practical experience rather than static supervision.",
        "keywords": "Large Language Models;Reinforcement Learning;Embodied AI;Constrained Hardware",
        "primary_area": "",
        "supplementary_material": "/attachment/21038dc6c3784a4b5ca7168760179434e5fc0e7c.zip",
        "author": "Liam Boyle;Nicolas Baumann;Paviththiren Sivasothilingam;Michele Magno;Luca Benini",
        "authorids": "boylel@ethz.ch;~Nicolas_Baumann1;~Paviththiren_Sivasothilingam1;~Michele_Magno1;~Luca_Benini2",
        "gender": ";M;M;M;M",
        "homepage": ";;;https://ee.ethz.ch/the-department/people-a-z/person-detail.michele-magno.html;https://ee.ethz.ch/the-department/people-a-z/person-detail.luca-benini.html",
        "dblp": ";;;;b/LucaBenini.html",
        "google_scholar": ";EhvMidkAAAAJ;;ytj7UUcAAAAJ;8riq3sYAAAAJ",
        "orcid": ";;;;0000-0001-8068-3806",
        "linkedin": ";;paviththiren-sivasothilingam-314617201?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app;;lubenini/",
        "or_profile": "boylel@ethz.ch;~Nicolas_Baumann1;~Paviththiren_Sivasothilingam1;~Michele_Magno1;~Luca_Benini2",
        "aff": ";ETHZ - ETH Zurich;ETHZ - ETH Zurich;ETHZ - ETH Zurich;ETHZ - ETH Zurich+University of Bologna",
        "aff_domain": ";ethz.edu;ethz.ch;ethz.ch;ethz.ch+unibo.it",
        "position": ";PhD student;MS student;Principal Researcher;Full Professor+Full Professor",
        "bibtex": "@inproceedings{\nboyle2025robotxr,\ntitle={RobotxR1: Enabling Embodied Robotic Intelligence on Large Language Models through Closed-Loop Reinforcement Learning},\nauthor={Liam Boyle and Nicolas Baumann and Paviththiren Sivasothilingam and Michele Magno and Luca Benini},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Ggu7Hh2xnn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Ggu7Hh2xnn",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;1;1+2",
        "aff_unique_norm": ";ETH Zurich;University of Bologna",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.ethz.ch;https://www.unibo.it",
        "aff_unique_abbr": ";ETHZ;Unibo",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1;1+2",
        "aff_country_unique": ";Switzerland;Italy"
    },
    {
        "id": "H0EgeP3feg",
        "title": "Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We propose Hand-Eye Autonomous Delivery (HEAD), a framework that learns navigation, locomotion, and reaching skills for humanoids, directly from human motion and vision perception data. We take a modular approach where the high-level planner commands the target position and orientation of the hands and eyes of the humanoid, delivered by the low-level policy that controls the whole-body movements. Specifically, the low-level whole-body controller learns to track the three points (eyes, left hand, and right hand) from existing large-scale human motion capture data while high-level policy learns from human data collected by Aria glasses. Our modular approach decouples the ego-centric vision perception from physical actions, promoting efficient learning and scalability to novel scenes. We evaluate our method both in simulation and in the real-world, demonstrating humanoid's capabilities to navigate and reach in complex environments designed for humans.",
        "keywords": "Learn from Human Data;Humanoid; Whole-Body Control",
        "primary_area": "",
        "supplementary_material": "/attachment/4774d1a2494615e593c011ad04d63eeb637f3e3b.zip",
        "author": "Sirui Chen;Yufei Ye;Zi-ang Cao;Pei Xu;Jennifer Lew;Karen Liu",
        "authorids": "~Sirui_Chen1;~Yufei_Ye1;~Zi-ang_Cao1;~Pei_Xu1;~Jennifer_Lew1;~Karen_Liu1",
        "gender": "M;F;M;;F;",
        "homepage": "https://ericcsr.github.io;https://judyye.github.io/;;;;https://cs.stanford.edu/~karenliu",
        "dblp": ";202/6172;;;;",
        "google_scholar": ";IgWjDugAAAAJ;;;;i28fU0MAAAAJ",
        "orcid": "0000-0002-4446-3627;;;;;0000-0001-5926-0905",
        "linkedin": ";;zi-ang-cao-robotics;;jennlew/;",
        "or_profile": "~Sirui_Chen1;~Yufei_Ye1;~Zi-ang_Cao1;~Pei_Xu1;~Jennifer_Lew1;~Karen_Liu1",
        "aff": "Stanford University;Max-Planck Institute;Stanford University;;Stanford University;Computer Science Department, Stanford University",
        "aff_domain": "stanford.edu;mpg.tuebingen.de;stanford.edu;;stanford.edu;cs.stanford.edu",
        "position": "PhD student;Intern;MS student;;Undergrad student;Full Professor",
        "bibtex": "@inproceedings{\nchen2025handeye,\ntitle={Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching},\nauthor={Sirui Chen and Yufei Ye and Zi-ang Cao and Pei Xu and Jennifer Lew and Karen Liu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=H0EgeP3feg}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=H0EgeP3feg",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;2;0;0",
        "aff_unique_norm": "Stanford University;Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V.;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.mpg.de;",
        "aff_unique_abbr": "Stanford;MPG;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;1;0;0;0",
        "aff_country_unique": "United States;Germany;"
    },
    {
        "id": "H0zFqW6QM0",
        "title": "AnyPlace: Learning Generalizable Object Placement for Robot Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Object placement in robotic tasks is inherently challenging due to the diversity of object geometries and placement configurations. We address this with AnyPlace, a two-stage method trained entirely on synthetic data, capable of predicting a wide range of feasible placement poses for real-world tasks. Our key insight is that by leveraging a Vision-Language Model (VLM) to identify approximate placement locations, we can focus only on the relevant regions for precise local placement, which enables us to train the low-level placement-pose-prediction model to capture multimodal placements efficiently. For training, we generate a fully synthetic dataset comprising 13 categories of randomly generated objects in 5370 different placement poses across three configurations (insertion, stacking, hanging) and train local placement-prediction models. We extensively evaluate our method in high-fidelity simulation and show that it consistently outperforms baseline approaches across all three tasks in terms of success rate, coverage of placement modes, and precision. In real-world experiments, our method achieves an average success and coverage rate of 76% across three tasks, where most baseline methods fail completely. We further validate the generalization of our approach on 16 real-world placement tasks, demonstrating that models trained purely on synthetic data can be directly transferred to the real world in a zero-shot setting. More at: https://anyplace-pnp.github.io.",
        "keywords": "Pick and Place;Robot Manipulation;Synthetic Dataset",
        "primary_area": "",
        "supplementary_material": "/attachment/42d7f353dcbe788df07198f74056ac90598d8acc.zip",
        "author": "Yuchi Zhao;Miroslav Bogdanovic;Chengyuan Luo;Steven Tohme;Kourosh Darvish;Alan Aspuru-Guzik;Florian Shkurti;Animesh Garg",
        "authorids": "~Yuchi_Zhao2;~Miroslav_Bogdanovic1;~Chengyuan_Luo1;~Steven_Tohme1;~Kourosh_Darvish1;~Alan_Aspuru-Guzik2;~Florian_Shkurti1;~Animesh_Garg1",
        "gender": "M;;M;M;M;M;M;M",
        "homepage": "https://y556zhao.github.io/;;https://github.com/nicklcy;https://steventohme.ca/;https://kouroshd.github.io/;http://matter.toronto.edu;http://www.cs.toronto.edu/~florian/;http://animesh.garg.tech",
        "dblp": ";189/7763;;;201/0505;;21/10333;123/5728",
        "google_scholar": "https://scholar.google.ca/citations?user=14RMAx0AAAAJ;;;;FwFFVdIAAAAJ;Ag_6KEgAAAAJ;https://scholar.google.ca/citations?hl=en;zp8V7ZMAAAAJ",
        "orcid": ";;;;;0000-0002-8277-4434;;0000-0003-0482-4296",
        "linkedin": "yuchi-allan-zhao;;chengyuan-luo-3968b3236/;steven-tohme/;kouroshdarvish/;;;animeshgarg/",
        "or_profile": "~Yuchi_Zhao2;~Miroslav_Bogdanovic1;~Chengyuan_Luo1;~Steven_Tohme1;~Kourosh_Darvish1;~Alan_Aspuru-Guzik2;~Florian_Shkurti1;~Animesh_Garg1",
        "aff": "University of Toronto;University of Toronto;Shanghai Jiaotong University;Wilfrid Laurier University;University of Toronto;University of Toronto+University of Toronto;University of Toronto;Georgia Institute of Technology",
        "aff_domain": "utoronto.ca;utoronto.ca;sjtu.edu.cn;wlu.ca;utoronto.ca;utoronto.ca+utoronto.ca;cs.toronto.edu;gatech.edu",
        "position": "PhD student;Postdoc;Undergrad student;Undergrad student;Researcher;Director+Full Professor;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nzhao2025anyplace,\ntitle={AnyPlace: Learning Generalizable Object Placement for Robot Manipulation},\nauthor={Yuchi Zhao and Miroslav Bogdanovic and Chengyuan Luo and Steven Tohme and Kourosh Darvish and Alan Aspuru-Guzik and Florian Shkurti and Animesh Garg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=H0zFqW6QM0}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=H0zFqW6QM0",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;2;0;0+0;0;3",
        "aff_unique_norm": "University of Toronto;Shanghai Jiao Tong University;Wilfrid Laurier University;Georgia Institute of Technology",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.sjtu.edu.cn;https://www.wlu.ca;https://www.gatech.edu",
        "aff_unique_abbr": "U of T;SJTU;WLU;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0;0+0;0;2",
        "aff_country_unique": "Canada;China;United States"
    },
    {
        "id": "HAmi1X11BO",
        "title": "Elucidating the Design Space of Torque-aware Vision-Language-Action Models",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Many robotic manipulation tasks require sensing and responding to force signals such as torque to assess whether the task has been successfully completed and to enable closed-loop control. However, current Vision-Language-Action (VLA) models lack the ability to integrate such subtle physical feedback. In this work, we explore Torque-aware VLA models, aiming to bridge this gap by systematically studying the design space for incorporating torque signals into existing VLA architectures. We identify and evaluate several strategies, leading to three key findings. First, introducing torque adapters into the decoder consistently outperforms inserting them into the encoder.  This is because torque signals align more closely with the decoder\u2019s input, and the decoder is more sensitive to variations in input. Second, torque history proves to be a critical signal. We find that the most effective way to incorporate it is by summarizing the entire history into a single token, as this preserves the original input pattern of the decoder. Third, inspired by joint prediction and planning paradigms in autonomous driving, we propose predicting torque as an auxiliary output, which further improves performance. This strategy encourages the model to build a physically grounded internal representation of interaction dynamics. Extensive quantitative and qualitative experiments across contact-rich manipulation benchmarks validate our findings. Code, models, and datasets will be released.",
        "keywords": "Torque Integration;VLA Models",
        "primary_area": "",
        "supplementary_material": "/attachment/25d0ae7e38b7f7283eb0cf8e9afcde5f3f31ad9c.zip",
        "author": "Zongzheng Zhang;Haobo Xu;Zhuo Yang;Chenghao Yue;Zehao Lin;Huan-ang Gao;Ziwei Wang;Hao Zhao",
        "authorids": "~Zongzheng_Zhang1;~Haobo_Xu2;~Zhuo_Yang6;~Chenghao_Yue1;~Zehao_Lin2;~Huan-ang_Gao1;~Ziwei_Wang2;~Hao_Zhao1",
        "gender": "M;;Not Specified;M;M;M;M;M",
        "homepage": "https://cvpr.thecvf.com/Conferences/2025;;;https://orcid.org/0009-0008-9098-4109;;https://c7w.tech/about;https://ziweiwangthu.github.io/;https://sites.google.com/view/fromandto",
        "dblp": ";;;;;339/0975;136/5574-1;08/3737-2.html",
        "google_scholar": ";;AntTZW8AAAAJ;;;WvbKfLgAAAAJ;cMTW09EAAAAJ;ygQznUQAAAAJ",
        "orcid": "0009-0007-6909-1587;;0009-0001-5572-1679;;0000-0001-5339-1963;;0000-0001-9225-8495;0000-0001-7903-581X",
        "linkedin": ";;;;;;;",
        "or_profile": "~Zongzheng_Zhang1;~Haobo_Xu2;~Zhuo_Yang6;~Chenghao_Yue1;~Zehao_Lin2;~Huan-ang_Gao1;~Ziwei_Wang2;~Hao_Zhao1",
        "aff": "Tsinghua University;;Beijing Institute of Technology;University of Electronic Science and Technology of China;Chongqing University;Tsinghua University;Nanyang Technological University;Tsinghua University+Peking University",
        "aff_domain": "mails.tsinghua.edu.cn;;bit.edu.cn;uestc.edu.cn;cqu.edu.cn;cs.tsinghua.edu.cn;ntu.edu.sg;tsinghua.edu.cn+pku.edu.cn",
        "position": "PhD student;;MS student;Undergrad student;Undergrad student;PhD student;Assistant Professor;Assistant Professor+Postdoc",
        "bibtex": "@inproceedings{\nzhang2025elucidating,\ntitle={Elucidating the Design Space of Torque-aware Vision-Language-Action Models},\nauthor={Zongzheng Zhang and Haobo Xu and Zhuo Yang and Chenghao Yue and Zehao Lin and Huan-ang Gao and Ziwei Wang and Hao Zhao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=HAmi1X11BO}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=HAmi1X11BO",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3;4;0;5;0+6",
        "aff_unique_norm": "Tsinghua University;;Beijing Institute of Technology;University of Electronic Science and Technology of China;Chongqing University;Nanyang Technological University;Peking University",
        "aff_unique_dep": ";;;;;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;http://www.bit.edu.cn/;https://www.uestc.edu.cn;https://www.cqu.edu.cn;https://www.ntu.edu.sg;http://www.pku.edu.cn",
        "aff_unique_abbr": "THU;;BIT;UESTC;CQU;NTU;Peking U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;2;0+0",
        "aff_country_unique": "China;;Singapore"
    },
    {
        "id": "HMcBBIg1Th",
        "title": "Extracting Visual Plans from Unlabeled Videos via Symbolic Guidance",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Visual planning, by offering a sequence of intermediate visual subgoals to a goal-conditioned low-level policy, achieves promising performance on long-horizon manipulation tasks. To obtain the subgoals, existing methods typically resort to video generation models but suffer from model hallucination and computational cost. We present Vis2Plan, an efficient, explainable and white-box visual planning framework powered by symbolic guidance. From raw, unlabeled play data, Vis2Plan harnesses vision foundation models to automatically extract a compact set of task symbols, which allows building a high-level symbolic transition graph for multi-goal, multi-stage planning. At test time, given a desired task goal, our planner conducts planning at the symbolic level and assembles a sequence of physically consistent intermediate sub-goal images grounded by the underlying symbolic representation. Our Vis2Plan outperforms strong diffusion video generation-based visual planners by delivering 53\\% higher aggregate success rate while generating visual plans 35$\\times$ faster. The results indicate that Vis2Plan is able to generate physically consistent image goals while offering fully inspectable reasoning steps.",
        "keywords": "offline imitation learning;planning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Wenyan Yang;Ahmet Tikna;Yi Zhao;Yuying Zhang;Luigi Palopoli;Marco Roveri;Joni Pajarinen",
        "authorids": "~Wenyan_Yang1;~Ahmet_Tikna1;~Yi_Zhao6;~Yuying_Zhang1;~Luigi_Palopoli2;~Marco_Roveri1;~Joni_Pajarinen2",
        "gender": "M;M;M;F;M;M;",
        "homepage": ";https://webapps.unitn.it/du/en/Persona/PER0239380/Pubblicazioni;https://zhaoyi11.github.io/;https://rl.aalto.fi/team/;https://luigipalopoli.disi.unitn.it/;https://sites.google.com/view/marco-roveri/home;",
        "dblp": ";;51/4138-1;;p/LuigiPalopoli2;83/563;23/8355",
        "google_scholar": "https://scholar.google.com/citations?hl=en;qq8-F8UAAAAJ;https://scholar.google.com/citations?hl=en;;https://scholar.google.com/citations?hl=it;wFLeBBkAAAAJ;https://scholar.google.fi/citations?user=-2fJStwAAAAJ",
        "orcid": ";;0009-0002-9979-595X;;0000-0001-8813-8685;0000-0001-9483-3940;0000-0003-4469-8191",
        "linkedin": ";;;;luigi-palopoli-0a213012/;marcoroveri/;",
        "or_profile": "~Wenyan_Yang1;~Ahmet_Tikna1;~Yi_Zhao6;~Yuying_Zhang1;~Luigi_Palopoli2;~Marco_Roveri1;~Joni_Pajarinen2",
        "aff": "Aalto University;University of Trento;Aalto University;Aalto University;University of Trento;University of Trento;Aalto University",
        "aff_domain": "aalto.fi;unitn.it;aalto.fi;aalto.fi;unitn.it;unitn.it;aalto.fi",
        "position": "Postdoc;PhD student;PhD student;PhD student;Full Professor;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\nyang2025extracting,\ntitle={Extracting Visual Plans from Unlabeled Videos via Symbolic Guidance},\nauthor={Wenyan Yang and Ahmet Tikna and Yi Zhao and Yuying Zhang and Luigi Palopoli and Marco Roveri and Joni Pajarinen},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=HMcBBIg1Th}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=HMcBBIg1Th",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0;1;1;0",
        "aff_unique_norm": "Aalto University;University of Trento",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.aalto.fi;https://www.unitn.it",
        "aff_unique_abbr": "Aalto;UniTN",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;1;1;0",
        "aff_country_unique": "Finland;Italy"
    },
    {
        "id": "HT34hQcU91",
        "title": "GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Learning manipulation skills from human demonstration videos offers a promising path toward generalizable and interpretable robotic intelligence\u2014particularly through the lens of *actionable affordances*.  However, transferring such knowledge remains challenging due to:  1) a lack of large-scale datasets with precise affordance annotations, and 2) insufficient exploration of affordances in diverse manipulation contexts. To address these gaps, we introduce **HOVA-500K**, a large-scale, affordance-annotated dataset comprising 500,000 images across 1,726 object categories and 675 actions. We also release a standardized benchmarking suite for multi-modal affordance reasoning. Built upon HOVA-500K, we present **GLOVER++**, a *global-to-local* affordance training framework that effectively transfers actionable affordance knowledge from human demonstrations to downstream open-vocabulary reasoning tasks. GLOVER++ achieves state-of-the-art results on the HOVA-500K benchmark and demonstrates strong generalization across diverse downstream robotic manipulation tasks. \nBy explicitly modeling actionable affordances, GLOVER++ facilitates robust transfer across scenes, modalities, and tasks. We hope that HOVA-500K and the GLOVER++ framework will serve as valuable resources for bridging the gap between human demonstrations and robotic manipulation capabilities. We will release our dataset, code and models.",
        "keywords": "Actionable Affordance;Affordance Transfer;Vision-Language Model;Human Demonstrations;Robotic Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/41cd1d2d4f16b80c246b2181b1c57622ebf7d45a.zip",
        "author": "Teli Ma;Jia Zheng;Zifan Wang;Ziyao Gao;Jiaming Zhou;Junwei Liang",
        "authorids": "~Teli_Ma1;~Jia_Zheng6;~Zifan_Wang7;~Ziyao_Gao1;~Jiaming_Zhou1;~Junwei_Liang1",
        "gender": "M;M;M;F;M;M",
        "homepage": "https://teleema.github.io/;;https://zifanw.notion.site/;;https://jiaming-zhou.github.io/;https://junweiliang.me/",
        "dblp": "276/3611;;;;;62/10704-1",
        "google_scholar": "tW37g0UAAAAJ;wowRHOgAAAAJ;GaJXZ-UAAAAJ;https://scholar.google.com/citations?hl=zh-CN;b3y40w8AAAAJ;bMedjfUAAAAJ",
        "orcid": ";;;0009-0007-2424-0796;;0000-0003-2219-5569",
        "linkedin": ";https://linkedin.com/in/jia-zheng-b949b4362;;;;junweiliang/",
        "or_profile": "~Teli_Ma1;~Jia_Zheng6;~Zifan_Wang7;~Ziyao_Gao1;~Jiaming_Zhou1;~Junwei_Liang1",
        "aff": "Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology (Guangzhou);Hong Kong University of Science and Technology",
        "aff_domain": "hkust-gz.edu.cn;hkust-gz.edu.cn;hkust-gz.edu.cn;connect.hkust-gz.edu.cn;hkust-gz.edu.cn;ust.hk",
        "position": "PhD student;Undergrad student;PhD student;PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nma2025glover,\ntitle={{GLOVER}++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation},\nauthor={Teli Ma and Jia Zheng and Zifan Wang and Ziyao Gao and Jiaming Zhou and Junwei Liang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=HT34hQcU91}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=HT34hQcU91",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ust.hk",
        "aff_unique_abbr": "HKUST",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "HXJ6pUSn1L",
        "title": "FLARE: Robot Learning with Implicit World Modeling",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We introduce **F**uture **LA**tent **R**presentation Alignm**E**nt (**FLARE**), a novel framework that integrates predictive world modeling into robot policy learning.\nBy aligning features from a diffusion transformer with latent embeddings of future observations, **FLARE** enables a diffusion transformer policy to anticipate latent representations of future observations, allowing it to reason about long-term consequences while generating actions.\nRemarkably lightweight, **FLARE** requires only minimal architectural modifications---adding a few tokens to standard vision-language-action (VLA) models---yet delivers substantial performance gains.\nAcross two challenging multitask simulation imitation learning benchmarks spanning single-arm and humanoid tabletop manipulation, **FLARE** achieves state-of-the-art performance, outperforming prior policy learning baselines by up to 26\\%.\nMoreover, **FLARE** unlocks the ability to co-train with human egocentric video demonstrations lacking action labels, significantly boosting policy generalization to a novel object with unseen geometry with as few as 1 robot demonstration.\nOur results establish **FLARE** as a general and scalable approach for combining implicit world modeling with high-frequency robotic control.",
        "keywords": "World Model;VLA;Humanoid Robotics",
        "primary_area": "",
        "supplementary_material": "/attachment/fab99d56793ab121fe588f2bafcb5a667fe10e0b.zip",
        "author": "Ruijie Zheng;Jing Wang;Scott Reed;Johan Bjorck;Yu Fang;Fengyuan Hu;Joel Jang;Kaushil Kundalia;Zongyu Lin;Lo\u00efc Magne;Avnish Narayan;You Liang Tan;Guanzhi Wang;Qi Wang;Jiannan Xiang;Yinzhen Xu;Seonghyeon Ye;Jan Kautz;Furong Huang;Yuke Zhu;Linxi Fan",
        "authorids": "~Ruijie_Zheng1;~Jing_Wang79;~Scott_Reed1;jbjorck@nvidia.com;~Yu_Fang2;~Fengyuan_Hu2;~Joel_Jang1;~Kaushil_Kundalia1;~Zongyu_Lin1;~Lo\u00efc_Magne1;~Avnish_Narayan1;~You_Liang_Tan1;~Guanzhi_Wang1;qiwang@nvidia.com;~Jiannan_Xiang1;~Yinzhen_Xu1;~Seonghyeon_Ye1;~Jan_Kautz1;~Furong_Huang1;~Yuke_Zhu1;~Linxi_Fan2",
        "gender": ";M;;;M;;M;;M;M;M;;M;;M;M;M;;F;M;",
        "homepage": "http://www.ruijiezheng.com;;https://scottreed.info;;http://squarefk.com/;;https://joeljang.github.io/;;;https://github.com/loicmagne;;https://youliangtan.github.io/;https://www.guanzhi.me/;;https://szxiangjn.github.io/;https://xyz-99.github.io;https://vano1205.github.io/;http://jankautz.com;https://furong-huang.com;https://yukezhu.me/;",
        "dblp": "294/8474;;;;;;;;273/7646;;;;239/8731;;230/3430;327/1997;301/8927;48/6214;72/8513;133/1772;154/6778",
        "google_scholar": ";cdL5PqgAAAAJ;jEANvfgAAAAJ;;;;xL-7eFEAAAAJ;;4ahRAd4AAAAJ;;jKvDskQAAAAJ;;QDmEj4MAAAAJ;;l8BS2wsAAAAJ;VaFCcJ8AAAAJ;https://scholar.google.co.kr/citations?user=JfGGjBoAAAAJ;P9FclNEAAAAJ;13yyuCcAAAAJ;mWGyYMsAAAAJ;sljtWIUAAAAJ",
        "orcid": ";;;;;;;;;;;;;;;;;;;;",
        "linkedin": ";jing-wang-457a67153/;;;;;joel-jang-1289331a5/;kaushilk?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app;;;avnishn/;youliang-tan/;;;;yinzhen-xu-620615190/;;;;;",
        "or_profile": "~Ruijie_Zheng1;~Jing_Wang79;~Scott_Reed1;jbjorck@nvidia.com;~Yu_Fang2;~Fengyuan_Hu2;~Joel_Jang1;~Kaushil_Kundalia1;~Zongyu_Lin1;~Lo\u00efc_Magne1;~Avnish_Narayan1;~You_Liang_Tan1;~Guanzhi_Wang1;qiwang@nvidia.com;~Jiannan_Xiang1;~Yinzhen_Xu1;~Seonghyeon_Ye1;~Jan_Kautz1;~Furong_Huang1;~Yuke_Zhu1;~Linxi_Fan2",
        "aff": "University of Maryland, College Park;Nanyang Technological University;NVIDIA;;;;Department of Computer Science, University of Washington;NVIDIA;University of California, Los Angeles;;NVIDIA;NVIDIA;California Institute of Technology;;University of California, San Diego;NVIDIA;Korea Advanced Institute of Science & Technology;NVIDIA;University of Maryland;Computer Science Department, University of Texas, Austin;NVIDIA",
        "aff_domain": "cs.umd.edu;ntu.edu.sg;nvidia.com;;;;cs.washington.edu;nvidia.com;cs.ucla.edu;;nvidia.com;nvidia.com;caltech.edu;;ucsd.edu;nvidia.com;kaist.ac.kr;nvidia.com;umd.edu;cs.utexas.edu;nvidia.com",
        "position": "PhD student;PhD student;Principal Researcher;;;;PhD student;Research Engineer;PhD student;;Researcher;Researcher;PhD student;;PhD student;Researcher;PhD student;VP Research;Associate Professor;Associate Professor;Researcher",
        "bibtex": "@inproceedings{\nzheng2025flare,\ntitle={{FLARE}: Robot Learning with Implicit World Modeling},\nauthor={Ruijie Zheng and Jing Wang and Scott Reed and Johan Bjorck and Yu Fang and Fengyuan Hu and Joel Jang and Kaushil Kundalia and Zongyu Lin and Lo{\\\"\\i}c Magne and Avnish Narayan and You Liang Tan and Guanzhi Wang and Qi Wang and Jiannan Xiang and Yinzhen Xu and Seonghyeon Ye and Jan Kautz and Furong Huang and Yuke Zhu and Linxi Fan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=HXJ6pUSn1L}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=HXJ6pUSn1L",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            21,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3;3;3;4;2;5;3;2;2;6;3;7;2;8;2;0;9;2",
        "aff_unique_norm": "University of Maryland;Nanyang Technological University;NVIDIA;;University of Washington;University of California, Los Angeles;California Institute of Technology;University of California, San Diego;Korea Advanced Institute of Science and Technology;University of Texas at Austin",
        "aff_unique_dep": ";;NVIDIA Corporation;;Department of Computer Science;;;;;Computer Science Department",
        "aff_unique_url": "https://www/umd.edu;https://www.ntu.edu.sg;https://www.nvidia.com;;https://www.washington.edu;https://www.ucla.edu;https://www.caltech.edu;https://www.ucsd.edu;https://www.kaist.ac.kr;https://www.utexas.edu",
        "aff_unique_abbr": "UMD;NTU;NVIDIA;;UW;UCLA;Caltech;UCSD;KAIST;UT Austin",
        "aff_campus_unique_index": "0;2;3;4;5;6",
        "aff_campus_unique": "College Park;;Seattle;Los Angeles;Pasadena;San Diego;Austin",
        "aff_country_unique_index": "0;1;0;0;0;0;0;0;0;0;0;3;0;0;0;0",
        "aff_country_unique": "United States;Singapore;;South Korea"
    },
    {
        "id": "HprBJupvvM",
        "title": "Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "How can robot manipulation policies generalize to novel tasks involving unseen object types and new motions? In this paper, we provide a solution in terms of predicting motion information from web data through human video generation and conditioning a robot policy on the generated video. Instead of attempting to scale robot data collection which is expensive, we show how we can leverage video generation models trained on easily available web data, for enabling generalization. \\textit{Our approach Gen2Act casts language-conditioned manipulation as zero-shot human video generation followed by execution with a single policy conditioned on the generated video.} To train the policy, we use an order of magnitude less robot interaction data compared to what the video prediction model was trained on. Gen2Act doesn't require fine-tuning the video model at all and we directly use a pre-trained model for generating human videos. Our results on diverse real-world scenarios show how Gen2Act enables manipulating unseen object types and performing novel motions for tasks not present in the robot data.",
        "keywords": "Robot Manipulation;Human Video Generation;Human Videos",
        "primary_area": "",
        "supplementary_material": "/attachment/4d7425e111afc38d718b2d47c9ee8bc884f0b91e.zip",
        "author": "Homanga Bharadhwaj;Debidatta Dwibedi;Abhinav Gupta;Shubham Tulsiani;Carl Doersch;Ted Xiao;Dhruv Shah;Fei Xia;Dorsa Sadigh;Sean Kirmani",
        "authorids": "~Homanga_Bharadhwaj1;~Debidatta_Dwibedi1;~Abhinav_Gupta1;~Shubham_Tulsiani1;~Carl_Doersch1;~Ted_Xiao1;~Dhruv_Shah1;~Fei_Xia1;~Dorsa_Sadigh1;~Sean_Kirmani1",
        "gender": "M;M;M;M;M;M;M;M;F;M",
        "homepage": "https://homangab.github.io/;https://debidatta.github.io/;http://www.cs.cmu.edu/~abhinavg;https://shubhtuls.github.io/;;https://www.tedxiao.me;http://cs.berkeley.edu/~shah;;https://dorsa.fyi/;https://kirmani.io/",
        "dblp": "223/5842;160/3739;36/7024-1;135/6623;12/8654;198/0598;;;117/3174;",
        "google_scholar": "https://scholar.google.ca/citations?user=wwW4HRQAAAAJ;EPfOJwQAAAAJ;https://scholar.google.com.tw/citations?user=bqL73OkAAAAJ;06rffEkAAAAJ;SBTxvCoAAAAJ;;;pqP5_PgAAAAJ;ZaJEZpYAAAAJ;iyEuK8kAAAAJ",
        "orcid": ";;;;;;;0000-0003-4343-1444;;",
        "linkedin": ";;;;;;;;;skirmani",
        "or_profile": "~Homanga_Bharadhwaj1;~Debidatta_Dwibedi1;~Abhinav_Gupta1;~Shubham_Tulsiani1;~Carl_Doersch1;~Ted_Xiao1;~Dhruv_Shah1;~Fei_Xia1;~Dorsa_Sadigh1;~Sean_Kirmani1",
        "aff": "School of Computer Science, Carnegie Mellon University;Google;SKILD AI+Carnegie Mellon University;Carnegie Mellon University;Google DeepMind;;Google DeepMind;Google;Google+Stanford University;Google DeepMind",
        "aff_domain": "andrew.cmu.edu;google.com;skild.ai+cmu.edu;cmu.edu;google.com;;google.com;google.com;google.com+stanford.edu;google.com",
        "position": "PhD student;Google;Principal Researcher+Full Professor;Assistant Professor;Research Scientist;;Research Scientist;Researcher;Researcher+Assistant Professor;Researcher",
        "bibtex": "@inproceedings{\nbharadhwaj2025genact,\ntitle={Gen2Act: Human Video Generation in Novel Scenarios enables Generalizable Robot Manipulation},\nauthor={Homanga Bharadhwaj and Debidatta Dwibedi and Abhinav Gupta and Shubham Tulsiani and Carl Doersch and Ted Xiao and Dhruv Shah and Fei Xia and Dorsa Sadigh and Sean Kirmani},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=HprBJupvvM}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=HprBJupvvM",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2+0;0;1;3;1;1;1+4;1",
        "aff_unique_norm": "Carnegie Mellon University;Google;SKILD AI;;Stanford University",
        "aff_unique_dep": "School of Computer Science;Google;;;",
        "aff_unique_url": "https://www.cmu.edu;https://www.google.com;;;https://www.stanford.edu",
        "aff_unique_abbr": "CMU;Google;;;Stanford",
        "aff_campus_unique_index": "0;1;;1;1+3",
        "aff_campus_unique": "Pittsburgh;Mountain View;;Stanford",
        "aff_country_unique_index": "0;0;0;0;2;2;0;0+0;2",
        "aff_country_unique": "United States;;United Kingdom"
    },
    {
        "id": "HqqyJ9A2fy",
        "title": "Neural Robot Dynamics",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Accurate and efficient simulation of modern robots remains challenging due to their high degrees of freedom and intricate mechanisms. Neural simulators have emerged as a promising alternative to traditional analytical simulators, capable of efficiently predicting complex dynamics and adapting to real-world data; however, existing neural simulators typically require application-specific training and fail to generalize to novel tasks and/or environments, primarily due to inadequate representations of the global state. In this work, we address the problem of learning generalizable neural simulators for robots that are structured as articulated rigid bodies. We propose *NeRD* (Neural Robot Dynamics), learned robot-specific dynamics models for predicting future states for articulated rigid bodies under contact constraints. *NeRD* uniquely replaces the low-level dynamics and contact solvers in an analytical simulator and employs a robot-centric and spatially-invariant simulation state representation. We integrate the learned *NeRD* models as an interchangeable backend solver within a state-of-the-art robotics simulator. We conduct extensive experiments to show that the *NeRD* simulators are stable and accurate over a thousand simulation steps; generalize across tasks and environment configurations; enable policy learning exclusively in a neural engine; and, unlike most classical simulators, can be fine-tuned from real-world data to bridge the gap between simulation and reality.",
        "keywords": "Robot Model Learning; Robotics Simulation; Neural Simulation",
        "primary_area": "",
        "supplementary_material": "/attachment/d924fa3f2e0b67ceef855b7a5cc7cebabd993370.zip",
        "author": "Jie Xu;Eric Heiden;Iretiayo Akinola;Dieter Fox;Miles Macklin;Yashraj Narang",
        "authorids": "~Jie_Xu7;~Eric_Heiden1;~Iretiayo_Akinola1;~Dieter_Fox1;~Miles_Macklin1;~Yashraj_Narang1",
        "gender": "M;;M;M;M;M",
        "homepage": "https://people.csail.mit.edu/jiex;https://eric-heiden.com/;;https://homes.cs.washington.edu/~fox/;https://mmacklin.com;",
        "dblp": "37/5126-28;;;f/DieterFox;;215/6022.html",
        "google_scholar": "3Tj5lWEAAAAJ;iWmtv7gAAAAJ;e1zesfMAAAAJ;DqXsbPAAAAAJ;;M3NuG7AAAAAJ",
        "orcid": ";;;;;0000-0001-5445-3759",
        "linkedin": ";;;;;",
        "or_profile": "~Jie_Xu7;~Eric_Heiden1;~Iretiayo_Akinola1;~Dieter_Fox1;~Miles_Macklin1;~Yashraj_Narang1",
        "aff": "NVIDIA;NVIDIA;NVIDIA;NVIDIA Research+Department of Computer Science;NVIDIA;NVIDIA",
        "aff_domain": "nvidia.com;nvidia.com;nvidia.com;nvidia.com+cs.washington.edu;nvidia.com;nvidia.com",
        "position": "Researcher;Researcher;Researcher;Senior Director of Robotics Research+Full Professor;Principal Engineer;Researcher",
        "bibtex": "@inproceedings{\nxu2025neural,\ntitle={Neural Robot Dynamics},\nauthor={Jie Xu and Eric Heiden and Iretiayo Akinola and Dieter Fox and Miles Macklin and Yashraj Narang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=HqqyJ9A2fy}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=HqqyJ9A2fy",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0+1;0;0",
        "aff_unique_norm": "NVIDIA;Unknown Institution",
        "aff_unique_dep": "NVIDIA Corporation;Department of Computer Science",
        "aff_unique_url": "https://www.nvidia.com;",
        "aff_unique_abbr": "NVIDIA;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "Hu3NoPMAg4",
        "title": "One Demo is Worth a Thousand Trajectories: Action-View Augmentation for Visuomotor Policies",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Visuomotor policies for manipulation have demonstrated remarkable potential in modeling complex robotic behaviors, yet minor alterations in the robot\u2019s initial configuration and unseen obstacles easily lead to out-of-distribution observations. Without extensive data collection effort, these result in catastrophic execution failures. In this work, we introduce an effective data augmentation framework that generates visually realistic fisheye image sequences and corresponding physically feasible action trajectories from real-world eye-in-hand demonstrations, captured with a portable parallel gripper with a single fisheye camera. We introduce a novel Gaussian Splatting formulation, adapted to wide FoV fisheye cameras, to reconstruct and edit the 3D scene with unseen objects. We utilize trajectory optimization to generate smooth, collision-free, view-rendering-friendly action trajectories and render visual observations from corresponding novel views. Comprehensive experiments in simulation and the real world show that our augmentation framework improves the success rate for various manipulation tasks in both the same scene and the augmented scene with obstacles requiring collision avoidance.",
        "keywords": "visuomotor policies;LfD;manipulation;3D vision;gaussian splat;motion planning",
        "primary_area": "",
        "supplementary_material": "/attachment/c4f93a51e33a17448ce861273cc52ed0365686e8.zip",
        "author": "Chuer Pan;Litian Liang;Dominik Bauer;Eric Cousineau;Benjamin Burchfiel;Siyuan Feng;Shuran Song",
        "authorids": "~Chuer_Pan1;~Litian_Liang1;~Dominik_Bauer1;~Eric_Cousineau1;~Benjamin_Burchfiel1;~Siyuan_Feng5;~Shuran_Song3",
        "gender": "F;;;M;M;;F",
        "homepage": "http://chuerpan.com/;https://www.litianliang.org/;https://dornik.github.io;https://eacousineau.com/;http://www.benburchfiel.com/;https://scholar.google.com/citations?user=W9HvNZwAAAAJ&hl=en;https://shurans.github.io/",
        "dblp": ";301/7979;00/7183;;136/9247;;",
        "google_scholar": "HBMZ02EAAAAJ;I9VWDKcAAAAJ;4KgUhEEAAAAJ;LOTPw48AAAAJ;eGoTK1YAAAAJ;W9HvNZwAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;0000-0002-1260-1319;;;;",
        "linkedin": ";litian-liang-0b6097171/;;eacousineau;benburchfiel/;;",
        "or_profile": "~Chuer_Pan1;~Litian_Liang1;~Dominik_Bauer1;~Eric_Cousineau1;~Benjamin_Burchfiel1;~Siyuan_Feng5;~Shuran_Song3",
        "aff": "Stanford University;;Columbia University;Toyota Research Institute;Dexterous Manipulation Group, Toyota Research Institute;;Stanford University",
        "aff_domain": "stanford.edu;;columbia.edu;tri.global;tri.global;;stanford.edu",
        "position": "PhD student;;Postdoc;Researcher;Researcher;;Assistant Professor",
        "bibtex": "@inproceedings{\npan2025one,\ntitle={One Demo is Worth a Thousand Trajectories: Action-View Augmentation for Visuomotor Policies},\nauthor={Chuer Pan and Litian Liang and Dominik Bauer and Eric Cousineau and Benjamin Burchfiel and Siyuan Feng and Shuran Song},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Hu3NoPMAg4}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Hu3NoPMAg4",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3;3;1;0",
        "aff_unique_norm": "Stanford University;;Columbia University;Toyota Research Institute",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.stanford.edu;;https://www.columbia.edu;https://www.tri.global",
        "aff_unique_abbr": "Stanford;;Columbia;TRI",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "IV35hjIZwz",
        "title": "In-Context Iterative Policy Improvement for Dynamic Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Attention-based architectures trained on internet-scale language data have demonstrated state of the art reasoning ability for various language-based tasks, such as logic problems and textual reasoning. Additionally, these Large Language Models (LLMs) have exhibited the ability to perform few-shot prediction via in-context learning, in which input-output examples provided in the prompt are generalized to new inputs. This ability furthermore extends beyond standard language tasks, enabling few-shot learning for general patterns. In this work, we consider the application of in-context learning with pre-trained language models for dynamic manipulation. Dynamic manipulation introduces several crucial challenges, including increased dimensionality, complex dynamics, and partial observability. To address this, we take an iterative approach, and formulate our in-context learning problem to predict adjustments to a parametric policy based on previous interactions. We show across several tasks in simulation and on a physical robot that utilizing in-context learning outperforms alternative methods in the low data regime.",
        "keywords": "In-Context Learning;Dynamic Manipulation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Mark Van der Merwe;Devesh K. Jha",
        "authorids": "~Mark_Van_der_Merwe1;~Devesh_K._Jha1",
        "gender": "M;M",
        "homepage": "https://mvandermerwe.github.io/;https://www.merl.com/people/jha",
        "dblp": "249/5378;158/6488",
        "google_scholar": "cKmwbi0AAAAJ;guk0mCkAAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Mark_Van_der_Merwe1;~Devesh_K._Jha1",
        "aff": "University of Michigan - Ann Arbor;Mitsubishi Electric Research Labs",
        "aff_domain": "umich.edu;merl.com",
        "position": "PhD student;Research Scientist",
        "bibtex": "@inproceedings{\nmerwe2025incontext,\ntitle={In-Context Iterative Policy Improvement for Dynamic Manipulation},\nauthor={Mark Van der Merwe and Devesh K. Jha},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=IV35hjIZwz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=IV35hjIZwz",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "University of Michigan;Mitsubishi Electric Research Laboratories",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umich.edu;https://www.merl.com",
        "aff_unique_abbr": "UM;MERL",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Ann Arbor;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "Ict1OjU9gl",
        "title": "Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Vision-Language-Action (VLA) models offer a pivotal approach to learning robotic manipulation at scale by repurposing large pre-trained Vision-Language-Models (VLM) to output robotic actions. However, adapting VLMs for robotic domains comes with an unnecessarily high computational cost, which we attribute to the tokenization scheme of visual inputs. In this work, we aim to enable efficient VLA training by proposing Oat-VLA, an Object-Agent-centric Tokenization for VLAs. Building on the insights of object-centric representation learning, our method introduces an inductive bias towards scene objects and the agent's own visual information. As a result, we find that Oat-VLA can drastically reduce the number of visual tokens  to just a few tokens without sacrificing performance. We reveal that Oat-VLA converges at least twice as fast as OpenVLA on the LIBERO suite, as well as outperform OpenVLA in diverse real-world pick and place tasks.",
        "keywords": "visual-language-action models;object-centric representations;robotic manipulation;imitation learning",
        "primary_area": "",
        "supplementary_material": "/attachment/961a2f003a0cd5596e1343ce32096c633263ef33.zip",
        "author": "Rokas Bendikas;Daniel Dijkman;Markus Peschl;Sanjay Haresh;Pietro Mazzaglia",
        "authorids": "~Rokas_Bendikas1;~Daniel_Dijkman1;~Markus_Peschl1;~Sanjay_Haresh1;~Pietro_Mazzaglia1",
        "gender": "M;;M;M;",
        "homepage": "https://rokas-bendikas.github.io/;;https://mlpeschl.com;https://www.sanjayharesh.com/;https://mazpie.github.io/",
        "dblp": ";;298/3144;253/0473;266/6084",
        "google_scholar": "_4b1EvAAAAAJ;;hinquVUAAAAJ;https://scholar.google.com/citations?hl=en;c-PYVTgAAAAJ",
        "orcid": "0000-0002-2801-1743;;;;0000-0003-3319-5986",
        "linkedin": "rokas-bendikas;;;;pietromazzaglia/",
        "or_profile": "~Rokas_Bendikas1;~Daniel_Dijkman1;~Markus_Peschl1;~Sanjay_Haresh1;~Pietro_Mazzaglia1",
        "aff": "University College London, University of London;;Qualcomm Inc, QualComm;Qualcomm, Inc.;Qualcomm Inc, QualComm",
        "aff_domain": "ucl.ac.uk;;qti.qualcomm.com;qti.qualcomm.com;qti.qualcomm.com",
        "position": "PhD student;;Researcher;Researcher;Researcher",
        "bibtex": "@inproceedings{\nbendikas2025focusing,\ntitle={Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models},\nauthor={Rokas Bendikas and Daniel Dijkman and Markus Peschl and Sanjay Haresh and Pietro Mazzaglia},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Ict1OjU9gl}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Ict1OjU9gl",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3;2",
        "aff_unique_norm": "University College London;;Qualcomm Incorporated;Qualcomm, Inc.",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ucl.ac.uk;;https://www.qualcomm.com;https://www.qualcomm.com",
        "aff_unique_abbr": "UCL;;Qualcomm;Qualcomm",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;2;2;2",
        "aff_country_unique": "United Kingdom;;United States"
    },
    {
        "id": "IuiB5iaMxy",
        "title": "Decentralized Aerial Manipulation of a Cable-Suspended Load Using Multi-Agent Reinforcement Learning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "This paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments:  https://github.com/anonymousCoRL/MDCM_CoRL2025",
        "keywords": "aerial manipulation;multi-agent reinforcement learning;micro-aerial vehicles",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jack Zeng;Andreu Matoses Gimenez;Eugene Vinitsky;Javier Alonso-Mora;Sihao Sun",
        "authorids": "~Jack_Zeng1;~Andreu_Matoses_Gimenez1;~Eugene_Vinitsky1;~Javier_Alonso-Mora1;~Sihao_Sun1",
        "gender": "M;M;M;M;M",
        "homepage": "https://jackzeng-robotics.github.io/;https://andreumatoses.github.io/;https://eugenevinitsky.github.io;https://www.autonomousrobots.nl/;https://sihaosun.github.io/",
        "dblp": ";;207/7772;18/9186;",
        "google_scholar": "YekQYJ8AAAAJ;mQvQ-DYAAAAJ;6dr5fLEAAAAJ;JydqDdEAAAAJ;OE18XDEAAAAJ",
        "orcid": ";0009-0000-2903-0834;;0000-0003-0058-570X;",
        "linkedin": "jackzeng27/;andreu-matoses;;javier-alonso-mora-8483b31b/?originalSubdomain=nl;",
        "or_profile": "~Jack_Zeng1;~Andreu_Matoses_Gimenez1;~Eugene_Vinitsky1;~Javier_Alonso-Mora1;~Sihao_Sun1",
        "aff": "Delft University of Technology;Delft University of Technology;New York University;Delft University of Technology;Delft University of Technology",
        "aff_domain": "student.tudelft.nl;tudelft.nl;nyu.edu;tudelft.nl;tudelft.nl",
        "position": "MS student;PhD student;Assistant Professor;Associate Professor;Researcher",
        "bibtex": "@inproceedings{\nzeng2025decentralized,\ntitle={Decentralized Aerial Manipulation of a Cable-Suspended Load Using Multi-Agent Reinforcement Learning},\nauthor={Jack Zeng and Andreu Matoses Gimenez and Eugene Vinitsky and Javier Alonso-Mora and Sihao Sun},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=IuiB5iaMxy}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=IuiB5iaMxy",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Delft University of Technology;New York University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tudelft.nl;https://www.nyu.edu",
        "aff_unique_abbr": "TU Delft;NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0;0",
        "aff_country_unique": "Netherlands;United States"
    },
    {
        "id": "J1Ekhe08QU",
        "title": "Articulated Object Estimation in the Wild",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Understanding the 3D motion of articulated objects is essential in robotic scene understanding, mobile manipulation, and motion planning. Prior methods for articulation estimation have primarily focused on controlled settings, assuming either fixed camera viewpoints or direct observations of various object states, which tend to fail in more realistic, unconstrained environments. In contrast, humans effortlessly infer articulation modes by watching others manipulating objects. Inspired by this, we introduce ArtiPoint, a novel estimation framework capable of inferring articulated object models under dynamic camera motion and partial observability. By combining deep point tracking with a factor graph optimization framework, ArtiPoint robustly estimates articulated part trajectories and articulation axes directly from raw RGB-D videos. To foster future research in this domain, we introduce Arti4D, the first ego-centric in-the-wild dataset capturing articulated object interactions at a scene level, accompanied with articulation labels and ground truth camera poses. We benchmark ArtiPoint against a range of classical and modern deep learning baselines, demonstrating its superior performance on Arti4D. We make our code and Arti4D publicly available at redacted-for-review.",
        "keywords": "Articulated Object Estimation;3D Scene Understanding;Interactive Perception",
        "primary_area": "",
        "supplementary_material": "/attachment/a4cb98bb365673d30928ac5813c4c9ff14caf38b.pdf",
        "author": "Abdelrhman Werby;Martin B\u00fcchner;Adrian R\u00f6fer;Chenguang Huang;Wolfram Burgard;Abhinav Valada",
        "authorids": "~Abdelrhman_Werby1;~Martin_B\u00fcchner1;~Adrian_R\u00f6fer1;~Chenguang_Huang1;~Wolfram_Burgard3;~Abhinav_Valada1",
        "gender": "M;;M;;M;M",
        "homepage": ";;http://aroefer.de;;https://www.utn.de/person/wolfram-burgard/;https://rl.uni-freiburg.de/people/valada",
        "dblp": ";;267/9279;;b/WolframBurgard;81/9531",
        "google_scholar": "T5wMfRIAAAAJ;;sQNEcpkAAAAJ;;zj6FavAAAAAJ;https://scholar.google.de/citations?user=LcARjz0AAAAJ",
        "orcid": ";;0000-0001-6132-0989;;0000-0002-5680-6500;0000-0003-4710-3114",
        "linkedin": "abdelrhman-werby-b06a671ab/;;;;burgard/?originalSubdomain=de;avalada",
        "or_profile": "~Abdelrhman_Werby1;~Martin_B\u00fcchner1;~Adrian_R\u00f6fer1;~Chenguang_Huang1;~Wolfram_Burgard3;~Abhinav_Valada1",
        "aff": "Universit\u00e4t Stuttgart+Albert-Ludwigs-Universit\u00e4t Freiburg;;University of Freiburg, Albert-Ludwigs-Universit\u00e4t Freiburg;;University of Technology Nuremberg;University of Freiburg",
        "aff_domain": "uni-stuttgart.de+uni-freiburg.de;;cs.uni-freiburg.de;;utn.de;uni-freiburg.de",
        "position": "PhD student+MS student;;PhD student;;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nwerby2025articulated,\ntitle={Articulated Object Estimation in the Wild},\nauthor={Abdelrhman Werby and Martin B{\\\"u}chner and Adrian R{\\\"o}fer and Chenguang Huang and Wolfram Burgard and Abhinav Valada},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=J1Ekhe08QU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=J1Ekhe08QU",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;2;3;2;4;3",
        "aff_unique_norm": "University of Stuttgart;Albert-Ludwigs-Universit\u00e4t Freiburg;;University of Freiburg;Nuremberg University of Technology",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.uni-stuttgart.de;https://www.uni-freiburg.de;;https://www.uni-freiburg.de;https://www.tu-nuernberg.de",
        "aff_unique_abbr": "Uni Stuttgart;Albert-Ludwigs-Universit\u00e4t;;UoF;TUN",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Freiburg",
        "aff_country_unique_index": "0+0;0;0;0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "JEGOuknVml",
        "title": "SIREN: Semantic, Initialization-Free Registration of Multi-Robot Gaussian Splatting Maps",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We present SIREN for registration of multi-robot\nGaussian Splatting (GSplat) maps, with zero access to camera\nposes, images, and inter-map transforms for initialization or fusion\nof local submaps. To realize these capabilities, SIREN harnesses\nthe versatility and robustness of semantics in three critical ways to\nderive a rigorous registration pipeline for multi-robot GSplat maps.\nFirst, SIREN utilizes semantics to identify feature-rich regions\nof the local maps where the registration problem is better posed,\neliminating the need for any initialization which is generally\nrequired in prior work. Second, SIREN identifies candidate\ncorrespondences between Gaussians in the local maps using\nrobust semantic features, constituting the foundation for robust\ngeometric optimization, coarsely aligning 3D Gaussian primitives\nextracted from the local maps. Third, this key step enables\nsubsequent photometric refinement of the transformation between\nthe submaps, where SIREN leverages novel-view synthesis in\nGSplat maps along with a semantics-based image filter to compute\na high-accuracy non-rigid transformation for the generation of a\nhigh-fidelity fused map. We demonstrate the superior performance\nof SIREN compared to competing baselines across a range of\nreal-world datasets, and in particular, across the most widely\nused robot hardware platforms, including a manipulator, drone,\nand quadruped. \nIn fact, in the most challenging scenes\nwhere accurate feature matching is extremely challenging,\nSIREN achieves about 90x\nsmaller rotation errors, 300x smaller translation errors, and\n44x smaller scale errors, compared to\ncompeting methods.\nWe will release the code and provide\na link to the project page after the review process.",
        "keywords": "Multi-Robot Radiance Field Mapping;Gaussian Splatting",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Olao Shorinwa;Jiankai Sun;Mac Schwager;Anirudha Majumdar",
        "authorids": "~Olao_Shorinwa1;~Jiankai_Sun6;~Mac_Schwager1;~Anirudha_Majumdar1",
        "gender": ";;M;M",
        "homepage": ";;https://msl.stanford.edu/;https://irom-lab.princeton.edu/majumdar/",
        "dblp": ";121/4211;22/7012;116/6436",
        "google_scholar": ";726MCb8AAAAJ;-EqbTXoAAAAJ;ibu3FwsAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Olao_Shorinwa1;~Jiankai_Sun6;~Mac_Schwager1;~Anirudha_Majumdar1",
        "aff": ";Stanford University;Stanford University;Google+Princeton University",
        "aff_domain": ";stanford.edu;stanford.edu;google.com+princeton.edu",
        "position": ";PhD student;Associate Professor;Researcher+Associate Professor",
        "bibtex": "@inproceedings{\nshorinwa2025siren,\ntitle={{SIREN}: Semantic, Initialization-Free Registration of Multi-Robot Gaussian Splatting Maps},\nauthor={Olao Shorinwa and Jiankai Sun and Mac Schwager and Anirudha Majumdar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=JEGOuknVml}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JEGOuknVml",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;2+3",
        "aff_unique_norm": ";Stanford University;Google;Princeton University",
        "aff_unique_dep": ";;Google;",
        "aff_unique_url": ";https://www.stanford.edu;https://www.google.com;https://www.princeton.edu",
        "aff_unique_abbr": ";Stanford;Google;Princeton",
        "aff_campus_unique_index": "1;1;2",
        "aff_campus_unique": ";Stanford;Mountain View",
        "aff_country_unique_index": "1;1;1+1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "JM2vDI6DlP",
        "title": "VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Human drivers rely on commonsense reasoning to navigate diverse and dynamic real-world scenarios. Existing end-to-end (E2E) autonomous driving (AD) models are typically optimized to mimic driving patterns observed in data, without capturing the underlying reasoning processes.  This limitation constrains their ability to handle challenging driving scenarios. To close this gap, we propose VLM-AD, a method that leverages vision-language models (VLMs) as teachers to enhance training by providing additional supervision that incorporates unstructured reasoning information and structured action labels. Such supervision enhances the model's ability to learn richer feature representations that capture the rationale behind driving patterns. Importantly, our method does not require a VLM during inference, making it practical for real-time deployment. When integrated with state-of-the-art methods, VLM-AD achieves significant improvements in planning accuracy and reduced collision rates on the nuScenes dataset. It further improves route completion and driving scores under closed-loop evaluation, demonstrating its effectiveness in long-horizon, interactive driving scenarios and its potential for safe and reliable real-world deployment.",
        "keywords": "End-to-End Autonomous Driving;Vision-Language Model",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yi Xu;Yuxin Hu;Zaiwei Zhang;Gregory P. Meyer;Siva Karthik Mustikovela;Siddhartha Srinivasa;Eric M. Wolff;Xin Huang",
        "authorids": "~Yi_Xu9;~Yuxin_Hu2;~Zaiwei_Zhang1;~Gregory_P._Meyer1;~Siva_Karthik_Mustikovela1;~Siddhartha_Srinivasa1;~Eric_M._Wolff1;~Xin_Huang8",
        "gender": "M;M;M;M;M;M;;",
        "homepage": "https://sites.google.com/view/homepage-of-yi-xu;;https://sites.google.com/a/utexas.edu/zaiwei-zhang/;http://gregmeyer.info/;http://sivakm.github.io;https://goodrobot.ai;;",
        "dblp": "14/5580-5;;186/4421;143/2072;188/6172;;;",
        "google_scholar": "https://scholar.google.com.hk/citations?user=12bRAdsAAAAJ;yO9Sz6AAAAAJ;;https://scholar.google.com/citations?hl=en;6oiQVn4AAAAJ;https://scholar.google.com.tw/citations?user=RCi98EAAAAAJ;;",
        "orcid": "0000-0001-5857-4179;;;;;;;",
        "linkedin": "yi-xu-884755185/;;;;msivak/;;;",
        "or_profile": "~Yi_Xu9;~Yuxin_Hu2;~Zaiwei_Zhang1;~Gregory_P._Meyer1;~Siva_Karthik_Mustikovela1;~Siddhartha_Srinivasa1;~Eric_M._Wolff1;~Xin_Huang8",
        "aff": "Northeastern University;GM Cruise LLC;Meta Facebook;Meta Facebook+Cruise;GM Cruise LLC;Cruise+University of Washington;;",
        "aff_domain": "northeastern.edu;getcruise.com;meta.com;meta.com+getcruise.com;getcruise.com;getcruise.com+washington.edu;;",
        "position": "PhD student;Researcher;Researcher;Researcher+Researcher;Researcher;Researcher+Full Professor;;",
        "bibtex": "@inproceedings{\nxu2025vlmad,\ntitle={{VLM}-{AD}: End-to-End Autonomous Driving through Vision-Language Model Supervision},\nauthor={Yi Xu and Yuxin Hu and Zaiwei Zhang and Gregory P. Meyer and Siva Karthik Mustikovela and Siddhartha Srinivasa and Eric M. Wolff and Xin Huang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=JM2vDI6DlP}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JM2vDI6DlP",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;2+3;1;3+4;5;5",
        "aff_unique_norm": "Northeastern University;GM Cruise LLC;Meta;Cruise;University of Washington;",
        "aff_unique_dep": ";;Meta Platforms, Inc.;;;",
        "aff_unique_url": "https://www.northeastern.edu;https://www.gmcruise.com;https://meta.com;https://www.cruise.com;https://www.washington.edu;",
        "aff_unique_abbr": "NEU;GM Cruise;Meta;Cruise;UW;",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "JXBm4Xfrvj",
        "title": "Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex Quadruped Mobility",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Reinforcement learning (RL)-based legged locomotion controllers often require meticulous reward tuning to track velocities or goal positions while preserving smooth motion on various terrains. \nMotion imitation methods via RL using demonstration data reduce reward engineering but fail to generalize to novel environments. \nWe address this by proposing a hierarchical RL framework in which a low-level policy is first pre-trained to imitate animal motions on flat ground, thereby establishing motion priors.\nA subsequent high-level, goal-conditioned policy then builds on these priors, learning residual corrections that enable perceptive locomotion, local obstacle avoidance, and goal-directed navigation across diverse and rugged terrains.\nSimulation experiments illustrate the effectiveness of learned residuals in adapting to progressively challenging uneven terrains while still preserving the locomotion characteristics provided by the motion priors.\nFurthermore, our results demonstrate improvements in motion regularization over baseline models trained without motion priors under similar reward setups. \nReal-world experiments with an ANYmal-D quadruped robot confirm our policy\u2019s capability to generalize animal-like locomotion skills to complex terrains, demonstrating smooth and efficient locomotion and local navigation performance amidst challenging terrains with obstacles.",
        "keywords": "Motion Priors;Reinforcement Learning;Legged Locomotion;Local Navigation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zewei Zhang;Chenhao Li;Takahiro Miki;Marco Hutter",
        "authorids": "~Zewei_Zhang3;~Chenhao_Li3;~Takahiro_Miki1;~Marco_Hutter1",
        "gender": "M;;M;M",
        "homepage": "https://takui9.github.io/;https://breadli428.github.io/;;http://www.rsl.ethz.ch",
        "dblp": ";186/9145;;04/2753",
        "google_scholar": ";kw1-DxQAAAAJ;nOl83tYAAAAJ;https://scholar.google.ch/citations?user=DO3quJYAAAAJ",
        "orcid": ";;;0000-0002-4285-4990",
        "linkedin": "zewzhang/;chenhao-li-86080b1b0/;;",
        "or_profile": "~Zewei_Zhang3;~Chenhao_Li3;~Takahiro_Miki1;~Marco_Hutter1",
        "aff": "EPFL - EPF Lausanne;ETHZ - ETH Zurich;;ETHZ - ETH Zurich",
        "aff_domain": "epfl.ch;ethz.ch;;ethz.ch",
        "position": "MS student;PhD student;;Associate Professor",
        "bibtex": "@inproceedings{\nzhang2025motion,\ntitle={Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex Quadruped Mobility},\nauthor={Zewei Zhang and Chenhao Li and Takahiro Miki and Marco Hutter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=JXBm4Xfrvj}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JXBm4Xfrvj",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "EPFL;ETH Zurich;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.epfl.ch;https://www.ethz.ch;",
        "aff_unique_abbr": "EPFL;ETHZ;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Lausanne;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland;"
    },
    {
        "id": "JeppaebLRD",
        "title": "FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Flow Models",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Developing efficient Vision-Language-Action (VLA) policies is crucial for practical robotics deployment, yet current approaches face prohibitive computational costs and resource requirements. Existing diffusion-based VLA policies require multi-billion-parameter models and massive datasets to achieve strong performance. We tackle this efficiency challenge with two contributions: intermediate-modality fusion, which reallocates capacity to the diffusion head by pruning up to 50% of LLM layers, and action-specific Global-AdaLN conditioning, which cuts parameters by 20% through modular adaptation. We integrate these advances into a novel 950 M-parameter VLA called FLOWER. Pretrained in just 200 H100 GPU hours, FLOWER delivers a 25.9% improvement over state-of-the-art baselines across 190 tasks spanning ten simulation and real-world benchmarks and demonstrates robustness across diverse robotic embodiments. All code, pretrained weights, and training recipes are publicly released to democratize efficient VLA development.",
        "keywords": "Imitation Learning;VLA;Language-conditioned Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/a5ba977e161f881e35e9141b2d11717a6fa66f2d.zip",
        "author": "Moritz Reuss;Hongyi Zhou;Marcel R\u00fchle;\u00d6mer Erdin\u00e7 Ya\u011fmurlu;Fabian Otto;Rudolf Lioutikov",
        "authorids": "~Moritz_Reuss1;~Hongyi_Zhou1;~Marcel_R\u00fchle1;~\u00d6mer_Erdin\u00e7_Ya\u011fmurlu1;~Fabian_Otto1;~Rudolf_Lioutikov1",
        "gender": "M;M;M;M;;M",
        "homepage": ";https://hongyizhoucn.github.io/;;https://github.com/omeryagmurlu;;https://rudolf.intuitive-robots.net",
        "dblp": "321/1769;;;;284/0547;151/9451",
        "google_scholar": "NLuzkPIAAAAJ;W35-J2sAAAAJ;;https://scholar.google.com/citations?hl=en;dV8eLH8AAAAJ;hvjV43MAAAAJ",
        "orcid": ";;;;0000-0003-3484-1054;",
        "linkedin": ";hongyi-zhou-9413b9242/;https://de.linkedin.com/in/marcel-r%C3%BChle-1348b3262?trk=public_profile_samename-profile;;ottofabian/;rudolf-lioutikov-74830730a/",
        "or_profile": "~Moritz_Reuss1;~Hongyi_Zhou1;~Marcel_R\u00fchle1;~\u00d6mer_Erdin\u00e7_Ya\u011fmurlu1;~Fabian_Otto1;~Rudolf_Lioutikov1",
        "aff": "Karlsruher Institut f\u00fcr Technologie;Karlsruher Institut f\u00fcr Technologie;Karlsruher Institut f\u00fcr Technologie;Karlsruher Institut f\u00fcr Technologie;Microsoft;Karlsruher Institut f\u00fcr Technologie",
        "aff_domain": "kit.edu;kit.edu;kit.edu;kit.edu;microsoft.com;kit.edu",
        "position": "PhD student;PhD student;Principal Researcher;MS student;Researcher;Tenure-Track Professor",
        "bibtex": "@inproceedings{\nreuss2025flower,\ntitle={{FLOWER}: Democratizing Generalist Robot Policies with Efficient Vision-Language-Flow Models},\nauthor={Moritz Reuss and Hongyi Zhou and Marcel R{\\\"u}hle and {\\\"O}mer Erdin{\\c{c}} Ya{\\u{g}}murlu and Fabian Otto and Rudolf Lioutikov},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=JeppaebLRD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JeppaebLRD",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "Karlsruher Institut f\u00fcr Technologie;Microsoft",
        "aff_unique_dep": ";Microsoft Corporation",
        "aff_unique_url": "https://www.kit.edu;https://www.microsoft.com",
        "aff_unique_abbr": "KIT;Microsoft",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1;0",
        "aff_country_unique": "Germany;United States"
    },
    {
        "id": "JibqR9sEdW",
        "title": "Embrace Contacts: humanoid shadowing with full body ground contacts",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Previous humanoid robot research works treat the robot as a bipedal mobile manipulation platform, where only the feet and hands contact the environment. However, we humans use all body parts to interact with the world, e.g., we sit in chairs, get up from the ground, or roll on the floor. Contacting the environment using body parts other than feet and hands brings significant challenges in both model-predictive control and reinforcement learning-based methods: an unpredictable contact sequence makes it almost impossible for model-predictive control to plan ahead in real time; the success of sim-to-real reinforcement learning for humanoids heavily depends on the acceleration of the rigid-body physical simulator and the simplification of collision detection. On the other hand, lacking extreme torso movement of humanoid data makes all other components non-trivial to design, such as dataset distribution, motion commands, and task rewards. To address these challenges, we propose a general humanoid motion framework that takes discrete motion commands and controls the robot\u2019s motor actions in real time. Using a GPU-accelerated simulator, we train a humanoid whole-body control policy that follows the high-level motion command in the real world in real time, even with stochastic contacts and extremely large robot base rotation and not-so-feasible motion commands.",
        "keywords": "Sim-to-Real;Deep Reinforcement Learning;Humanoid;Whole-body Control",
        "primary_area": "",
        "supplementary_material": "/attachment/ed7d013b412c417a382de938b8cb8d0d600d06c5.zip",
        "author": "Ziwen Zhuang;Hang Zhao",
        "authorids": "~Ziwen_Zhuang1;~Hang_Zhao1",
        "gender": "M;M",
        "homepage": "https://ziwenzhuang.github.io;http://www.mit.edu/~hangzhao/",
        "dblp": ";",
        "google_scholar": "GE8fpdwAAAAJ;DmahiOYAAAAJ",
        "orcid": ";",
        "linkedin": "leozhuang;",
        "or_profile": "~Ziwen_Zhuang1;~Hang_Zhao1",
        "aff": "Shanghai Qi Zhi Institute;Tsinghua University",
        "aff_domain": "sqz.ac.cn;tsinghua.edu.cn",
        "position": "Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nzhuang2025embrace,\ntitle={Embrace Contacts: humanoid shadowing with full body ground contacts},\nauthor={Ziwen Zhuang and Hang Zhao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=JibqR9sEdW}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JibqR9sEdW",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1",
        "aff_unique_norm": "Shanghai Qi Zhi Institute;Tsinghua University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.qz.io;https://www.tsinghua.edu.cn",
        "aff_unique_abbr": ";THU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "JrhMGXZnja",
        "title": "CARE: Enhancing Safety of Visual Navigation through Collision Avoidance via Repulsive Estimation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We propose CARE (Collision Avoidance via Repulsive Estimation) for improving the robustness of learning-based visual navigation methods. Recently, visual navigation models, particularly foundation models, have demonstrated promising performance by generating viable trajectories using only RGB images. However, these policies can generalize poorly to environments containing out-of-distribution (OOD) scenes characterized by unseen objects or different camera setups (e.g., variations in field of view, camera pose, or focal length). Without fine-tuning, such models could produce trajectories that lead to collisions, necessitating substantial efforts in data collection and additional training.\nTo address this limitation, we introduce CARE, an attachable module that enhances the safety of visual navigation without requiring additional range sensors or fine-tuning of pretrained models. CARE can be integrated seamlessly into any RGB-based navigation model that generates local robot trajectories. It dynamically adjusts trajectories produced by a pretrained model using repulsive force vectors computed from depth images estimated directly from RGB inputs. \nWe evaluate CARE by integrating it with state-of-the-art visual navigation models across diverse robot platforms. Real-world experiments show that CARE significantly reduces collisions (up to 100%) without compromising navigation performance in goal-conditioned navigation, and further improves collision-free travel distance (up to 10.7\u00d7) in exploration tasks.",
        "keywords": "Vision-based Navigation;Reactive Planning;Collision Avoidance",
        "primary_area": "",
        "supplementary_material": "/attachment/09f2c8ae6f890af5c4d34e93a4ad6fbe3f82e652.zip",
        "author": "Joonkyung Kim;Joonyeol Sim;Woojun Kim;Katia P. Sycara;Changjoo Nam",
        "authorids": "~Joonkyung_Kim1;~Joonyeol_Sim1;~Woojun_Kim1;~Katia_P._Sycara1;~Changjoo_Nam1",
        "gender": "M;M;M;F;M",
        "homepage": ";https://github.com/joonyeolsim;;;https://sites.google.com/site/changjoonam",
        "dblp": ";;236/4974;s/KatiaPSycara;48/7747",
        "google_scholar": "_Bamlg4AAAAJ;nedwNoEAAAAJ;https://scholar.google.co.kr/citations?user=bcHWCBoAAAAJ;VWv6a9kAAAAJ;H_2cbhsAAAAJ",
        "orcid": ";;;;",
        "linkedin": "joonkyung-kim-95ab6b207/;joonyeolsim/;;;",
        "or_profile": "~Joonkyung_Kim1;~Joonyeol_Sim1;~Woojun_Kim1;~Katia_P._Sycara1;~Changjoo_Nam1",
        "aff": "Texas A&M University - College Station+Sogang University;Sogang University;Carnegie Mellon University;Carnegie Mellon University;Sogang University",
        "aff_domain": "tamu.edu+sogang.ac.kr;sogang.ac.kr;cmu.edu;cmu.edu;sogang.ac.kr",
        "position": "PhD student+MS student;MS student;Postdoc;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nkim2025care,\ntitle={{CARE}: Enhancing Safety of Visual Navigation through Collision Avoidance via Repulsive Estimation},\nauthor={Joonkyung Kim and Joonyeol Sim and Woojun Kim and Katia P. Sycara and Changjoo Nam},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=JrhMGXZnja}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=JrhMGXZnja",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;1;2;2;1",
        "aff_unique_norm": "Texas A&M University;Sogang University;Carnegie Mellon University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tamu.edu;https://www.sogang.ac.kr;https://www.cmu.edu",
        "aff_unique_abbr": "TAMU;Sogang;CMU",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "College Station;",
        "aff_country_unique_index": "0+1;1;0;0;1",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "K7KLc4FexO",
        "title": "Agreement Volatility: A Second-Order Metric for Uncertainty Quantification in Surgical Robot Learning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Autonomous surgical robots are a promising solution to the increasing demand for surgery amid a shortage of surgeons. Recent work has proposed learning-based approaches for the autonomous manipulation of soft tissue. However, due to variability in aspects such as tissue geometries and stiffnesses, these methods do not always perform well, especially in out-of-distribution settings. To address this challenge, we propose a novel second-order metric for uncertainty quantification, agreement volatility, that enables successful and efficient collaborative handoffs between a human operator and a robot during soft-tissue manipulation by allowing the robot to know when to cede control to human operators and when to resume autonomous operation. We validate our approach using the daVinci Research Kit (dVRK) surgical robot to perform risk-aware physical soft-tissue manipulation. Our experimental results demonstrate that our proposed agreement volatility metric improves system success rates and leads to a 10\\% lower reliance on human interventions compared to a variance-only baseline.  We further demonstrate the usefulness of our agreement volatility metric as a spatial uncertainty map over geometric point cloud data, enabling uncertainty attribution which provides insight into regions of the input causing uncertainty.",
        "keywords": "Uncertainty Quantification;Uncertainty Attribution;Surgical Robotics",
        "primary_area": "",
        "supplementary_material": "/attachment/65d86d56d9f0616ccd23a6c6f44425266b7f30bd.zip",
        "author": "Jordan Thompson;Britton Jordan;Daniel S. Brown;Alan Kuntz",
        "authorids": "~Jordan_Thompson1;u1064972@utah.edu;~Daniel_S._Brown1;~Alan_Kuntz1",
        "gender": "M;;M;",
        "homepage": "https://jordanleethompson.wordpress.com/;;https://www.cs.utah.edu/~dsbrown/;https://users.cs.utah.edu/~adk/",
        "dblp": ";;141/7769;",
        "google_scholar": ";;https://scholar.google.com/citations?hl=en;7uZPiGoAAAAJ",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Jordan_Thompson1;u1064972@utah.edu;~Daniel_S._Brown1;~Alan_Kuntz1",
        "aff": "University of Utah;;University of Utah;University of Utah",
        "aff_domain": "utah.edu;;utah.edu;utah.edu",
        "position": "PhD student;;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nthompson2025agreement,\ntitle={Agreement Volatility: A Second-Order Metric for Uncertainty Quantification in Surgical Robot Learning},\nauthor={Jordan Thompson and Britton Jordan and Daniel S. Brown and Alan Kuntz},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=K7KLc4FexO}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=K7KLc4FexO",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of Utah;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utah.edu;",
        "aff_unique_abbr": "Utah;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "KHWIHwnYbn",
        "title": "Learning Deployable Locomotion Control via Differentiable Simulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Differentiable simulators promise to improve sample efficiency in robot learning by providing analytic gradients of the system dynamics. Yet, their application to contact-rich tasks like locomotion is complicated by the inherently non-smooth nature of contact, impeding effective gradient-based optimization. Existing works thus often rely on soft contact models that provide smooth gradients but lack physical accuracy, constraining results to simulation. To address this limitation, we propose a differentiable contact model designed to provide informative gradients while maintaining high physical fidelity. We demonstrate the efficacy of our approach by training a quadrupedal locomotion policy within our differentiable simulator leveraging analytic gradients and successfully transferring the learned policy zero-shot to the real world. To the best of our knowledge, this represents the first successful sim-to-real transfer of a legged locomotion policy learned entirely within a differentiable simulator, establishing the feasibility of using differentiable simulation for real-world locomotion control.",
        "keywords": "Differentiable Simulation;Contact Modeling;Quadruped Locomotion",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Clemens Schwarke;Victor Klemm;Joshua Bagajo;Jean Pierre Sleiman;Ignat Georgiev;Jesus Tordesillas Torres;Marco Hutter",
        "authorids": "~Clemens_Schwarke1;~Victor_Klemm1;~Joshua_Bagajo1;~Jean_Pierre_Sleiman1;~Ignat_Georgiev1;~Jesus_Tordesillas_Torres1;~Marco_Hutter1",
        "gender": "M;M;M;M;M;;M",
        "homepage": ";https://victorklemm.com;;;http://imgeorgiev.com/;https://jtorde.github.io;http://www.rsl.ethz.ch",
        "dblp": ";;;;;228/6616;04/2753",
        "google_scholar": ";-3pMVPUAAAAJ;;FW-U2M8AAAAJ;https://scholar.google.co.uk/citations?user=1Yu0vQkAAAAJ;QKcwqsQAAAAJ;https://scholar.google.ch/citations?user=DO3quJYAAAAJ",
        "orcid": ";0000-0002-6752-3397;;0000-0002-9935-8787;;0000-0001-6848-4070;0000-0002-4285-4990",
        "linkedin": "clemensschwarke/;https://linkedin.com/in/victor-klemm-6a68231ab;joshua-bagajo;;imgeorgiev/;jesus-tordesillas-torres/;",
        "or_profile": "~Clemens_Schwarke1;~Victor_Klemm1;~Joshua_Bagajo1;~Jean_Pierre_Sleiman1;~Ignat_Georgiev1;~Jesus_Tordesillas_Torres1;~Marco_Hutter1",
        "aff": "ETHZ - ETH Zurich;ETHZ - ETH Zurich;ETHZ - ETH Zurich;;Georgia Institute of Technology;ETHZ - ETH Zurich;ETHZ - ETH Zurich",
        "aff_domain": "ethz.ch;ethz.ch;ethz.ch;;gatech.edu;ethz.ch;ethz.ch",
        "position": "PhD student;PhD student;MS student;;PhD student;Postdoc;Associate Professor",
        "bibtex": "@inproceedings{\nschwarke2025learning,\ntitle={Learning Deployable Locomotion Control via Differentiable Simulation},\nauthor={Clemens Schwarke and Victor Klemm and Joshua Bagajo and Jean Pierre Sleiman and Ignat Georgiev and Jesus Tordesillas Torres and Marco Hutter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=KHWIHwnYbn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=KHWIHwnYbn",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1;2;0;0",
        "aff_unique_norm": "ETH Zurich;;Georgia Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ethz.ch;;https://www.gatech.edu",
        "aff_unique_abbr": "ETHZ;;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;2;0;0",
        "aff_country_unique": "Switzerland;;United States"
    },
    {
        "id": "KKpdjQK6nT",
        "title": "Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Humans do not passively observe the visual world---we actively look in order to act. Motivated by this principle, we introduce EyeRobot, a robotic system with gaze behavior that emerges from the need to complete real-world tasks. We develop a mechanical eyeball that can freely rotate to observe its surroundings and train a gaze policy to control it using reinforcement learning. We accomplish this by introducing a BC-RL loop trained using teleoperated demonstrations recorded with a 360 camera. The resulting video enables a simulation environment that supports rendering arbitrary eyeball viewpoints, allowing reinforcement learning of gaze behavior. The hand (BC) agent is trained from rendered eye observations, and the eye (RL) agent is rewarded when the hand produces correct actions. In this way, hand-eye coordination emerges as the eye looks towards regions which allow the hand to complete the task. We evaluate EyeRobot on five large workspace manipulation tasks and compare performance to two common camera setups: wrist and external cameras. Our experiments suggest EyeRobot exhibits hand-eye coordination which effectively facilitates action such as visual search or target switching, which enable manipulation across large workspaces.",
        "keywords": "Active Vision;Reinforcement Learning;Behavior Cloning;Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/b9930586a682cdced121a3c9c482d7db2da99dc9.zip",
        "author": "Justin Kerr;Kush Hari;Ethan Weber;Chung Min Kim;Brent Yi;tyler bonnen;Ken Goldberg;Angjoo Kanazawa",
        "authorids": "~Justin_Kerr1;~Kush_Hari1;~Ethan_Weber1;~Chung_Min_Kim1;~Brent_Yi1;~tyler_bonnen1;~Ken_Goldberg1;~Angjoo_Kanazawa1",
        "gender": "M;M;M;;M;M;M;F",
        "homepage": "https://kerrj.github.io/;;https://ethanweber.me/;https://chungmin99.github.io/;;https://tzler.github.io/;http://goldberg.berkeley.edu/;https://people.eecs.berkeley.edu/~kanazawa/",
        "dblp": ";;262/6076;305/3515;239/5167;;g/KennethYGoldberg;119/1305",
        "google_scholar": ";qeOl9rMAAAAJ;zw1TzeEAAAAJ;ODr5lMgAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.co.uk/citations?user=6ZkcZUAAAAAJ;https://scholar.google.com.tw/citations?user=8fztli4AAAAJ;Ci-_QYIAAAAJ",
        "orcid": ";;0000-0002-1117-2372;;;0000-0001-8709-1651;0000-0001-6747-9499;",
        "linkedin": ";;ethan-weber-0901b4118/;;;;goldbergken/;",
        "or_profile": "~Justin_Kerr1;~Kush_Hari1;~Ethan_Weber1;~Chung_Min_Kim1;~Brent_Yi1;~tyler_bonnen1;~Ken_Goldberg1;~Angjoo_Kanazawa1",
        "aff": "University of California, Berkeley;University of California, Berkeley;Massachusetts Institute of Technology+University of California, Berkeley;University of California, Berkeley;University of California, Berkeley;Electrical Engineering & Computer Science Department, University of California, Berkeley;University of California, Berkeley;University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;mit.edu+berkeley.edu;berkeley.edu;berkeley.edu;eecs.berkeley.edu;berkeley.edu;berkeley.edu",
        "position": "PhD student;PhD student;MS student+PhD student;PhD student;PhD student;Postdoc;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nkerr2025eye,\ntitle={Eye, Robot: Learning to Look to Act with a {BC}-{RL} Perception-Action Loop},\nauthor={Justin Kerr and Kush Hari and Ethan Weber and Chung Min Kim and Brent Yi and tyler bonnen and Ken Goldberg and Angjoo Kanazawa},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=KKpdjQK6nT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=KKpdjQK6nT",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1+0;0;0;0;0;0",
        "aff_unique_norm": "University of California, Berkeley;Massachusetts Institute of Technology",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;https://web.mit.edu",
        "aff_unique_abbr": "UC Berkeley;MIT",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "KSKzA1mwKs",
        "title": "Constraint-Preserving Data Generation for One-Shot Visuomotor Policy Generalization",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Large-scale demonstration data has powered key breakthroughs in robot manipulation, but collecting that data remains costly and time-consuming. To this end, we present Constraint-Preserving Data Generation (CP-Gen), a method that uses a single expert trajectory to generate robot demonstrations containing novel object geometries and poses. These generated demonstrations are used to train closed-loop visuomotor policies that transfer zero-shot to the real world. Similar to prior data-generation work focused on pose variations, CP-Gen first decomposes expert demonstrations into free-space motions and robot skills. Unlike prior work, we achieve geometry-aware data generation by formulating robot skills as keypoint-trajectory constraints: keypoints on the robot or grasped object must track a reference trajectory defined relative to a task-relevant object. To generate a new demonstration, CP-Gen samples pose and geometry transforms for each task-relevant object, then applies these transforms to the object and its associated keypoints or keypoint trajectories. We optimize robot joint configurations so that the keypoints on the robot or grasped object track the transformed keypoint trajectory, and then motion plan a collision-free path to the first optimized joint configuration. Using demonstrations generated by CP-Gen, we train visuomotor policies that generalize across variations in object geometries and poses. Experiments on 16 simulation tasks and four real-world tasks, featuring multi-stage, non-prehensile and tight-tolerance manipulation, show that policies trained using our method achieve an average success rate of 77%, outperforming the best baseline which achieves an average success rate of 50\\%.",
        "keywords": "Imitation Learning;Data Generation;Robot Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/f450cedb83b5e3222428d5f7134cb5465e438654.zip",
        "author": "Kevin Lin;Varun Ragunath;Andrew McAlinden;Aaditya Prasad;Jimmy Wu;Yuke Zhu;Jeannette Bohg",
        "authorids": "~Kevin_Lin6;varun.ragunath@utexas.edu;andrewmca@utexas.edu;~Aaditya_Prasad2;~Jimmy_Wu1;~Yuke_Zhu1;~Jeannette_Bohg1",
        "gender": ";;;M;M;M;",
        "homepage": "https://kevin-thankyou-lin.github.io/;;;;http://jimmyyhwu.github.io;https://yukezhu.me/;https://web.stanford.edu/~bohg/",
        "dblp": ";;;;00/8739;133/1772;52/7377",
        "google_scholar": ";;;https://scholar.google.com/citations?hl=en;UoQdAc4AAAAJ;mWGyYMsAAAAJ;rjnJnEkAAAAJ",
        "orcid": ";;;;;;0000-0002-4921-7193",
        "linkedin": ";;;aaditya-prasad/;;;",
        "or_profile": "~Kevin_Lin6;varun.ragunath@utexas.edu;andrewmca@utexas.edu;~Aaditya_Prasad2;~Jimmy_Wu1;~Yuke_Zhu1;~Jeannette_Bohg1",
        "aff": "University of Texas at Austin;;;Stanford University;Princeton University;Computer Science Department, University of Texas, Austin;Stanford University",
        "aff_domain": "utexas.edu;;;stanford.edu;princeton.edu;cs.utexas.edu;stanford.edu",
        "position": "PhD student;;;Undergrad student;PhD student;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nlin2025constraintpreserving,\ntitle={Constraint-Preserving Data Generation for One-Shot Visuomotor Policy Generalization},\nauthor={Kevin Lin and Varun Ragunath and Andrew McAlinden and Aaditya Prasad and Jimmy Wu and Yuke Zhu and Jeannette Bohg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=KSKzA1mwKs}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=KSKzA1mwKs",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;2;3;0;2",
        "aff_unique_norm": "University of Texas at Austin;;Stanford University;Princeton University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.utexas.edu;;https://www.stanford.edu;https://www.princeton.edu",
        "aff_unique_abbr": "UT Austin;;Stanford;Princeton",
        "aff_campus_unique_index": "0;2;0;2",
        "aff_campus_unique": "Austin;;Stanford",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "KUSYJIlKor",
        "title": "Omni-Perception: Omnidirectional Collision Avoidance of Legged Robots in Dynamic Environments",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Agile locomotion in complex 3D environments requires robust spatial awareness to safely avoid diverse obstacles such as aerial clutter, uneven terrain, and dynamic agents. Depth-based perception approaches often struggle with sensor noise, lighting variability, computational overhead from intermediate representations (e.g., elevation maps), and difficulties with non-planar obstacles, limiting performance in unstructured environments. In contrast, direct integration of LiDAR sensing into end-to-end learning for legged locomotion remains underexplored.We propose Omni-Perception, an end-to-end locomotion policy that achieves 3D spatial awareness and omnidirectional collision avoidance by directly processing raw LiDAR point clouds. At its core is PD-RiskNet (Proximal-Distal Risk-Aware Hierarchical Network), a novel perception module that interprets spatio-temporal LiDAR data for environmental risk assessment. To facilitate efficient policy learning, we develop a high-fidelity LiDAR simulation toolkit with realistic noise modeling and fast raycasting, compatible with platforms such as Isaac Gym, Genesis, and MuJoCo, enabling scalable training and effective sim-to-real transfer.Learning reactive control policies directly from raw LiDAR data enables the robot to navigate complex environments with static and dynamic obstacles more robustly than approaches relying on intermediate maps or limited sensing. We validate Omni-Perception through real-world experiments and extensive simulation, demonstrating strong omnidirectional avoidance capabilities and superior locomotion performance in highly dynamic environments.We will open-source our code and models.",
        "keywords": "Locomotion;Reinforcement Learning;Legged robot",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zifan Wang;Teli Ma;Yufei Jia;Xun Yang;Jiaming Zhou;Wenlong OUYANG;Qiang Zhang;Junwei Liang",
        "authorids": "~Zifan_Wang7;~Teli_Ma1;~Yufei_Jia1;~Xun_Yang6;~Jiaming_Zhou1;~Wenlong_OUYANG2;~Qiang_Zhang10;~Junwei_Liang1",
        "gender": "M;M;F;M;M;M;;M",
        "homepage": "https://zifanw.notion.site/;https://teleema.github.io/;https://github.com/TATP-233;https://kyrieyoung.notion.site;https://jiaming-zhou.github.io/;;;https://junweiliang.me/",
        "dblp": ";276/3611;;;;;;62/10704-1",
        "google_scholar": "GaJXZ-UAAAAJ;tW37g0UAAAAJ;;;b3y40w8AAAAJ;;;bMedjfUAAAAJ",
        "orcid": ";;;;;;;0000-0003-2219-5569",
        "linkedin": ";;;;;wenlong-ouyang-860565332/;;junweiliang/",
        "or_profile": "~Zifan_Wang7;~Teli_Ma1;~Yufei_Jia1;~Xun_Yang6;~Jiaming_Zhou1;~Wenlong_OUYANG2;~Qiang_Zhang10;~Junwei_Liang1",
        "aff": "Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Tsinghua University;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology (Guangzhou);Hong Kong University of Science and Technology;;Hong Kong University of Science and Technology",
        "aff_domain": "hkust-gz.edu.cn;hkust-gz.edu.cn;tsinghua.edu.cn;connect.hkust-gz.edu.cn;hkust-gz.edu.cn;connect.hkust-gz.edu.cn;;ust.hk",
        "position": "PhD student;PhD student;PhD student;MS student;PhD student;PhD student;;Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025omniperception,\ntitle={Omni-Perception: Omnidirectional Collision Avoidance of Legged Robots in Dynamic Environments},\nauthor={Zifan Wang and Teli Ma and Yufei Jia and Xun Yang and Jiaming Zhou and Wenlong OUYANG and Qiang Zhang and Junwei Liang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=KUSYJIlKor}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=KUSYJIlKor",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;0;0;2;0",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Tsinghua University;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ust.hk;https://www.tsinghua.edu.cn;",
        "aff_unique_abbr": "HKUST;THU;",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "KXzkAje2uQ",
        "title": "WoMAP: World Models For Embodied Open-Vocabulary Object Localization",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Active object localization remains a critical challenge for robots, requiring efficient\nexploration of partially observable environments. However, state-of-the-art robot\npolicies either struggle to generalize beyond demonstration datasets (e.g., imitation\nlearning methods) or fail to generate physically grounded actions (e.g., VLMs).\nTo address these limitations, we introduce WoMAP (World Models for Active\nPerception): a recipe for training open-vocabulary object localization policies that:\n(i) uses a Gaussian Splatting-based real-to-sim-to-real pipeline for scalable data\ngeneration without the need for expert demonstrations, (ii) distills dense rewards\nsignals from open-vocabulary object detectors, and (iii) leverages a latent world\nmodel for dynamics and rewards prediction to ground high-level action proposals\nat inference time. Rigorous simulation and hardware experiments demonstrate WoMAP's superior performance in a wide range of zero-shot object localization tasks, with a 63\\% success rate compared to 10\\%success rate compared to a VLM baseline, and only a 10 - 20\\% drop in performance when directly transferring from sim to real.",
        "keywords": "Active Perception;World Models;Object Localization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Tenny Yin;Zhiting Mei;Tao Sun;Ola Sho;Anirudha Majumdar;Emily Zhou;Jeremy Bao;Miyu Yamane;Lihan Zha",
        "authorids": "~Tenny_Yin1;~Zhiting_Mei1;~Tao_Sun25;~Ola_Sho1;~Anirudha_Majumdar1;~Emily_Zhou2;~Jeremy_Bao1;~Miyu_Yamane1;~Lihan_Zha1",
        "gender": "F;F;M;;M;;M;F;M",
        "homepage": "https://tenny-yinyijun.github.io/;https://may0mei.github.io/;https://taosun99.github.io/;;https://irom-lab.princeton.edu/majumdar/;;;;https://lihzha.github.io",
        "dblp": ";;;;116/6436;;;;",
        "google_scholar": "rvU3pjYAAAAJ;aWwlcNsAAAAJ;;;ibu3FwsAAAAJ;;;;3UErXEwAAAAJ",
        "orcid": ";;;;;;;;",
        "linkedin": ";;;;;zhou-emily/;jeremy-bao/;miyu-yamane;lihan-zha-111682264/",
        "or_profile": "~Tenny_Yin1;~Zhiting_Mei1;~Tao_Sun25;~Ola_Sho1;~Anirudha_Majumdar1;~Emily_Zhou2;~Jeremy_Bao1;~Miyu_Yamane1;~Lihan_Zha1",
        "aff": "Princeton University;Princeton University;McGill University;;Google+Princeton University;Princeton University;Princeton University;Princeton University;Princeton University",
        "aff_domain": "princeton.edu;princeton.edu;mcgill.ca;;google.com+princeton.edu;princeton.edu;princeton.edu;princeton.edu;princeton.edu",
        "position": "PhD student;PhD student;PhD student;;Researcher+Associate Professor;Undergrad student;Undergrad student;Undergrad student;PhD student",
        "bibtex": "@inproceedings{\nyin2025womap,\ntitle={Wo{MAP}: World Models For Embodied Open-Vocabulary Object Localization},\nauthor={Tenny Yin and Zhiting Mei and Tao Sun and Ola Sho and Anirudha Majumdar and Emily Zhou and Jeremy Bao and Miyu Yamane and Lihan Zha},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=KXzkAje2uQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=KXzkAje2uQ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;2;3+0;0;0;0;0",
        "aff_unique_norm": "Princeton University;McGill University;;Google",
        "aff_unique_dep": ";;;Google",
        "aff_unique_url": "https://www.princeton.edu;https://www.mcgill.ca;;https://www.google.com",
        "aff_unique_abbr": "Princeton;McGill;;Google",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;1;0+0;0;0;0;0",
        "aff_country_unique": "United States;Canada;"
    },
    {
        "id": "L5PJBd8ahD",
        "title": "UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Estimating the 6D pose of novel objects is a fundamental yet challenging problem in robotics, often relying on access to object CAD models. \n    However, acquiring such models can be costly and impractical. \n    Recent approaches aim to bypass this requirement by leveraging strong priors from foundation models to reconstruct objects from single or multi-view images, but typically require additional training or produce hallucinated geometry.\n    To this end, we propose $\\textit{UnPose}$, a novel framework for zero-shot, model-free 6D object pose estimation and reconstruction that exploits 3D priors and uncertainty estimates from a pre-trained diffusion model. \n    Specifically, starting from a single-view RGB-D frame, $\\textit{UnPose}$ uses a multi-view diffusion model to estimate an initial 3D model using 3D Gaussian Splatting (3DGS) representation, along with pixel-wise epistemic uncertainty estimates.\n    As additional observations become available, we incrementally refine the 3DGS model by fusing new views guided by the diffusion model\u2019s uncertainty,\n    thereby, continuously improving the pose estimation accuracy and 3D reconstruction quality. \n    To ensure global consistency, the diffusion prior-generated views and subsequent observations are further integrated in a pose graph and jointly optimized into a coherent 3DGS field.\n    Extensive experiments demonstrate that $\\textit{UnPose}$ significantly outperforms existing approaches in both 6D pose estimation accuracy and 3D reconstruction quality.\n    We further showcase its practical applicability in real-world robotic manipulation tasks.",
        "keywords": "6D Pose Estimation;Diffusion Model;Object Reconstruction",
        "primary_area": "",
        "supplementary_material": "/attachment/9e39eaacdc7d4a5b3166caaf2ac3d10731fe0c4c.zip",
        "author": "Zhaodong Jiang;Ashish Sinha;Tongtong Cao;Yuan Ren;Bingbing Liu;Binbin Xu",
        "authorids": "~Zhaodong_Jiang1;~Ashish_Sinha1;~Tongtong_Cao1;~Yuan_Ren2;~Bingbing_Liu2;~Binbin_Xu1",
        "gender": "M;M;M;M;M;",
        "homepage": ";https://sinashish.github.io;;;;https://binbin-xu.github.io/",
        "dblp": ";128/4147;229/5950.html;;;20/3602-1",
        "google_scholar": ";https://scholar.google.ca/citations?user=9JNZpoQAAAAJ;;;-rCulKwAAAAJ;https://scholar.google.co.uk/citations?user=874PofoAAAAJ",
        "orcid": "0009-0002-7708-222X;0000-0003-1395-6629;;;;",
        "linkedin": ";https://linkedin.com/in/sinashish;tongtong-cao-9aa07017a/;yuan-ren-ba024620/;;",
        "or_profile": "~Zhaodong_Jiang1;~Ashish_Sinha1;~Tongtong_Cao1;~Yuan_Ren2;~Bingbing_Liu2;~Binbin_Xu1",
        "aff": "University of Toronto;Huawei Technologies Ltd.;Huawei Technologies Ltd.;Huawei Technologies Canada Inc.;Huawei Technologies Ltd.;Huawei Technologies Ltd.",
        "aff_domain": "utoronto.ca;huawei.com;huawei.com;huawei.com;huawei.com;huawei.com",
        "position": "Undergrad student;Researcher;Researcher;Researcher;R&D Manager;Researcher",
        "bibtex": "@inproceedings{\njiang2025unpose,\ntitle={UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation},\nauthor={Zhaodong Jiang and Ashish Sinha and Tongtong Cao and Yuan Ren and Bingbing Liu and Binbin Xu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=L5PJBd8ahD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=L5PJBd8ahD",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;2;1;1",
        "aff_unique_norm": "University of Toronto;Huawei;Huawei Technologies Canada Inc.",
        "aff_unique_dep": ";Huawei Technologies;",
        "aff_unique_url": "https://www.utoronto.ca;https://www.huawei.com;",
        "aff_unique_abbr": "U of T;Huawei;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "Canada;China;"
    },
    {
        "id": "LRG1xvtiwL",
        "title": "D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Learning bimanual manipulation is challenging due to its high dimensionality and tight coordination required between two arms. Eye-in-hand imitation learning, which uses wrist-mounted cameras, simplifies perception by focusing on task-relevant views. However, collecting diverse demonstrations remains costly, motivating the need for scalable data augmentation. While prior work has explored visual augmentation in single-arm settings, extending these approaches to bimanual manipulation requires generating viewpoint-consistent observations across both arms and producing corresponding action labels that are both valid and feasible. In this work, we propose Diffusion for COordinated Dual-arm Data Augmentation (D-CODA), a method for offline data augmentation tailored to eye-in-hand bimanual imitation learning that trains a diffusion model to synthesize novel, viewpoint-consistent wrist-camera images for both arms while simultaneously generating joint-space action labels. It employs constrained optimization to ensure that augmented states involving gripper-to-object contacts adhere to constraints suitable for bimanual coordination. We evaluate D-CODA on 5 simulated and 3 real-world tasks. Our results across 2250 simulation trials and 180 real-world trials demonstrate that it outperforms baselines and ablations, showing its potential for scalable data augmentation in eye-in-hand bimanual manipulation. Our anonymous website is at: https://dcodaaug.github.io/D-CODA/.",
        "keywords": "Data Augmentation;Bimanual Manipulation;Diffusion Models",
        "primary_area": "",
        "supplementary_material": "/attachment/3b1b9a68ac0df259190b2b97d4e4dd8f86e60d57.zip",
        "author": "I-Chun Arthur Liu;Jason Chen;Gaurav S. Sukhatme;Daniel Seita",
        "authorids": "~I-Chun_Arthur_Liu1;~Jason_Chen7;~Gaurav_S._Sukhatme1;~Daniel_Seita1",
        "gender": "M;M;M;",
        "homepage": "http://arthurliu.com/;https://jasoonchen.com/;http://www-robotics.usc.edu/~gaurav/;",
        "dblp": ";;s/GauravSSukhatme;",
        "google_scholar": "ToWC_fgAAAAJ;;https://scholar.google.com.tw/citations?user=lRUi-A8AAAAJ;",
        "orcid": "0000-0001-7144-634X;;0000-0003-2408-474X;",
        "linkedin": "i-chun-arthur-liu/;;gaurav-sukhatme-9b6420b/;",
        "or_profile": "~I-Chun_Arthur_Liu1;~Jason_Chen7;~Gaurav_S._Sukhatme1;~Daniel_Seita1",
        "aff": "University of Southern California;University of Southern California;Amazon+University of Southern California;",
        "aff_domain": "usc.edu;usc.edu;amazon.com+usc.edu;",
        "position": "PhD student;Undergrad student;Amazon Scholar+Full Professor;",
        "bibtex": "@inproceedings{\nliu2025dcoda,\ntitle={D-{CODA}: Diffusion for Coordinated Dual-Arm Data Augmentation},\nauthor={I-Chun Arthur Liu and Jason Chen and Gaurav S. Sukhatme and Daniel Seita},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=LRG1xvtiwL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=LRG1xvtiwL",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1+0;2",
        "aff_unique_norm": "University of Southern California;Amazon;",
        "aff_unique_dep": ";Amazon.com, Inc.;",
        "aff_unique_url": "https://www.usc.edu;https://www.amazon.com;",
        "aff_unique_abbr": "USC;Amazon;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "LbeMepi89R",
        "title": "RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Robotic chemists promise to both liberate human experts from repetitive tasks and accelerate scientific discovery, yet remain in their infancy. Chemical experiments involve long-horizon procedures over hazardous and deformable substances, where success requires not only task completion but also strict compliance with experimental norms. To address these challenges, we propose RoboChemist, a dual-loop framework that integrates Vision-Language Models (VLMs) with Vision-Language-Action (VLA) models. Unlike prior VLM-based systems (e.g., VoxPoser, ReKep) that rely on depth perception and struggle with transparent labware, and existing VLA systems (e.g., RDT, $\\pi_0$) that lack semantic-level feedback for complex tasks, our method leverages a VLM to serve as (1) a planner to decompose tasks into primitive actions, (2) a visual prompt generator to guide VLA models, and (3) a monitor to assess task success and regulatory compliance. Notably, we introduce a VLA interface that accepts image-based visual targets from the VLM, enabling precise, goal-conditioned control. Our system successfully executes both primitive actions and complete multi-step chemistry protocols. Results show significant improvements in both success rate and compliance rate over state-of-the-art VLM and VLA baselines, while also demonstrating strong generalization to objects and tasks. Code, data, and models will be released.",
        "keywords": "Robotic Chemistry;VLA Models;Visual Prompting",
        "primary_area": "",
        "supplementary_material": "/attachment/9cff0bd680fd1f23494bd66d32902e4dc84f7865.zip",
        "author": "Zongzheng Zhang;Chenghao Yue;Haobo Xu;Minwen Liao;Xianglin Qi;Huan-ang Gao;Ziwei Wang;Hao Zhao",
        "authorids": "~Zongzheng_Zhang1;~Chenghao_Yue1;~Haobo_Xu2;~Minwen_Liao1;~Xianglin_Qi1;~Huan-ang_Gao1;~Ziwei_Wang2;~Hao_Zhao1",
        "gender": "M;M;;F;M;M;M;M",
        "homepage": "https://cvpr.thecvf.com/Conferences/2025;https://orcid.org/0009-0008-9098-4109;;https://github.com/99warnings100errors/deep;https://github.com/Kazuha-1029/Xianglin-Qi;https://c7w.tech/about;https://ziweiwangthu.github.io/;https://sites.google.com/view/fromandto",
        "dblp": ";;;;;339/0975;136/5574-1;08/3737-2.html",
        "google_scholar": ";;;;;WvbKfLgAAAAJ;cMTW09EAAAAJ;ygQznUQAAAAJ",
        "orcid": "0009-0007-6909-1587;;;;;;0000-0001-9225-8495;0000-0001-7903-581X",
        "linkedin": ";;;;;;;",
        "or_profile": "~Zongzheng_Zhang1;~Chenghao_Yue1;~Haobo_Xu2;~Minwen_Liao1;~Xianglin_Qi1;~Huan-ang_Gao1;~Ziwei_Wang2;~Hao_Zhao1",
        "aff": "Tsinghua University;University of Electronic Science and Technology of China;;Xinjiang University;Tsinghua University;Tsinghua University;Nanyang Technological University;Tsinghua University+Peking University",
        "aff_domain": "mails.tsinghua.edu.cn;uestc.edu.cn;;xju.edu.cn;mails.tsinghua.edu.cn;cs.tsinghua.edu.cn;ntu.edu.sg;tsinghua.edu.cn+pku.edu.cn",
        "position": "PhD student;Undergrad student;;Undergrad student;Undergrad student;PhD student;Assistant Professor;Assistant Professor+Postdoc",
        "bibtex": "@inproceedings{\nzhang2025robochemist,\ntitle={RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation},\nauthor={Zongzheng Zhang and Chenghao Yue and Haobo Xu and Minwen Liao and Xianglin Qi and Huan-ang Gao and Ziwei Wang and Hao Zhao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=LbeMepi89R}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=LbeMepi89R",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3;0;0;4;0+5",
        "aff_unique_norm": "Tsinghua University;University of Electronic Science and Technology of China;;Xinjiang University;Nanyang Technological University;Peking University",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.uestc.edu.cn;;http://www.xju.edu.cn;https://www.ntu.edu.sg;http://www.pku.edu.cn",
        "aff_unique_abbr": "THU;UESTC;;XJU;NTU;Peking U",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;2;0+0",
        "aff_country_unique": "China;;Singapore"
    },
    {
        "id": "LnryWopsfJ",
        "title": "Mobi-$\\pi$: Mobilizing Your Robot Learning Policy",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Learned visuomotor policies are capable of performing increasingly complex manipulation tasks. However, most of these policies are trained on data collected from limited robot positions and camera viewpoints. This leads to poor generalization to novel robot positions, which limits the use of these policies on mobile platforms, especially for precise tasks like pressing buttons or turning faucets. In this work, we formulate the \"policy mobilization\" problem: find a mobile robot base pose in a novel environment that is in distribution with respect to a manipulation policy trained on a limited set of camera viewpoints. Compared to retraining the policy itself to be more robust to unseen robot base pose initializations, policy mobilization decouples navigation from manipulation and thus does not require additional demonstrations. With that, our formulation is still compatible with any approach that improves manipulation policy robustness. To study policy mobilization, we introduce the Mobi-$\\pi$ framework, which includes: (1) metrics that quantify the difficulty of mobilizing a given policy, (2) a suite of simulated mobile manipulation tasks based on RoboCasa to evaluate policy mobilization, (3) visualization tools for analysis, and (4) several baseline methods. We also propose a novel approach that bridges navigation and manipulation by optimizing the robot's base pose to align with an in-distribution base pose for a learned policy. Our approach utilizes a 3D Gaussian Splatting model for novel viewpoint synthesis, a score function to evaluate pose suitability, as well as sampling-based optimization to identify optimal robot poses. We show that our approach on average outperforms the best baseline by 7.65$\\times$ in simulation and 2.38$\\times$ in the real world, demonstrating its effectiveness for policy mobilization.",
        "keywords": "Policy Mobilization;Mobile Manipulation;Robot Learning;Robot Perception",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jingyun Yang;Isabella Huang;Brandon Vu;Max Bajracharya;Rika Antonova;Jeannette Bohg",
        "authorids": "~Jingyun_Yang1;~Isabella_Huang2;~Brandon_Vu1;max.bajracharya@tri.global;~Rika_Antonova1;~Jeannette_Bohg1",
        "gender": "M;F;;;;",
        "homepage": "https://yjy0625.github.io;;https://brandonvu.super.site/;;;https://web.stanford.edu/~bohg/",
        "dblp": ";;;;;52/7377",
        "google_scholar": "7XBAa2QAAAAJ;04Ta98kAAAAJ;;;;rjnJnEkAAAAJ",
        "orcid": ";;;;;0000-0002-4921-7193",
        "linkedin": ";;;;;",
        "or_profile": "~Jingyun_Yang1;~Isabella_Huang2;~Brandon_Vu1;max.bajracharya@tri.global;~Rika_Antonova1;~Jeannette_Bohg1",
        "aff": "Stanford University;Toyota Research Institute;Stanford University;;;Stanford University",
        "aff_domain": "stanford.edu;tri.global;stanford.edu;;;stanford.edu",
        "position": "PhD student;Researcher;MS student;;;Assistant Professor",
        "bibtex": "@inproceedings{\nyang2025mobipi,\ntitle={Mobi-\\${\\textbackslash}pi\\$: Mobilizing Your Robot Learning Policy},\nauthor={Jingyun Yang and Isabella Huang and Brandon Vu and Max Bajracharya and Rika Antonova and Jeannette Bohg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=LnryWopsfJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=LnryWopsfJ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;2;2;0",
        "aff_unique_norm": "Stanford University;Toyota Research Institute;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.tri.global;",
        "aff_unique_abbr": "Stanford;TRI;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "Lv5lmtLGIQ",
        "title": "Towards Embodiment Scaling Laws in Robot Locomotion",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Developing generalist agents that operate across diverse tasks, environments, and robot embodiments is a grand challenge in robotics and artificial intelligence. While substantial progress has been made in cross-task and cross-environment generalization, achieving broad generalization to novel embodiments remains elusive. In this work, we study embodiment scaling laws \u2014 the hypothesis that increasing the quantity of training embodiments improves generalization to unseen ones. To explore this, we procedurally generate a dataset of $\\sim$1,000 varied robot embodiments, spanning humanoids, quadrupeds, and hexapods, and train embodiment-specific reinforcement learning experts for legged locomotion. We then distill these experts into a single generalist policy capable of handling diverse observation and action spaces. Our large-scale study reveals that generalization performance improves with the number of training embodiments. Notably, a policy trained on the full dataset zero-shot transfers to diverse unseen embodiments in both simulation and real-world evaluations. These results provide preliminary empirical evidence for embodiment scaling laws and suggest that scaling up embodiment quantity may serve as a foundation for building generalist robot agents.",
        "keywords": "Cross-Embodiment Learning;Robot Locomotion;Robotic Foundation Models;Reinforcement Learning;Imitation Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/02a91e98c88db65c5d30b829caf4b7d83b16ff26.zip",
        "author": "Bo Ai;Liu Dai;Nico Bohlinger;Dichen Li;Tongzhou Mu;Zhanxin Wu;K. Fay;Henrik I Christensen;Jan Peters;Hao Su",
        "authorids": "~Bo_Ai1;~Liu_Dai1;~Nico_Bohlinger1;~Dichen_Li1;~Tongzhou_Mu1;~Zhanxin_Wu1;~K._Fay1;~Henrik_I_Christensen1;~Jan_Peters3;~Hao_Su1",
        "gender": "M;M;M;M;M;F;Not Specified;M;M;M",
        "homepage": "https://albertboai.com/;https://liudai.notion.site/;https://www.ias.informatik.tu-darmstadt.de/Team/NicoBohlinger;;http://cseweb.ucsd.edu/~t3mu/;https://zhanxinwu.com/;;https://www.hichristensen.com;https://www.jan-peters.net;http://ai.ucsd.edu/~haosu",
        "dblp": ";323/3239;;;183/0943;;;c/HIChristensen;p/JanPeters1;09/4945-1",
        "google_scholar": "KlE77HAAAAAJ;https://scholar.google.com/citations?hl=en;5SBR9tEAAAAJ;;uVsZydYAAAAJ;e5ymbE8AAAAJ;;MA8rI0MAAAAJ;https://scholar.google.de/citations?user=-kIVAcAAAAAJ;1P8Zu04AAAAJ",
        "orcid": "0000-0002-9009-3823;;;;;;0009-0007-7470-2776;0000-0002-7465-7502;0000-0002-5266-8091;",
        "linkedin": "bo-ai;;;dichen-li-a84ba7291/;;;;henrikichristensen/;janrpeters/;",
        "or_profile": "~Bo_Ai1;~Liu_Dai1;~Nico_Bohlinger1;~Dichen_Li1;~Tongzhou_Mu1;~Zhanxin_Wu1;~K._Fay1;~Henrik_I_Christensen1;~Jan_Peters3;~Hao_Su1",
        "aff": "University of California, San Diego;University of California, San Diego;Technische Universit\u00e4t Darmstadt;University of California, San Diego;University of California, San Diego;Cornell University;University of California, San Diego;Computer Science and Engineering Department, University of California, San Diego;German Research Center for AI+TU Darmstadt;University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;tu-darmstadt.de;ucsd.edu;ucsd.edu;cornell.edu;ucsd.edu;cse.ucsd.edu;dfki.de+tu-darmstadt.de;ucsd.edu",
        "position": "PhD student;PhD student;PhD student;MS student;PhD student;PhD student;PhD student;Full Professor;Principal Researcher+Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nai2025towards,\ntitle={Towards Embodiment Scaling Laws in Robot Locomotion},\nauthor={Bo Ai and Liu Dai and Nico Bohlinger and Dichen Li and Tongzhou Mu and Zhanxin Wu and K. Fay and Henrik I Christensen and Jan Peters and Hao Su},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Lv5lmtLGIQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Lv5lmtLGIQ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;0;2;0;0;3+1;0",
        "aff_unique_norm": "University of California, San Diego;Technische Universit\u00e4t Darmstadt;Cornell University;German Research Center for Artificial Intelligence",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ucsd.edu;https://www.tu-darmstadt.de;https://www.cornell.edu;https://www.dfki.de/",
        "aff_unique_abbr": "UCSD;TUD;Cornell;DFKI",
        "aff_campus_unique_index": "0;0;0;0;0;0;2;0",
        "aff_campus_unique": "San Diego;;Darmstadt",
        "aff_country_unique_index": "0;0;1;0;0;0;0;0;1+1;0",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "M1e2PEMLp2",
        "title": "Distributed Upload and Active Labeling for Resource-Constrained Fleet Learning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In multi-robot systems, fleets are often deployed to collect data that improves the performance of machine learning models for downstream perception and planning. However, real-world robotic deployments generate vast amounts of data across diverse conditions, while only a small portion can be transmitted or labeled due to limited bandwidth, constrained onboard storage, and high annotation costs. To address these challenges, we propose Distributed Upload and Active Labeling (DUAL), a decentralized, two-stage data collection framework for resource-constrained robotic fleets. In the first stage, each robot independently selects a subset of its local observations to upload under storage and communication constraints. In the second stage, the cloud selects a subset of uploaded data to label, subject to a global annotation budget. We evaluate DUAL on classification tasks spanning multiple sensing modalities, as well as on RoadNet\u2014a real-world dataset we collected from vehicle-mounted cameras for time and weather classification. We further validate our approach in a physical experiment using a Franka Emika Panda robot arm, where it learns to move a red cube to a green bowl. Finally, we test DUAL on trajectory prediction using the nuScenes autonomous driving dataset to assess generalization to complex prediction tasks. Across all settings, DUAL consistently outperforms state-of-the-art baselines, achieving up to 31.1\\% gain in classification accuracy and a 13\\% improvement in real-world robotics task completion rates.",
        "keywords": "Fleet Learning;Distributed Data Collection;Submodular Maximization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Oguzhan Akcin;Harsh Goel;Ruihan Zhao;Sandeep P. Chinchali",
        "authorids": "~Oguzhan_Akcin2;~Harsh_Goel1;~Ruihan_Zhao1;~Sandeep_P._Chinchali1",
        "gender": "M;M;M;",
        "homepage": ";https://harshg99.github.io/hgmain/;https://philipzrh.com;",
        "dblp": "311/3023;316/5714;236/4741-1;",
        "google_scholar": "2elIEXoAAAAJ;6kKhD_4AAAAJ;;",
        "orcid": ";;;",
        "linkedin": "oguzhan-akcin-0907/;;;",
        "or_profile": "~Oguzhan_Akcin2;~Harsh_Goel1;~Ruihan_Zhao1;~Sandeep_P._Chinchali1",
        "aff": "The University of Texas at Austin;University of Texas at Austin;University of Texas at Austin;",
        "aff_domain": "utexas.edu;utexas.edu;utexas.edu;",
        "position": "PhD student;PhD student;PhD student;",
        "bibtex": "@inproceedings{\nakcin2025distributed,\ntitle={Distributed Upload and Active Labeling for Resource-Constrained Fleet Learning},\nauthor={Oguzhan Akcin and Harsh Goel and Ruihan Zhao and Sandeep P. Chinchali},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=M1e2PEMLp2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=M1e2PEMLp2",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1",
        "aff_unique_norm": "University of Texas at Austin;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.utexas.edu;",
        "aff_unique_abbr": "UT Austin;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "MCC3MG2aRH",
        "title": "ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Vision-Language Models (VLMs) have revolutionized artificial intelligence and robotics due to their commonsense reasoning capabilities. In robotic manipulation, VLMs are used primarily as high-level planners, but recent work has also studied their lower-level reasoning ability, which refers to making decisions about precise robot movements. However, the community currently lacks a clear and common benchmark that can evaluate how well VLMs can aid low-level reasoning in robotics. Consequently, we propose a novel benchmark, ManipBench, to evaluate the low-level robot manipulation reasoning capabilities of VLMs across various dimensions, including how well they understand object-object interactions and deformable object manipulation. We extensively test 35 common and state-of-the-art VLM families on our benchmark, including variants to test different model sizes. The performance of VLMs significantly varies across tasks, and there is a strong correlation between this performance and trends in our real-world manipulation tasks. It also shows that there remains a significant gap between these models and human-level understanding.",
        "keywords": "Vision-Language Models;Robotics Benchmark;Robot Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/7a438c5c1577cc2ad75e3f591fe9280449a03539.zip",
        "author": "Enyu Zhao;Vedant Raval;Hejia Zhang;Jiageng Mao;Zeyu Shangguan;Stefanos Nikolaidis;Yue Wang;Daniel Seita",
        "authorids": "~Enyu_Zhao1;~Vedant_Raval1;~Hejia_Zhang1;~Jiageng_Mao1;~Zeyu_Shangguan1;~Stefanos_Nikolaidis1;~Yue_Wang2;~Daniel_Seita1",
        "gender": "M;M;M;;M;;M;",
        "homepage": ";;https://www.hejiazhang.me;;;http://stefanosnikolaidis.net/;https://yuewang.xyz;",
        "dblp": "132/0132;;172/9965;;334/1715;62/6555;33/4822-41;",
        "google_scholar": "https://scholar.google.com/citations?hl=zh-CN;3ewNCB0AAAAJ;h_0iAx4AAAAJ;;;;v-AEFIEAAAAJ;",
        "orcid": ";;;;0000-0003-1435-6959;;;",
        "linkedin": "enyu-zhao-564566250/;vedantraval23;hejia-zhang-3914bb154/;;;;;",
        "or_profile": "~Enyu_Zhao1;~Vedant_Raval1;~Hejia_Zhang1;~Jiageng_Mao1;~Zeyu_Shangguan1;~Stefanos_Nikolaidis1;~Yue_Wang2;~Daniel_Seita1",
        "aff": ";University of Southern California;Skild AI;;University of Southern California;University of Southern California;University of Southern California+NVIDIA;",
        "aff_domain": ";usc.edu;skild.ai;;usc.edu;usc.edu;usc.edu+nvidia.com;",
        "position": ";MS student;Researcher;;PhD student;Assistant Professor;Assistant Professor+Researcher;",
        "bibtex": "@inproceedings{\nzhao2025manipbench,\ntitle={ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation},\nauthor={Enyu Zhao and Vedant Raval and Hejia Zhang and Jiageng Mao and Zeyu Shangguan and Stefanos Nikolaidis and Yue Wang and Daniel Seita},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=MCC3MG2aRH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MCC3MG2aRH",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;0;1;1;1+3;0",
        "aff_unique_norm": ";University of Southern California;Skild AI;NVIDIA",
        "aff_unique_dep": ";;;NVIDIA Corporation",
        "aff_unique_url": ";https://www.usc.edu;;https://www.nvidia.com",
        "aff_unique_abbr": ";USC;;NVIDIA",
        "aff_campus_unique_index": "1;1;1;1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "1;1;1;1+1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "MJmzIunyBy",
        "title": "SimShear: Sim-to-Real Shear-based Tactile Servoing",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We present SimShear: a sim-to-real pipeline for tactile control that allows use of shear information without explicitly modeling shear dynamics in simulation. Shear, which arises from lateral movements across contact surfaces, are critical for tasks involving dynamic object interactions but are challenging to simulate. We introduce shPix2pix: a shear-conditioned U-Net GAN that transforms simulated tactile images absent of shear plus a vector encoding shear information into realistic equivalents that include shear deformations, and show this outperforms baseline pix2pix methods for simulating tactile images and pose/shear prediction. This is applied to two control tasks using a pair of low-cost desktop robotic arms equipped with a vision-based tactile sensor: first, a tactile tracking task, where a follower arm tracks a surface moved by the leader arm; second, a collaborative co-lift task, where both arms jointly hold an object while the leader arm moves along a prescribed trajectory. Our method maintain contact errors within 1-2 mm across varied trajectories where shear sensing is essential for task performance. This work validates the use of sim-to-real shear modeling with rigid-body simulators, opening new possibilities for simulation in tactile robotics.",
        "keywords": "tactile sensing;sim-to-real;tactile servoing",
        "primary_area": "",
        "supplementary_material": "/attachment/b54cefe642594c6be6694a28284cd168481ec9c4.zip",
        "author": "Kipp Freud;Yijiong Lin;Nathan F. Lepora",
        "authorids": "~Kipp_Freud1;~Yijiong_Lin1;~Nathan_F._Lepora1",
        "gender": "M;;",
        "homepage": "https://kippfreud.com/;;https://www.lepora.com",
        "dblp": ";;76/10010",
        "google_scholar": ";;fb2WiJgAAAAJ",
        "orcid": "0009-0004-8940-3558;;",
        "linkedin": "kipp-freud-571578b4/;;",
        "or_profile": "~Kipp_Freud1;~Yijiong_Lin1;~Nathan_F._Lepora1",
        "aff": "University of Bristol;;University of Bristol",
        "aff_domain": "bristol.ac.uk;;bristol.ac.uk",
        "position": "PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nfreud2025simshear,\ntitle={SimShear: Sim-to-Real Shear-based Tactile Servoing},\nauthor={Kipp Freud and Yijiong Lin and Nathan F. Lepora},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=MJmzIunyBy}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MJmzIunyBy",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "University of Bristol;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.bristol.ac.uk;",
        "aff_unique_abbr": "Bristol;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "MpJTyAqA0t",
        "title": "Learning a Unified Policy for Position and Force Control in Legged Loco-Manipulation",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Robotic loco-manipulation tasks often involve contact-rich interactions with the environment, requiring the joint modeling of contact force and robot position. However, recent visuomotor policies often focus solely on position or force control, overlooking their integration. In this work, we propose a unified policy for legged robots that jointly models force and position control learned without reliance on force sensors. By simulating diverse combinations of active position and force commands alongside external disturbances force, we use reinforcement learning to learn a policy that estimates forces from the robot's historical states and compensates for them through position and velocity adjustments. Such a policy enables a wide range of manipulation behaviors under varying combinations of force and position inputs, including position tracking, force application, force tracking, and compliant robot behaviors. Additionally, we demonstrate that the learned policy enhances trajectory-based imitation learning pipelines by incorporating essential contact information through its force estimation module, achieving approximately ~39.5% higher success rates across four challenging contact-rich manipulation tasks compared to position-control policies. Extensive experiments on both a quadrupedal mobile manipulation platform and a humanoid validate the versatility and robustness of the proposed policy across diverse scenarios.",
        "keywords": "Unified Force and Position Control;Force-aware Imitation Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/baeddf086027ac9ad7f3c4f140ee861390f5b594.zip",
        "author": "Peiyuan Zhi;Peiyang Li;Jianqin Yin;Baoxiong Jia;Siyuan Huang",
        "authorids": "~Peiyuan_Zhi1;~Peiyang_Li4;~Jianqin_Yin1;~Baoxiong_Jia1;~Siyuan_Huang2",
        "gender": "M;;F;M;M",
        "homepage": ";https://github.com/deathpoker;https://teacher.bupt.edu.cn/yinjianqin/zh_CN/index.htm;https://buzz-beater.github.io/;https://siyuanhuang.com/",
        "dblp": ";;21/2631.html;206/8738;62/885-1",
        "google_scholar": ";https://scholar.google.com/citations?hl=zh-CN;QK5K52MAAAAJ;qIBUK6sAAAAJ;1NN7Ee8AAAAJ",
        "orcid": ";;0000-0002-1595-2499;0000-0002-4968-3290;",
        "linkedin": "https://www.linkedin.cn/incareer/in/ACoAACFioJYB2Vv1V168aPEg3BLvNdzZLQA1UcM;;;baoxiong-jia-2b6094122?trk=public_post-text;",
        "or_profile": "~Peiyuan_Zhi1;~Peiyang_Li4;~Jianqin_Yin1;~Baoxiong_Jia1;~Siyuan_Huang2",
        "aff": ";Beijing University of Posts and Telecommunications;Beijing University of Posts and Telecommunications;Beijing Institute for General Artificial Intelligence;Beijing Institute for General Artificial Intelligence",
        "aff_domain": ";bupt.edu.cn;bupt.edu.cn;bigai.ai;bigai.ai",
        "position": ";PhD student;Full Professor;Researcher;Researcher",
        "bibtex": "@inproceedings{\nzhi2025learning,\ntitle={Learning a Unified Policy for Position and Force Control in Legged Loco-Manipulation},\nauthor={Peiyuan Zhi and Peiyang Li and Jianqin Yin and Baoxiong Jia and Siyuan Huang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=MpJTyAqA0t}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=MpJTyAqA0t",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;2;2",
        "aff_unique_norm": ";Beijing University of Posts and Telecommunications;Beijing Institute for General Artificial Intelligence",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";http://www.bupt.edu.cn/;http://www.bigaiai.org/",
        "aff_unique_abbr": ";BUPT;BIGAI",
        "aff_campus_unique_index": "1;1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "1;1;1;1",
        "aff_country_unique": ";China"
    },
    {
        "id": "NOQwVh1Gib",
        "title": "DiWA: Diffusion Policy Adaptation with World Models",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Fine-tuning diffusion policies with reinforcement learning (RL) presents significant challenges. The long denoising sequence for each action prediction impedes effective reward propagation. Additionally, standard RL methods require millions of physical interaction steps, making fine-tuning even more challenging. Prior work models the denoising steps in diffusion policies as a Markov Decision Process to adapt to RL policy updates, but its heavy reliance on environment interactions still leads to inefficiency. To bridge this gap, we introduce DiWA, a novel framework that leverages a world model for fine-tuning diffusion-based robotic skills entirely offline with reinforcement learning. Unlike model-free approaches that require millions of environment interactions to fine-tune a repertoire of robot skills, DiWA achieves effective adaptation using a world model trained once on a few hundred thousand offline play interactions. This results in dramatically improved sample efficiency, making the approach significantly more practical and safer for real-world robot learning. On the challenging CALVIN benchmark, DiWA improves performance across eight tasks using only offline adaptation, while requiring orders of magnitude fewer physical interactions than model-free baselines. To our knowledge, this is the first demonstration of fine-tuning diffusion policies for real-world robotic skills using an offline world model. We make the code publicly available at _redacted-for-review_.",
        "keywords": "World Models;Imitation Learning;Reinforcement Learning;Diffusion Policies",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Akshay L Chandra;Iman Nematollahi;Chenguang Huang;Tim Welschehold;Wolfram Burgard;Abhinav Valada",
        "authorids": "~Akshay_L_Chandra1;~Iman_Nematollahi1;~Chenguang_Huang1;~Tim_Welschehold1;~Wolfram_Burgard3;~Abhinav_Valada1",
        "gender": "M;M;;;M;M",
        "homepage": "https://akshaychandra.com/;http://www2.informatik.uni-freiburg.de/~nematoli/;;http://www2.informatik.uni-freiburg.de/~twelsche/;https://www.utn.de/person/wolfram-burgard/;https://rl.uni-freiburg.de/people/valada",
        "dblp": ";220/6770;;;b/WolframBurgard;81/9531",
        "google_scholar": "https://scholar.google.com/citations?hl=en;https://scholar.google.com/citations?hl=en;;https://scholar.google.de/citations?hl=en;zj6FavAAAAAJ;https://scholar.google.de/citations?user=LcARjz0AAAAJ",
        "orcid": ";;;;0000-0002-5680-6500;0000-0003-4710-3114",
        "linkedin": "acl21;;;;burgard/?originalSubdomain=de;avalada",
        "or_profile": "~Akshay_L_Chandra1;~Iman_Nematollahi1;~Chenguang_Huang1;~Tim_Welschehold1;~Wolfram_Burgard3;~Abhinav_Valada1",
        "aff": "University of Freiburg, Albert-Ludwigs-Universit\u00e4t Freiburg;CS Department, University of Freiburg, Germany, Albert-Ludwigs-Universit\u00e4t Freiburg;;Universit\u00e4t Freiburg;University of Technology Nuremberg;University of Freiburg",
        "aff_domain": "cs.uni-freiburg.de;informatik.uni-freiburg.de;;uni-freiburg.de;utn.de;uni-freiburg.de",
        "position": "PhD student;Researcher;;Postdoc;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nchandra2025diwa,\ntitle={Di{WA}: Diffusion Policy Adaptation with World Models},\nauthor={Akshay L Chandra and Iman Nematollahi and Chenguang Huang and Tim Welschehold and Wolfram Burgard and Abhinav Valada},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=NOQwVh1Gib}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=NOQwVh1Gib",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;2;0",
        "aff_unique_norm": "University of Freiburg;;Nuremberg University of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.uni-freiburg.de;;https://www.tu-nuernberg.de",
        "aff_unique_abbr": "UoF;;TUN",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Freiburg;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "Nt4LmgZ7v9",
        "title": "Fast Flow-based Visuomotor Policies via Conditional Optimal Transport Couplings",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Diffusion and flow matching policies have recently demonstrated remarkable performance in robotic applications by accurately capturing multimodal robot trajectory distributions. However, their computationally expensive inference, due to the numerical integration of an ODE or SDE, limits their applicability as real-time controllers for robots. We introduce a methodology that utilizes conditional Optimal Transport couplings between noise and samples to enforce straight solutions in the flow ODE for robot action generation tasks. We show that naively coupling noise and samples fails in conditional tasks and propose incorporating condition variables into the coupling process to improve few-step performance. The proposed few-step policy achieves a 4\\% higher success rate with a 10$\\times$ speed-up compared to Diffusion Policy on a diverse set of simulation tasks. Moreover, it produces high-quality and diverse action trajectories within 1-2 steps on a set of real-world robot tasks. Our method also retains the same training complexity as Diffusion Policy and vanilla Flow Matching, in contrast to distillation-based approaches.",
        "keywords": "Flow Matching;Optimal Transport;Imitation Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/854f2c235af031acffe0b816af15093ce2b1a711.zip",
        "author": "Andreas Sochopoulos;Nikolay Malkin;Nikolaos Tsagkas;Joao Moura;Michael Gienger;Sethu Vijayakumar",
        "authorids": "~Andreas_Sochopoulos1;~Nikolay_Malkin1;~Nikolaos_Tsagkas1;~Joao_Moura1;~Michael_Gienger1;~Sethu_Vijayakumar1",
        "gender": "M;;M;M;M;M",
        "homepage": ";;https://tsagkas.github.io/;https://sites.google.com/view/joaomoura;https://www.honda-ri.de;https://homepages.inf.ed.ac.uk/svijayak/",
        "dblp": ";;;;;68/1347",
        "google_scholar": "6w9786sAAAAJ;;cZgkD_oAAAAJ;https://scholar.google.co.uk/citations?user=1L5kTRcAAAAJ;https://scholar.google.de/citations?user=oU2jyxMAAAAJ;JdRs1sQAAAAJ",
        "orcid": ";;;;;",
        "linkedin": ";;;joaopousamoura/;;",
        "or_profile": "~Andreas_Sochopoulos1;~Nikolay_Malkin1;~Nikolaos_Tsagkas1;~Joao_Moura1;~Michael_Gienger1;~Sethu_Vijayakumar1",
        "aff": "University of Edinburgh;;University of Edinburgh;University of Edinburgh;;University of Edinburgh",
        "aff_domain": "ed.ac.uk;;ed.ac.uk;ed.ac.uk;;ed.ac.uk",
        "position": "PhD student;;PhD student;Postdoc;;Full Professor",
        "bibtex": "@inproceedings{\nsochopoulos2025fast,\ntitle={Fast Flow-based Visuomotor Policies via Conditional Optimal Transport Couplings},\nauthor={Andreas Sochopoulos and Nikolay Malkin and Nikolaos Tsagkas and Joao Moura and Michael Gienger and Sethu Vijayakumar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Nt4LmgZ7v9}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Nt4LmgZ7v9",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0;1;0",
        "aff_unique_norm": "University of Edinburgh;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ed.ac.uk;",
        "aff_unique_abbr": "Edinburgh;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "NtnPVwUCAH",
        "title": "$Door(s)$: Junction State Estimation for Efficient Exploration in Reinforcement Learning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Exploration is one of the important bottlenecks for efficient learning in reinforcement learning, especially in the presence of sparse rewards. One way to traverse the environment faster is by passing through junctions, or metaphorical doors, in the state space. We propose a novel heuristic, $Door(s)$, focused on such narrow passages that serve as pathways to a large number of other states. Our approach works by estimating the state occupancy distribution and allows computation of its entropy, which forms the basis for our measure. Its computation is more sample-efficient compared to other similar methods and robustly works over longer horizons. Our results highlight the detection of dead-end states, show increased exploration efficiency, and demonstrate that $Door(s)$ encodes specific behaviors useful for downstream learning of various robotic manipulation tasks.",
        "keywords": "Reinforcement learning;Intrinsic motivation;Junction States;Information theory;Heuristic;Exploration",
        "primary_area": "",
        "supplementary_material": "/attachment/ccadef649d2b4bcd46d5252e084789a032752277.zip",
        "author": "Benjamin Fele;Jan Babic",
        "authorids": "~Benjamin_Fele1;~Jan_Babic1",
        "gender": "M;",
        "homepage": "https://nbr.ijs.si/benjamin-fele/;https://nbr.ijs.si",
        "dblp": ";",
        "google_scholar": "FyFj_kcAAAAJ;",
        "orcid": "0000-0002-8241-1756;",
        "linkedin": ";",
        "or_profile": "~Benjamin_Fele1;~Jan_Babic1",
        "aff": "Jozef Stefan Institute+Faculty of Computer and Information Science, University of Ljubljana;",
        "aff_domain": "ijs.si+fri.uni-lj.si;",
        "position": "Researcher+PhD student;",
        "bibtex": "@inproceedings{\nfele2025doors,\ntitle={\\$Door(s)\\$: Junction State Estimation for Efficient Exploration in Reinforcement Learning},\nauthor={Benjamin Fele and Jan Babic},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=NtnPVwUCAH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=NtnPVwUCAH",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;2",
        "aff_unique_norm": "Jozef Stefan Institute;University of Ljubljana;",
        "aff_unique_dep": ";Faculty of Computer and Information Science;",
        "aff_unique_url": "https://www.ijs.si;https://www.fcis.unilj.si;",
        "aff_unique_abbr": "JSI;UL;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0",
        "aff_country_unique": "Slovenia;"
    },
    {
        "id": "O2RGlBfR9H",
        "title": "ImLPR: Image-based LiDAR Place Recognition using Vision Foundation Models",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "LiDAR Place Recognition (LPR) is a key component in robotic localization, enabling robots to align current scans with prior maps of their environment. While Visual Place Recognition (VPR) has embraced Vision Foundation Models (VFMs) to enhance descriptor robustness, LPR has relied on task-specific models with limited use of pre-trained foundation-level knowledge. This is due to the lack of 3D foundation models and the challenges of using VFM with LiDAR point clouds. To tackle this, we introduce ImLPR, a novel pipeline that employs a pre-trained DINOv2 VFM to generate rich descriptors for LPR. To our knowledge, ImLPR is the first method to leverage a VFM to support LPR. ImLPR converts raw point clouds into Range Image Views (RIV) to leverage VFM in the LiDAR domain. It employs MultiConv adapters and Patch-InfoNCE loss for effective feature learning. We validate ImLPR using public datasets where it outperforms state-of-the-art (SOTA) methods in intra-session and inter-session LPR with top Recall@1 and F1 scores across various LiDARs. We also demonstrate that RIV outperforms Bird\u2019s-Eye-View (BEV) as a representation choice for adapting LiDAR for VFM. We release ImLPR as open source for the robotics community.",
        "keywords": "LiDAR Place Recognition;Deep Learning;Vision Foundation Model",
        "primary_area": "",
        "supplementary_material": "/attachment/a9fd7a34fe7ec49c151ec29b70ece3cd6c9c2034.zip",
        "author": "Minwoo Jung;Lanke Frank Tarimo Fu;Maurice Fallon;Ayoung Kim",
        "authorids": "~Minwoo_Jung1;~Lanke_Frank_Tarimo_Fu1;~Maurice_Fallon1;~Ayoung_Kim1",
        "gender": ";M;M;",
        "homepage": "http://minwoo0611.github.io/;https://github.com/fulkast;https://ori.ox.ac.uk/ori-people/maurice-fallon/;",
        "dblp": ";;68/7394.html;",
        "google_scholar": "aKPTi7gAAAAJ;;https://scholar.google.co.uk/citations?user=BqV8LaoAAAAJ;",
        "orcid": "0000-0002-5623-6288;;0000-0003-2940-0879;",
        "linkedin": "minwoo-jung-a6757524a/;;;",
        "or_profile": "~Minwoo_Jung1;~Lanke_Frank_Tarimo_Fu1;~Maurice_Fallon1;~Ayoung_Kim1",
        "aff": "Seoul National University;University of Oxford;University of Oxford;",
        "aff_domain": "snu.ac.kr;robots.ox.ac.uk;ox.ac.uk;",
        "position": "PhD student;PhD student;Assistant Professor;",
        "bibtex": "@inproceedings{\njung2025imlpr,\ntitle={Im{LPR}: Image-based Li{DAR} Place Recognition using Vision Foundation Models},\nauthor={Minwoo Jung and Lanke Frank Tarimo Fu and Maurice Fallon and Ayoung Kim},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=O2RGlBfR9H}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=O2RGlBfR9H",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Seoul National University;University of Oxford;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.snu.ac.kr;https://www.ox.ac.uk;",
        "aff_unique_abbr": "SNU;Oxford;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "South Korea;United Kingdom;"
    },
    {
        "id": "OXHF0BvmRT",
        "title": "Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We introduce a model for monocular RGB relative pose estimation of a ground robot that trains from scratch without pose labels nor prior knowledge about the robot's shape or appearance.\nAt training time, we assume: (i) a robot fitted with multiple LEDs, whose states are independent and known at each frame; (ii) knowledge of the approximate viewing direction of each LED; and (iii) availability of a calibration image with a known target distance, to address the ambiguity of monocular depth estimation.\nTraining data is collected by a pair of robots moving randomly without needing external infrastructure or human supervision.\nOur model trains on the task of predicting from an image the state of each LED on the robot.\nIn doing so, it learns to predict the position of the robot in the image, its distance, and its relative bearing.\nAt inference time, the state of the LEDs is unknown, can be arbitrary, and does not affect the pose estimation performance.\nQuantitative experiments indicate that our approach: is competitive with SoA approaches that require supervision from pose labels or a CAD model of the robot; generalizes to different domains; and handles multi-robot pose estimation.",
        "keywords": "Self-supervised Learning;Pretext Task;Visual Pose Estimation",
        "primary_area": "",
        "supplementary_material": "/attachment/4b55d114fbad89460ca3ba3ed96602a03999f6a9.zip",
        "author": "Nicholas Carlotti;Mirko Nava;Alessandro Giusti",
        "authorids": "~Nicholas_Carlotti1;~Mirko_Nava2;~Alessandro_Giusti1",
        "gender": "M;M;",
        "homepage": ";https://mirko.nava.website/;",
        "dblp": ";;",
        "google_scholar": "hPOoxrsAAAAJ;baTfv5MAAAAJ;",
        "orcid": ";0000-0002-3736-0419;",
        "linkedin": ";mirko-nava/;",
        "or_profile": "~Nicholas_Carlotti1;~Mirko_Nava2;~Alessandro_Giusti1",
        "aff": "Universita della Svizzera Italiana;IDSIA - SUPSI;",
        "aff_domain": "usi.ch;idsia.ch;",
        "position": "PhD student;Postdoc;",
        "bibtex": "@inproceedings{\ncarlotti2025selfsupervised,\ntitle={Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying {LED} States},\nauthor={Nicholas Carlotti and Mirko Nava and Alessandro Giusti},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=OXHF0BvmRT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=OXHF0BvmRT",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2",
        "aff_unique_norm": "Universita della Svizzera Italiana;IDSIA - SUPSI;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.usi.ch;;",
        "aff_unique_abbr": "USI;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland;"
    },
    {
        "id": "P0uqo7CpL8",
        "title": "Contrastive Forward Prediction Reinforcement Learning for Adaptive Fault-Tolerant Legged Robots",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In complex environments, adaptive and fault-tolerant capabilities are essential for legged robot locomotion. To address this challenge, this study proposes a reinforcement learning framework that integrates contrastive learning with forward prediction to achieve fault-tolerant locomotion for legged robots. This framework constructs a forward prediction model with contrastive learning, incorporating a comparator and a forward model. The forward model predicts the robot's subsequent state, and the comparator compares these predictions with actual states to generate critical prediction errors. These errors are systematically integrated into the controller, facilitating the continuous adjustment and refinement of control signals.Experiments on quadruped robots across different terrains and various joint damage scenarios have verified the effectiveness of our method, especially the functions of the comparator and the forward model. Furthermore, robots can adapt to locked joints without prior training, demonstrating zero-shot transfer capability. Finally, the proposed method demonstrates universal applicability to both quadruped and hexapod robots, highlighting its potential for broader applications in legged robotics.",
        "keywords": "Legged robot locomotion;Fault tolerance control;Deep Reinforcement learning",
        "primary_area": "",
        "supplementary_material": "/attachment/f72d839da5387ae2986d9101e308adf3a3732ae1.zip",
        "author": "Yangqing Fu;Yang Zhang;Qiyue Yang;Liyun Yan;Zhanxiang Cao;Yue Gao",
        "authorids": "~Yangqing_Fu1;zhangyang-sjtu-2022@sjtu.edu.cn;yangqiyue@sjtu.edu.cn;ylyem9x@sjtu.edu.cn;caozx1110@sjtu.edu.cn;~Yue_Gao8",
        "gender": "M;;;;;F",
        "homepage": ";;;;;https://gaoyue.sjtu.edu.cn/",
        "dblp": "310/4032;;;;;",
        "google_scholar": "9uLitNAAAAAJ;;;;;jlweMD8AAAAJ",
        "orcid": ";;;;;",
        "linkedin": ";;;;;",
        "or_profile": "~Yangqing_Fu1;zhangyang-sjtu-2022@sjtu.edu.cn;yangqiyue@sjtu.edu.cn;ylyem9x@sjtu.edu.cn;caozx1110@sjtu.edu.cn;~Yue_Gao8",
        "aff": "Shanghai Jiaotong University;;;;;Shanghai Jiaotong University",
        "aff_domain": "sjtu.edu.cn;;;;;sjtu.edu.cn",
        "position": "PhD student;;;;;Associate Professor",
        "bibtex": "@inproceedings{\nfu2025contrastive,\ntitle={Contrastive Forward Prediction Reinforcement Learning for Adaptive Fault-Tolerant Legged Robots},\nauthor={Yangqing Fu and Yang Zhang and Qiyue Yang and Liyun Yan and Zhanxiang Cao and Yue Gao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=P0uqo7CpL8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=P0uqo7CpL8",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;1;1;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;",
        "aff_unique_abbr": "SJTU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "PMKwnV6Azi",
        "title": "HALO : Human Preference Aligned Offline Reward Learning for Robot Navigation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In this paper, we introduce HALO, a novel Offline Reward Learning algorithm that quantifies human intuition in navigation into a vision-based reward function for robot navigation. HALO learns a reward model from offline data, leveraging expert trajectories collected from mobile robots. During training, actions are randomly sampled from the action space around the expert action and ranked using a Boltzmann probability distribution that combines their distance to the expert action with human preference scores derived from intuitive navigation queries based on the corresponding egocentric camera feed. These scores establish preference rankings, enabling the training of a novel reward model based on Plackett-Luce loss, which allows for preference-driven navigation. To demonstrate the effectiveness of HALO, we deploy its reward model in two downstream applications: (i) an offline learned policy trained directly on the HALO-derived rewards, and (ii) a model-predictive-control (MPC) based planner that incorporates the HALO reward as an additional cost term. This showcases the versatility of HALO across both learning-based and classical navigation frameworks. Our real-world deployments on a Clearpath Husky across multiple scenarios demonstrate that policies trained with HALO achieve improved performance over state-of-the-art methods in terms of success rate and normalized trajectory length while maintaining lower Fr\u00e9chet distance with the human expert trajectories.",
        "keywords": "Reward Modelling;Preference Alignment;Vision-based Navigation",
        "primary_area": "",
        "supplementary_material": "/attachment/39805ec99e50afe82d5a50fa67811f41fc9119bc.zip",
        "author": "Gershom Seneviratne;Jianyu An;Sahire Ellahy;Kasun Weerakoon;Mohamed Bashir Elnoor;Jonathan Deepak Kannan;Amogha Thalihalla Sunil;Dinesh Manocha",
        "authorids": "~Gershom_Seneviratne1;~Jianyu_An1;~Sahire_Ellahy1;~Kasun_Weerakoon1;~Mohamed_Bashir_Elnoor1;~Jonathan_Deepak_Kannan1;~Amogha_Thalihalla_Sunil1;~Dinesh_Manocha3",
        "gender": "M;Not Specified;M;M;;M;F;M",
        "homepage": "https://gershom.me/;https://www.cs.umd.edu/people/jianyu34;;https://www.kasunweerakoon.org/;;;;https://www.cs.umd.edu/people/dmanocha",
        "dblp": ";;;;;;;m/DineshManocha",
        "google_scholar": "iNiD2wYAAAAJ;;;-JsBJxUAAAAJ;;;;X08l_4IAAAAJ",
        "orcid": ";;;;;;;0000-0001-7047-9801",
        "linkedin": ";;sahire-ellahy-3b54ab187/;kasun-weerakoon/;;jonathankannan/;amoghasunil/;dinesh-manocha-2311846",
        "or_profile": "~Gershom_Seneviratne1;~Jianyu_An1;~Sahire_Ellahy1;~Kasun_Weerakoon1;~Mohamed_Bashir_Elnoor1;~Jonathan_Deepak_Kannan1;~Amogha_Thalihalla_Sunil1;~Dinesh_Manocha3",
        "aff": "University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park;;University of Maryland, College Park;University of Maryland, College Park;University of Maryland, College Park",
        "aff_domain": "umd.edu;umd.edu;umd.edu;umd.edu;;umd.edu;umd.edu;umd.edu",
        "position": "PhD student;PhD student;MS student;PhD student;;MS student;MS student;Professor",
        "bibtex": "@inproceedings{\nseneviratne2025halo,\ntitle={{HALO} : Human Preference Aligned Offline Reward Learning for Robot Navigation},\nauthor={Gershom Seneviratne and Jianyu An and Sahire Ellahy and Kasun Weerakoon and Mohamed Bashir Elnoor and Jonathan Deepak Kannan and Amogha Thalihalla Sunil and Dinesh Manocha},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=PMKwnV6Azi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=PMKwnV6Azi",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;1;0;0;0",
        "aff_unique_norm": "University of Maryland;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www/umd.edu;",
        "aff_unique_abbr": "UMD;",
        "aff_campus_unique_index": "0;0;0;0;0;0;0",
        "aff_campus_unique": "College Park;",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "PTJopl2uaU",
        "title": "Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Generalist robot policies trained on large-scale datasets such as Open X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks. However, they often struggle to generalize beyond the distribution of their training data. In this paper, we investigate the underlying cause of this limited generalization capability.  We identify shortcut learning\u2014the reliance on task-irrelevant features\u2014as a key impediment to generalization. Through comprehensive theoretical and empirical analysis, we uncover two primary contributors to shortcut learning: (1) limited diversity within individual sub-datasets, and (2) significant distributional disparities across sub-datasets, leading to dataset fragmentation. These issues arise from the inherent structure of large-scale datasets like OXE, which are typically composed of multiple sub-datasets collected independently across varied environments and embodiments. Our findings provide critical insights into dataset collection strategies that can reduce shortcut learning and enhance the generalization ability of generalist robot policies. Moreover, in scenarios where acquiring new large-scale data is impractical, we demonstrate that carefully selected robotic data augmentation strategies can effectively reduce shortcut learning in existing offline datasets, thereby improving generalization capabilities of generalist robot policies, e.g., $\\pi_0$ in the SIMPLER Environment.",
        "keywords": "Generalist Robot Policies;Shortcut Learning;Large-scale Robotic Datasets",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Youguang Xing;Xu Luo;Junlin Xie;Lianli Gao;Heng Tao Shen;Jingkuan Song",
        "authorids": "~Youguang_Xing1;~Xu_Luo1;~Junlin_Xie3;~Lianli_Gao1;~Heng_Tao_Shen3;~Jingkuan_Song3",
        "gender": "M;M;M;F;;M",
        "homepage": "https://github.com/Lucky-Light-Sun;https://frankluox.github.io/;https://memo.sylin.host;https://lianligao.github.io/;;https://cfm.uestc.edu.cn/~songjingkuan/",
        "dblp": ";06/2622-3;;123/9849.html;;70/10575",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;z6OXVpIAAAAJ;https://scholar.google.com.au/citations?user=zsm2dpYAAAAJ;;F5Zy9V4AAAAJ",
        "orcid": ";0000-0001-9827-1244;;;;0000-0002-2549-8322",
        "linkedin": ";;;;;",
        "or_profile": "~Youguang_Xing1;~Xu_Luo1;~Junlin_Xie3;~Lianli_Gao1;~Heng_Tao_Shen3;~Jingkuan_Song3",
        "aff": "University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;Galaxea+University of Electronic Science and Technology of China;University of Electronic Science and Technology of China;;Tongji University",
        "aff_domain": "uestc.edu.cn;uestc.edu.cn;galaxea.ai+uestc.edu.cn;uestc.edu.cn;;tongji.edu.cn",
        "position": "Undergrad student;PhD student;Intern+MS student;Full Professor;;Full Professor",
        "bibtex": "@inproceedings{\nxing2025shortcut,\ntitle={Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation},\nauthor={Youguang Xing and Xu Luo and Junlin Xie and Lianli Gao and Heng Tao Shen and Jingkuan Song},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=PTJopl2uaU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=PTJopl2uaU",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1+0;0;2;3",
        "aff_unique_norm": "University of Electronic Science and Technology of China;Galaxea;;Tongji University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.uestc.edu.cn;;;https://www.tongji.edu.cn",
        "aff_unique_abbr": "UESTC;;;Tongji",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "PwKsCO6TAF",
        "title": "KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Underactuated soft robot hands offer inherent safety and adaptability advantages over rigid systems, but developing dexterous manipulation skills remains challenging. While imitation learning shows promise for complex manipulation tasks, traditional approaches struggle with soft systems due to demonstration collection challenges and ineffective state representations. We present KineSoft, a framework enabling direct kinesthetic teaching of soft robotic hands by leveraging their natural compliance as a skill teaching advantage rather than only as a control challenge. KineSoft makes two key contributions: (1) an internal strain sensing array providing occlusion-free proprioceptive shape estimation, and (2) a shape-based imitation learning framework that uses proprioceptive feedback with a low-level shape-conditioned controller to ground diffusion-based policies. This enables human demonstrators to physically guide the robot while the system learns to associate proprioceptive patterns with successful manipulation strategies. We validate KineSoft through physical experiments, demonstrating superior shape estimation accuracy compared to baseline methods, precise shape-trajectory tracking, and higher task success rates compared to baseline imitation learning approaches. KineSoft's results demonstrate that embracing the inherent properties of soft robots leads to intuitive and robust dexterous manipulation capabilities.",
        "keywords": "Soft Robots;Proprioceptive Estimation;Imitation Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/3eddc5813ac10f0629f331a7a9202390d195c7a2.zip",
        "author": "Uksang Yoo;Jonathan Francis;Jean Oh;Jeffrey Ichnowski",
        "authorids": "~Uksang_Yoo1;~Jonathan_Francis1;~Jean_Oh2;~Jeffrey_Ichnowski1",
        "gender": ";;F;M",
        "homepage": ";;http://www.cs.cmu.edu/~jeanoh/;https://ichnow.ski",
        "dblp": ";;62/4860;89/1741",
        "google_scholar": ";;;1OdtfywAAAAJ",
        "orcid": ";;;0000-0003-4874-9478",
        "linkedin": ";;;",
        "or_profile": "~Uksang_Yoo1;~Jonathan_Francis1;~Jean_Oh2;~Jeffrey_Ichnowski1",
        "aff": ";;Carnegie Mellon University;Carnegie Mellon University",
        "aff_domain": ";;cmu.edu;cmu.edu",
        "position": ";;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nyoo2025kinesoft,\ntitle={KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands},\nauthor={Uksang Yoo and Jonathan Francis and Jean Oh and Jeffrey Ichnowski},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=PwKsCO6TAF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=PwKsCO6TAF",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": ";Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.cmu.edu",
        "aff_unique_abbr": ";CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "Q0H9xlNdVm",
        "title": "SafeBimanual: Diffusion-based trajectory optimization for safe bimanual manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Bimanual manipulation has been widely applied in household services and manufacturing, which enables the complex task completion with coordination requirements. Recent diffusion-based policy learning approaches have achieved promising performance in modeling action distributions for bimanual manipulation. However, they ignored the physical safety constraints of bimanual manipulation, which leads to the dangerous behaviors with damage to robots and objects.  To this end, we propose a test-time trajectory optimization framework named SafeBimanual for any pre-trained diffusion-based bimanual manipulation policies, which imposes the safety constraints on bimanual actions to avoid dangerous robot behaviors with improved success rate. Specifically, we design diverse cost functions for safety constraints in different dual-arm cooperation patterns including avoidance of tearing objects and collision between arms and objects, which optimizes the manipulator trajectories with guided sampling of diffusion denoising process. Moreover, we employ a vision-language model (VLM) to schedule the cost functions by specifying keypoints and corresponding pairwise relationship, so that the optimal safety constraint is dynamically generated in the entire bimanual manipulation process. SafeBimanual demonstrates superiority on 8 simulated tasks in RoboTwin with a 11.1\\% increase in success rate and a 18.9\\% reduction in unsafe interactions over state-of-the-art diffusion-based methods. Extensive experiments on 4 real-world tasks further verify its practical value by improving the success rate by 32.5\\%.",
        "keywords": "Bimanual manipulation;diffusion policy;trajectory optimization",
        "primary_area": "",
        "supplementary_material": "/attachment/3760de4076abb4a60d07b8454a9209f5120b75f9.zip",
        "author": "Haoyuan Deng;Wenkai Guo;Qianzhun Wang;Zhenyu Wu;Ziwei Wang",
        "authorids": "~Haoyuan_Deng1;~Wenkai_Guo2;~Qianzhun_Wang1;~Zhenyu_Wu6;~Ziwei_Wang2",
        "gender": "M;M;M;;M",
        "homepage": "https://denghaoyuan123.github.io/;;;https://github.com/Gary3410;https://ziweiwangthu.github.io/",
        "dblp": "394/8771;;;;136/5574-1",
        "google_scholar": "https://scholar.google.com/citations?hl=zh-CN;5XqiG60AAAAJ;;https://scholar.google.com/citations?hl=zh-CN;cMTW09EAAAAJ",
        "orcid": "0009-0002-5975-0394;;;0009-0002-4827-6017;0000-0001-9225-8495",
        "linkedin": "haoyuan-deng-b7234a270/;wenkai-guo-ntu-sg/;wang-qianzhun/;;",
        "or_profile": "~Haoyuan_Deng1;~Wenkai_Guo2;~Qianzhun_Wang1;~Zhenyu_Wu6;~Ziwei_Wang2",
        "aff": "Nanyang Technological University+Nanyang Technological University;Nanyang Technological University;Nanyang Technological University;Beijing University of Posts and Telecommunications;Nanyang Technological University",
        "aff_domain": "ntu.edu.sg+ntu.edu.sg;ntu.edu.sg;ntu.edu.sg;bupt.edu.cn;ntu.edu.sg",
        "position": "PhD student+MS student;MS student;MS student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\ndeng2025safebimanual,\ntitle={SafeBimanual: Diffusion-based trajectory optimization for safe bimanual manipulation},\nauthor={Haoyuan Deng and Wenkai Guo and Qianzhun Wang and Zhenyu Wu and Ziwei Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Q0H9xlNdVm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Q0H9xlNdVm",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+0;0;0;1;0",
        "aff_unique_norm": "Nanyang Technological University;Beijing University of Posts and Telecommunications",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ntu.edu.sg;http://www.bupt.edu.cn/",
        "aff_unique_abbr": "NTU;BUPT",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Beijing",
        "aff_country_unique_index": "0+0;0;0;1;0",
        "aff_country_unique": "Singapore;China"
    },
    {
        "id": "QPTZfaxATs",
        "title": "RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Vision-Language-Action (VLA) models, pre-trained on large-scale imitation learning datasets, have demonstrated remarkable capabilities in visuomotor control. However, these models exhibit diverse failure modes in unstructured real-world environments, limiting the widespread adoption of VLAs in robotics. Efforts to enhance the robustness and generalization of VLAs have gradually shifted from the pre-training to the post-training phase. Yet, the potential of scaling test-time compute remains underexplored. In this paper, we investigate test-time scaling for robotics through the lens of sampling and verification. We first demonstrate that the relationship between action error and the number of generated samples follows an exponentiated power law across a range of VLAs, indicating the existence of inference-time scaling laws. Building on this insight, we propose a synthetic data generation pipeline for training a Vision-Language Model (VLM)-based action verifier, and show that scaling the synthetic dataset consistently improves verification and downstream accuracy. We then introduce RoboMonkey, a test-time scaling framework for VLAs. At deployment, RoboMonkey samples a small set of actions from a VLA, applies Gaussian perturbations and majority voting to construct an action proposal distribution, and then uses the VLM-based verifier to select the optimal action. Through extensive evaluations across simulated and real-world environments, we show that pairing existing VLAs with RoboMonkey yields significant performance gains, achieving a 25\\% absolute improvement on out-of-distribution tasks and 8\\% higher average success rate on in-distribution tasks. Additionally, when adapting to new robot setups, we show that fine-tuning both VLAs and action verifiers yields a 7\\% performance increase compared to fine-tuning VLAs alone.",
        "keywords": "Vision-Language-Action Models;Test-Time Scaling;Reward Learning;Imitation Learning;Generalist Policies;Visuomotor Control",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jacky Kwok;Christopher Agia;Rohan Sinha;Matt Foutter;Shulu Li;Ion Stoica;Azalia Mirhoseini;Marco Pavone",
        "authorids": "~Jacky_Kwok2;~Christopher_Agia1;~Rohan_Sinha1;~Matt_Foutter1;~Shulu_Li1;~Ion_Stoica1;~Azalia_Mirhoseini3;~Marco_Pavone1",
        "gender": "M;M;;M;;M;;M",
        "homepage": ";https://www.chrisagia.com/;https://www.stanford.edu/;https://www.linkedin.com/in/matthew-foutter-7b1366192/;;http://people.eecs.berkeley.edu/~istoica/;;https://web.stanford.edu/~pavone/",
        "dblp": ";268/3555;;;;s/IonStoica;;91/3382-1.html",
        "google_scholar": ";t8Em5FwAAAAJ;;;;vN-is70AAAAJ;;RhOpyXcAAAAJ",
        "orcid": ";0000-0002-1208-2539;;;0009-0001-7289-6577;;;",
        "linkedin": "jackykwok02/;agiachris/;;;;ionstoica;;",
        "or_profile": "~Jacky_Kwok2;~Christopher_Agia1;~Rohan_Sinha1;~Matt_Foutter1;~Shulu_Li1;~Ion_Stoica1;~Azalia_Mirhoseini3;~Marco_Pavone1",
        "aff": "Stanford University;Stanford University;Stanford University;;University of California, Berkeley+Fudan University;University of California, Berkeley;;NVIDIA+Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;;berkeley.edu+fudan.edu.cn;berkeley.edu;;nvidia.com+stanford.edu",
        "position": "PhD student;PhD student;PhD student;;PhD student+Undergrad student;Full Professor;;Director, Autonomous Vehicle Research+Associate Professor",
        "bibtex": "@inproceedings{\nkwok2025robomonkey,\ntitle={RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models},\nauthor={Jacky Kwok and Christopher Agia and Rohan Sinha and Matt Foutter and Shulu Li and Ion Stoica and Azalia Mirhoseini and Marco Pavone},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=QPTZfaxATs}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=QPTZfaxATs",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1;2+3;2;1;4+0",
        "aff_unique_norm": "Stanford University;;University of California, Berkeley;Fudan University;NVIDIA",
        "aff_unique_dep": ";;;;NVIDIA Corporation",
        "aff_unique_url": "https://www.stanford.edu;;https://www.berkeley.edu;https://www.fudan.edu.cn;https://www.nvidia.com",
        "aff_unique_abbr": "Stanford;;UC Berkeley;Fudan;NVIDIA",
        "aff_campus_unique_index": "0;0;0;2;2;0",
        "aff_campus_unique": "Stanford;;Berkeley",
        "aff_country_unique_index": "0;0;0;0+2;0;0+0",
        "aff_country_unique": "United States;;China"
    },
    {
        "id": "QtVZUPCKrY",
        "title": "Long Range Navigator (LRN): Extending robot planning horizons beyond metric maps",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "A robot navigating an outdoor environment with no prior knowledge of the space must rely on its local sensing, which is in the form of a local metric map or local policy with some fixed horizon. A limited planning horizon can often result in myopic decisions leading the robot off course or worse, into very difficult terrain. In this work, we make a key observation that long range navigation only necessitates identifying good frontier directions for planning instead of full map knowledge. To address this, we introduce Long Range Navigator (LRN), which learns to predict \u2018affordable\u2019 frontier directions from high-dimensional camera images. LRN is trained entirely on unlabeled egocentric videos, making it scalable and adaptable. In off-road tests on Spot and a large vehicle, LRN reduces human interventions and improves decision speed when integrated into existing navigation stacks.",
        "keywords": "Robot Perception;Sensing & Vision;Robot Planning;Navigation;Field Robotics",
        "primary_area": "",
        "supplementary_material": "/attachment/64336ea9bb331ae6e6f9904de1d02d8c32d6657e.zip",
        "author": "Matt Schmittle;Rohan Baijal;Nathan Hatch;Rosario Scalise;Mateo Guaman Castro;Sidharth Talia;Khimya Khetarpal;Byron Boots;Siddhartha Srinivasa",
        "authorids": "~Matt_Schmittle1;~Rohan_Baijal1;~Nathan_Hatch1;~Rosario_Scalise1;~Mateo_Guaman_Castro1;~Sidharth_Talia1;~Khimya_Khetarpal1;~Byron_Boots1;~Siddhartha_Srinivasa1",
        "gender": "M;M;M;Not Specified;M;M;F;;M",
        "homepage": "https://www.mattschmittle.com;https://rohanblueboybaijal.github.io;http://nhatch.GitHub.io;https://robotics.cs.washington.edu;https://mateoguaman.com/;https://www.sidharthtalia.com/;https://kkhetarpal.github.io/;;https://goodrobot.ai",
        "dblp": ";;;;;341/3748;186/3048;;",
        "google_scholar": "3uClq6kAAAAJ;p5VhtosAAAAJ;;;0hkU6uEAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.ca/citations?user=VLOUhF0AAAAJ;;https://scholar.google.com.tw/citations?user=RCi98EAAAAAJ",
        "orcid": ";;;;0000-0002-4335-2538;;;;",
        "linkedin": ";;;;;;;;",
        "or_profile": "~Matt_Schmittle1;~Rohan_Baijal1;~Nathan_Hatch1;~Rosario_Scalise1;~Mateo_Guaman_Castro1;~Sidharth_Talia1;~Khimya_Khetarpal1;~Byron_Boots1;~Siddhartha_Srinivasa1",
        "aff": "University of Washington;Department of Computer Science, University of Washington;Georgia Institute of Technology;Carnegie Mellon University+University of Washington;Department of Computer Science, University of Washington;University of Washington;;;Cruise+University of Washington",
        "aff_domain": "uw.edu;cs.washington.edu;gatech.edu;cmu.edu+uw.edu;cs.washington.edu;uw.edu;;;getcruise.com+washington.edu",
        "position": "PhD student;PhD student;PhD student;MS student+PhD student;PhD student;PhD student;;;Researcher+Full Professor",
        "bibtex": "@inproceedings{\nschmittle2025long,\ntitle={Long Range Navigator ({LRN}): Extending robot planning horizons beyond metric maps},\nauthor={Matt Schmittle and Rohan Baijal and Nathan Hatch and Rosario Scalise and Mateo Guaman Castro and Sidharth Talia and Khimya Khetarpal and Byron Boots and Siddhartha Srinivasa},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=QtVZUPCKrY}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=QtVZUPCKrY",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;2+0;0;0;3;3;4+0",
        "aff_unique_norm": "University of Washington;Georgia Institute of Technology;Carnegie Mellon University;;Cruise",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.washington.edu;https://www.gatech.edu;https://www.cmu.edu;;https://www.cruise.com",
        "aff_unique_abbr": "UW;Georgia Tech;CMU;;Cruise",
        "aff_campus_unique_index": "1;;1;",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0;0;0+0;0;0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "R5Y7Sr1DIe",
        "title": "LLM-Guided Probabilistic Program Induction for POMDP Model Estimation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Partially Observable Markov Decision Processes (POMDPs) model decision making under uncertainty. While there are many approaches to approximately solving POMDPs, we aim to address the problem of learning such models. In particular, we are interested in a subclass of POMDPs wherein the components of the model, including the observation function, reward function, transition function, and initial state distribution function, can be modeled as low-complexity probabilistic graphical models in the form of a short probabilistic program. Our strategy to learn these programs uses an LLM as a prior, generating candidate probabilistic programs that are then tested against the empirical distribution and adjusted through feedback. We experiment on a number of classical toy POMDP problems, simulated MiniGrid domains, and two real mobile-base robotics search domains involving partial observability. Our results show that using an LLM to guide in the construction of a low-complexity POMDP model can be more effective than tabular POMDP learning, behavior cloning, or direct LLM planning.",
        "keywords": "POMDP;LLM;Inference;Probabilistic;Induction;Robotics;Uncertainty",
        "primary_area": "",
        "supplementary_material": "/attachment/b81f2dbb7b2c819e00602b33436daebeeee32bc7.zip",
        "author": "Aidan Curtis;Hao Tang;Thiago Veloso;Kevin Ellis;Joshua B. Tenenbaum;Tom\u00e1s Lozano-P\u00e9rez;Leslie Pack Kaelbling",
        "authorids": "~Aidan_Curtis2;~Hao_Tang5;~Thiago_Veloso1;~Kevin_Ellis1;~Joshua_B._Tenenbaum1;~Tom\u00e1s_Lozano-P\u00e9rez1;~Leslie_Pack_Kaelbling1",
        "gender": "M;M;M;M;;M;F",
        "homepage": ";https://haotang1995.github.io/;;https://www.cs.cornell.edu/~ellisk/;;http://people.csail.mit.edu/tlp/;http://people.csail.mit.edu/lpk/",
        "dblp": ";07/5751-8;;;t/JoshuaBTenenbaum;90/752;k/LesliePackKaelbling",
        "google_scholar": "tRJf4Q8AAAAJ;;;L7XI6asAAAAJ;;gQOKAggAAAAJ;IcasIiwAAAAJ",
        "orcid": ";;;;;;0000-0001-6054-7145",
        "linkedin": ";;thiago-jvds/;;;;",
        "or_profile": "~Aidan_Curtis2;~Hao_Tang5;~Thiago_Veloso1;~Kevin_Ellis1;~Joshua_B._Tenenbaum1;~Tom\u00e1s_Lozano-P\u00e9rez1;~Leslie_Pack_Kaelbling1",
        "aff": "Massachusetts Institute of Technology;Cornell University;Massachusetts Institute of Technology;Cornell University;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;cornell.edu;mit.edu;cornell.edu;mit.edu;mit.edu;mit.edu",
        "position": "PhD student;PhD student;Undergrad student;Assistant Professor;Professor;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\ncurtis2025llmguided,\ntitle={{LLM}-Guided Probabilistic Program Induction for {POMDP} Model Estimation},\nauthor={Aidan Curtis and Hao Tang and Thiago Veloso and Kevin Ellis and Joshua B. Tenenbaum and Tom{\\'a}s Lozano-P{\\'e}rez and Leslie Pack Kaelbling},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=R5Y7Sr1DIe}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=R5Y7Sr1DIe",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;1;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Cornell University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://web.mit.edu;https://www.cornell.edu",
        "aff_unique_abbr": "MIT;Cornell",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "R5oDzOZYwb",
        "title": "ZipMPC: Compressed Context-Dependent MPC Cost via Imitation Learning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "The computational burden of model predictive control (MPC) limits its application on real-time systems, such as robots, and often requires the use of short prediction horizons. This not only affects the control performance, but also increases the difficulty of designing MPC cost functions that reflect the desired long-term objective. This paper proposes ZipMPC, a method that imitates a long-horizon MPC behaviour by learning a compressed and context-dependent cost function for a short-horizon MPC. It improves performance over alternative methods, such as approximate explicit MPC and automatic cost parameter tuning, in particular in terms of i) optimizing the long-term objective; ii) maintaining computational costs comparable to a short-horizon MPC; iii) ensuring constraint satisfaction; and iv) generalizing control behaviour to environments not observed during training. For this purpose, ZipMPC leverages the concept of differentiable MPC with neural networks to propagate gradients of the imitation loss through the MPC optimization. We validate our proposed method in simulation and real-world experiments on autonomous racing. ZipMPC consistently completes laps faster than selected baselines, achieving lap times close to the long-horizon MPC baseline. In challenging scenarios where the short-horizon MPC baseline fails to complete a lap, ZipMPC is able to do so. In particular, these performance gains are also observed on tracks unseen during training.",
        "keywords": "Imitation Learning;Model Predictive Control;Context-Dependent Control",
        "primary_area": "",
        "supplementary_material": "/attachment/e7e513cc97aa3cc903e1d4951310230c7b6f412e.zip",
        "author": "Rahel Rickenbach;Alan Lahoud;Erik Schaffernicht;Melanie Zeilinger;Johannes A. Stork",
        "authorids": "~Rahel_Rickenbach1;~Alan_Lahoud1;~Erik_Schaffernicht1;~Melanie_Zeilinger1;~Johannes_A._Stork1",
        "gender": ";M;M;F;",
        "homepage": "https://idsc.ethz.ch/research-zeilinger/people/person-detail.MjA3MzY4.TGlzdC8xOTI5LDg4NTM5MTE3.html;;;;",
        "dblp": ";;85/5178;41/7142;",
        "google_scholar": ";;https://scholar.google.se/citations?hl=sv;;",
        "orcid": ";0000-0003-0216-006X;0000-0002-0804-8637;0000-0003-4570-7571;",
        "linkedin": ";lahoudalan/;;;",
        "or_profile": "~Rahel_Rickenbach1;~Alan_Lahoud1;~Erik_Schaffernicht1;~Melanie_Zeilinger1;~Johannes_A._Stork1",
        "aff": "ETHZ - ETH Zurich;\u00d6rebro University;Technical University of Applied Sciences W\u00fcrzburg-Schweinfurt+\u00d6rebro University;ETHZ - ETH Zurich;",
        "aff_domain": "ethz.ch;oru.se;thws.de+oru.se;ethz.ch;",
        "position": "PhD student;PhD student;Full Professor+Lecturer;Associate Professor;",
        "bibtex": "@inproceedings{\nrickenbach2025zipmpc,\ntitle={Zip{MPC}: Compressed Context-Dependent {MPC} Cost via Imitation Learning},\nauthor={Rahel Rickenbach and Alan Lahoud and Erik Schaffernicht and Melanie Zeilinger and Johannes A. Stork},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=R5oDzOZYwb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=R5oDzOZYwb",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2+1;0;3",
        "aff_unique_norm": "ETH Zurich;\u00d6rebro University;Technical University of Applied Sciences W\u00fcrzburg-Schweinfurt;",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ethz.ch;https://www.oru.se;;",
        "aff_unique_abbr": "ETHZ;\u00d6rebro U;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;0",
        "aff_country_unique": "Switzerland;Sweden;"
    },
    {
        "id": "RFmezNsPWV",
        "title": "DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Enabling robots to perform diverse tasks across varied environments is a central challenge in robot learning. While vision-language-action (VLA) models have shown promise for generalizable robot skills, realizing their full potential requires addressing limitations in action representation and efficient training. Current VLA models often focus on scaling the vision-language model (VLM) component, while the action space representation remains a critical bottleneck. This paper introduces DexVLA, a novel framework designed to enhance the efficiency and generalization capabilities of VLAs for complex, long-horizon tasks across diverse robot embodiments. DexVLA features a novel diffusion-based action expert, scaled to one billion parameters, designed for cross-embodiment learning. A novel embodiment curriculum learning strategy facilitates efficient training: (1) pre-training the diffusion expert on cross-embodiment data, (2) aligning the VLA model to specific embodiments, and (3) post-training for rapid adaptation to new tasks.  We conduct comprehensive experiments across multiple embodiments, including single-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability to challenging tasks without task-specific adaptation, its ability to learn dexterous skills on novel embodiments with limited data, and its capacity to complete complex, long-horizon tasks using only direct language prompting, such as laundry folding. In all settings, our method demonstrates superior performance compared to state-of-the-art models like OpenVLA and $\\pi_{0}$.",
        "keywords": "Vision-Language-Action Models;Robotics Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/ce918a6c0ab8711f3a85b73a59b38f2510125f4b.zip",
        "author": "Junjie Wen;Yichen Zhu;Jinming Li;Zhibin Tang;Chaomin Shen;Feifei Feng",
        "authorids": "~Junjie_Wen2;~Yichen_Zhu1;~Jinming_Li1;~Zhibin_Tang1;~Chaomin_Shen1;~Feifei_Feng1",
        "gender": ";M;M;M;M;M",
        "homepage": ";;https://scholar.google.com.hk/citations?user=kzgMk0gAAAAJ&hl=zh-CN;https://github.com/tangzb89;;",
        "dblp": ";;;;32/3402-1;27/4916.html",
        "google_scholar": ";eyKyrbsAAAAJ;https://scholar.google.com.hk/citations?user=kzgMk0gAAAAJ;7gVKzuIAAAAJ;;",
        "orcid": ";0000-0001-5126-838X;;;;",
        "linkedin": ";;;;;fengff/",
        "or_profile": "~Junjie_Wen2;~Yichen_Zhu1;~Jinming_Li1;~Zhibin_Tang1;~Chaomin_Shen1;~Feifei_Feng1",
        "aff": ";Midea Group;Shanghai University;Shanghai Jingzhi Industrial ;East China Normal University;Midea Group",
        "aff_domain": ";midea.com;shu.edu.cn;jingzhi-sh.com;ecnu.edu.cn;midea.com",
        "position": ";Researcher;MS student;Researcher;Associate Professor;Researcher",
        "bibtex": "@inproceedings{\nwen2025dexvla,\ntitle={Dex{VLA}: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control},\nauthor={Junjie Wen and Yichen Zhu and Jinming Li and Zhibin Tang and Chaomin Shen and Feifei Feng},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=RFmezNsPWV}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RFmezNsPWV",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3;4;1",
        "aff_unique_norm": ";Midea Group;Shanghai University;Shanghai Jingzhi Industrial;East China Normal University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": ";https://www.mideaglobal.com;https://www.shu.edu.cn;;http://www.ecnu.edu.cn",
        "aff_unique_abbr": ";Midea;SHU;;ECNU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1;1",
        "aff_country_unique": ";China"
    },
    {
        "id": "RUSscFSEfD",
        "title": "FastUMI: A Scalable and Hardware-Independent Universal Manipulation Interface with Dataset",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Real-world manipulation datasets for robotic arms remain scarce due to the high costs, rigid hardware dependencies, and complex setup procedures associated with existing data collection methods.\nWe introduce, a redesigned Universal Manipulation Interface (UMI) that addresses these challenges, enabling low-cost, scalable, and rapid deployment across heterogeneous platforms.\nFastUMI achieves this through: (i) hardware decoupling via extensive mechanical reengineering, which removes dependence on specialized robotic components while preserving a consistent visual perspective; (ii) replacement of complex visual\u2013inertial odometry with a commercial off-the-shelf tracker, simplifying the software stack without compromising pose estimation accuracy; and (iii) the provision of an integrated ecosystem that streamlines data acquisition, automates quality control, and ensures compatibility with both standard and enhanced imitation-learning pipelines.\nTo facilitate further research, we release an open-access dataset comprising over 15,000 real-world demonstrations spanning 24 tasks constituting one of the most extensive UMI-like resources to date.\nEmpirical evaluations show that FastUMI supports rapid deployment, reduces operational overhead, and delivers robust performance across diverse manipulation scenarios, advancing scalable data-driven robotic learning.",
        "keywords": "Imitation Learning;Manipulation;Data Collection",
        "primary_area": "",
        "supplementary_material": "/attachment/24bb7424e17d45d43d64331a6148fedaae7b5a74.zip",
        "author": "Zhaxizhuoma;Kehui Liu;Chuyue Guan;Zhongjie Jia;Ziniu Wu;Xin Liu;Tianyu Wang;Shuai Liang;Pengan CHEN;Pingrui Zhang;Haoming Song;Delin Qu;Dong Wang;Zhigang Wang;Nieqing Cao;Yan Ding;Bin Zhao;Xuelong Li",
        "authorids": "~Zhaxizhuoma1;~Kehui_Liu1;~Chuyue_Guan1;~Zhongjie_Jia1;~Ziniu_Wu3;~Xin_Liu62;~Tianyu_Wang10;~Shuai_Liang2;~Pengan_CHEN1;~Pingrui_Zhang1;~Haoming_Song1;~Delin_Qu1;~Dong_Wang1;~Zhigang_Wang3;~Nieqing_Cao1;~Yan_Ding5;~Bin_Zhao7;~Xuelong_Li2",
        "gender": "F;M;M;M;M;M;M;M;M;F;M;M;M;;;M;M;M",
        "homepage": "https://zxzm-zak.github.io/;https://github.com/MrKeee;;;https://research-information.bris.ac.uk/en/persons/ziniu-wu;https://github.com/AlexLiu-bit;https://github.com/Star-UU-Wang;https://www.cocubefun.com/;https://github.com/cpa2001/;https://github.com/zhangpingrui;https://github.com/HaomingSong;https://delinqu.github.io/;https://redwang.github.io/;;;https://yding25.github.io/;https://iopen.nwpu.edu.cn/info/1347/2105.htm;",
        "dblp": ";;;;;;;;386/2854;364/9381.html;379/3587;73/2731;40/3934-28;;;57/4533-2;73/4325-1.html;l/XuelongLi",
        "google_scholar": ";Kj1nPMkAAAAJ;;;;;;;;;;https://scholar.google.com/citations?view_op=list_works;dasL9V4AAAAJ;;5GVcOTEAAAAJ;0rP_rGUAAAAJ;https://scholar.google.com.hk/citations?user=DQB0hqwAAAAJ;ahUibskAAAAJ",
        "orcid": ";;;;;;;;;;;0009-0003-8775-0380;;;;0000-0002-7949-4351;;0000-0002-0019-4197",
        "linkedin": ";;chuyue-harrison-g-ab331a1aa?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app;%E9%92%9F%E6%9D%B0-%E8%B4%BE-b034b234a/;;;;;;;;;;;;;;",
        "or_profile": "~Zhaxizhuoma1;~Kehui_Liu1;~Chuyue_Guan1;~Zhongjie_Jia1;~Ziniu_Wu3;~Xin_Liu62;~Tianyu_Wang10;~Shuai_Liang2;~Pengan_CHEN1;~Pingrui_Zhang1;~Haoming_Song1;~Delin_Qu1;~Dong_Wang1;~Zhigang_Wang3;~Nieqing_Cao1;~Yan_Ding5;~Bin_Zhao7;~Xuelong_Li2",
        "aff": "Shanghai Jiaotong University+Shanghai AI Laboratory;Northwest Polytechnical University Xi'an+Shanghai AI Laboratory;Stanford University;Shanghai Jiaotong University;University of Bristol;Shanghai Jiaotong University+Shanghai AI Laboratory;Fudan University;Fudan University+Shanghai AI Laboratory;The Chinese University of Hong Kong+Shanghai AI Laboratory+University of Hong Kong;Fudan University+Shanghai AI Laboratory;Shanghai Jiaotong University;Fudan University+Shanghai AI Laboratory;Shanghai AI Laboratory;;Xi'an Jiaotong-Liverpool University;OneStar Robotics+Shanghai AI Laboratory;Shanghai Artificial Intelligence Labortory;China Telecom+Northwestern Polytechnical University",
        "aff_domain": "sjtu.edu.cn+pjlab.org.cn;nwpu.edu.cn+pjlab.org.cn;stanford.edu;sjtu.edu.cn;bristol.ac.uk;sjtu.edu.cn+pjlab.org.cn;fudan.edu.cn;fudan.edu.cn+pjlab.org.cn;cuhk.edu.hk+pjlab.org.cn+hku.hk;fudan.edu.cn+pjlab.org.cn;sjtu.edu.cn;fudan.edu.cn+pjlab.org.cn;pjlab.org.cn;;xjtlu.edu.cn;xonestar.com+pjlab.org.cn;pjlab.org.cn;chinatelecom.cn+nwpu.edu.cn",
        "position": "PhD student+Intern;PhD student+Researcher;MS student;PhD student;PhD student;PhD student+Intern;PhD student;PhD student+Intern;PhD student+Researcher+MS student;PhD student+Intern;PhD student;PhD student+Intern;Researcher;;Assistant Professor;Principal Researcher+Researcher;Young Scientist;Full Professor+Full Professor",
        "bibtex": "@inproceedings{\nzhaxizhuoma2025fastumi,\ntitle={Fast{UMI}: A Scalable and Hardware-Independent Universal Manipulation Interface with Dataset},\nauthor={Zhaxizhuoma and Kehui Liu and Chuyue Guan and Zhongjie Jia and Ziniu Wu and Xin Liu and Tianyu Wang and Shuai Liang and Pengan CHEN and Pingrui Zhang and Haoming Song and Delin Qu and Dong Wang and Zhigang Wang and Nieqing Cao and Yan Ding and Bin Zhao and Xuelong Li},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=RUSscFSEfD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RUSscFSEfD",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            18,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;2+1;3;0;4;0+1;5;5+1;6+1+7;5+1;0;5+1;1;8;9;10+1;11;12+13",
        "aff_unique_norm": "Shanghai Jiao Tong University;Shanghai AI Laboratory;Northwest Polytechnical University;Stanford University;University of Bristol;Fudan University;Chinese University of Hong Kong;University of Hong Kong;;Xi'an Jiao Tong-Liverpool University;OneStar Robotics;Shanghai Artificial Intelligence Laboratory;China Telecom;Northwestern Polytechnical University",
        "aff_unique_dep": ";;;;;;;;;;;;;",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.shanghai-ai-lab.com;http://www.nwpu.edu.cn;https://www.stanford.edu;https://www.bristol.ac.uk;https://www.fudan.edu.cn;https://www.cuhk.edu.hk;https://www.hku.hk;;https://www.xjtu.edu.cn/en;;http://www.shailab.org/;https://www.chinatelecom.com.cn;https://www.nwpu.edu.cn",
        "aff_unique_abbr": "SJTU;SAIL;NWPU;Stanford;Bristol;Fudan;CUHK;HKU;;XJTLU;;Shanghai AI Lab;CT;NWPU",
        "aff_campus_unique_index": ";1;2;;;3+3;;;1;;",
        "aff_campus_unique": ";Xi'an;Stanford;Hong Kong SAR",
        "aff_country_unique_index": "0+0;0+0;1;0;2;0+0;0;0+0;0+0+0;0+0;0;0+0;0;0;0;0;0+0",
        "aff_country_unique": "China;United States;United Kingdom;"
    },
    {
        "id": "RqNpzq17kw",
        "title": "ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Due to the powerful vision-language reasoning and generalization abilities, multimodal large language models (MLLMs) have garnered significant attention in the field of end-to-end (E2E) autonomous driving. However, their application to closed-loop systems remains underexplored, and current MLLM-based methods have not shown clear superiority to mainstream E2E imitation learning approaches. In this work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed for closed-loop driving through holistic reasoning with a self-supervised Next Scene Prediction task and supervised Decision Chain-of-Thought process. This dual mechanism encourages the model to align visual representations with actionable driving context, while promoting interpretable and causally grounded decision making. We curate a planning-oriented decision reasoning dataset, namely PDR, comprising 210k diverse and high-quality samples. Our method outperforms the mainstream E2E imitation learning method by a large margin of 19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan demonstrates strong zero-shot generalization on unseen DOS benchmark, highlighting its adaptability in handling zero-shot corner cases.",
        "keywords": "Multimodal LLM;Closed-Loop Evaluation;Autonomous Driving",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xueyi Liu;Zuodong Zhong;Qichao Zhang;Yuxin Guo;Yupeng Zheng;Junli Wang;Dongbin Zhao;Yun-Fu Liu;Zhiguo Su;Yinfeng Gao;Qiao Lin;Chen Huiyong",
        "authorids": "~Xueyi_Liu3;~Zuodong_Zhong1;~Qichao_Zhang3;~Yuxin_Guo2;~Yupeng_Zheng1;~Junli_Wang4;~Dongbin_Zhao1;~Yun-Fu_Liu1;~Zhiguo_Su1;~Yinfeng_Gao1;~Qiao_Lin2;~Chen_Huiyong1",
        "gender": "M;M;;F;;M;M;M;M;M;M;M",
        "homepage": "https://github.com/Liuxueyi;;;;;;http://people.ucas.ac.cn/~zhaodongbin?language=en;;https://github.com/PoulSu;https://github.com/gaoyinfeng;;https://mail.126.com/js6/main.jsp?sid=TFQPnaSxpQAvZBWLUOxxmAayXEMtYCOJ&df=webmail126#module=mbox.ListModule%7C%7B%22fid%22%3A1%2C%22order%22%3A%22date%22%2C%22desc%22%3Atrue%7D",
        "dblp": ";;;;;;40/255;;;;;",
        "google_scholar": ";;;x_0spxgAAAAJ;;atyhKdMAAAAJ;;H7hsLC4AAAAJ;;;;",
        "orcid": ";0009-0004-4448-7567;;;;;0000-0001-8218-9633;;;;;",
        "linkedin": ";;;;;;;;;;john-lin-81199228a;",
        "or_profile": "~Xueyi_Liu3;~Zuodong_Zhong1;~Qichao_Zhang3;~Yuxin_Guo2;~Yupeng_Zheng1;~Junli_Wang4;~Dongbin_Zhao1;~Yun-Fu_Liu1;~Zhiguo_Su1;~Yinfeng_Gao1;~Qiao_Lin2;~Chen_Huiyong1",
        "aff": "Institute of Automation, Chinese Academy of Sciences;University of Science and Technology Beijing+Chinese Academy of Sciences;;Tencent PCG+Institute of Automation, Chinese Academy of Sciences;;Tsinghua University+Institute of Automation, Chinese Academy of Sciences;Institute of Automation, Chinese Academy of Sciences;eacon;EACON Technology Co., Itd;University of Science and Technology Beijing;Zhejiang University;eacon",
        "aff_domain": "ia.ac.cn;ustb.edu.cn+cas.ac.cn;;tencent.com+ia.ac.cn;;mails.tsinghua.edu.cn+ia.ac.cn;ia.ac.cn;eacon.com;eacon.com;ustb.edu.cn;zju.edu.cn;eacon.com",
        "position": "PhD student;Undergrad student+Intern;;Research Intern+PhD student;;Intern+MS student;Full Professor;PhD student;Researcher;PhD student;Researcher;Intern",
        "bibtex": "@inproceedings{\nliu2025reasonplan,\ntitle={ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving},\nauthor={Xueyi Liu and Zuodong Zhong and Qichao Zhang and Yuxin Guo and Yupeng Zheng and Junli Wang and Dongbin Zhao and Yun-Fu Liu and Zhiguo Su and Yinfeng Gao and Qiao Lin and Chen Huiyong},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=RqNpzq17kw}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=RqNpzq17kw",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            12,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1+0;2;3+0;2;4+0;0;5;6;1;7;5",
        "aff_unique_norm": "Chinese Academy of Sciences;University of Science and Technology Beijing;;Tencent;Tsinghua University;eacon;EACON Technology Co., Itd;Zhejiang University",
        "aff_unique_dep": "Institute of Automation;;;PCG (Platform and Content Group);;;;",
        "aff_unique_url": "http://www.ia.cas.cn;http://www.ustb.edu.cn;;https://www.tencent.com;https://www.tsinghua.edu.cn;;;https://www.zju.edu.cn",
        "aff_unique_abbr": "CAS;USTB;;Tencent PCG;THU;;;ZJU",
        "aff_campus_unique_index": ";;",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0+0;0+0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "SNvUSjVm6C",
        "title": "Robust Dexterous Grasping of General Objects",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "The ability to robustly grasp a variety of objects is essential for dexterous robots. In this paper, we present a framework for zero-shot dynamic dexterous grasping using single-view visual inputs, designed to be resilient to various disturbances. Our approach utilizes a hand-centric object shape representation based on dynamic distance vectors between finger joints and object surfaces. This representation captures the local shape around potential contact regions rather than focusing on detailed global object geometry, thereby enhancing generalization to shape variations and uncertainties. To address perception limitations, we integrate a privileged teacher policy with a mixed curriculum learning approach, allowing the student policy to effectively distill grasping capabilities and explore for adaptation to disturbances. Trained in simulation, our method achieves success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects, demonstrating remarkable generalization. Quantitative and qualitative results validate the robustness of our policy against various disturbances.",
        "keywords": "Dexterous Grasping; Reinforcement Learning; Robot Manipulation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hui Zhang;Zijian wu;Linyi Huang;Sammy Christen;Jie Song",
        "authorids": "~Hui_Zhang31;~Zijian_wu9;~Linyi_Huang3;~Sammy_Christen1;~Jie_Song1",
        "gender": "M;;F;;M",
        "homepage": ";;;;https://ait.ethz.ch/people/song/",
        "dblp": ";;;;09/4756-6",
        "google_scholar": "https://scholar.google.com/citations?view_op=list_works;56ug9BgAAAAJ;boUC8sYAAAAJ;;https://scholar.google.com/citations?hl=en",
        "orcid": "0009-0009-5115-1806;;;;0009-0003-7484-1937",
        "linkedin": ";;;;",
        "or_profile": "~Hui_Zhang31;~Zijian_wu9;~Linyi_Huang3;~Sammy_Christen1;~Jie_Song1",
        "aff": "ETHZ - ETH Zurich;Hong Kong University of Science and Technology;The Hong Kong University of Science and Technology (GZ);;Hong Kong University of Science and Technology",
        "aff_domain": "ethz.ch;connect.hkust-gz.edu.cn;connect.hkust-gz.edu.cn;;hkust-gz.edu.cn",
        "position": "PhD student;MS student;PhD student;;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025robust,\ntitle={Robust Dexterous Grasping of General Objects},\nauthor={Hui Zhang and Zijian wu and Linyi Huang and Sammy Christen and Jie Song},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=SNvUSjVm6C}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=SNvUSjVm6C",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;2;1",
        "aff_unique_norm": "ETH Zurich;Hong Kong University of Science and Technology;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ethz.ch;https://www.ust.hk;",
        "aff_unique_abbr": "ETHZ;HKUST;",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;1;1;1",
        "aff_country_unique": "Switzerland;China;"
    },
    {
        "id": "SUqMCzslNH",
        "title": "Poke and Strike: Learning Task-Informed Exploration Policies",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In many dynamic robotic tasks, such as striking pucks into a goal outside the reachable workspace, the robot must first identify the relevant physical properties of the object for successful task execution, as it is unable to recover from failure or retry without human intervention. To address this challenge, we propose a task-informed exploration approach, based on reinforcement learning, that trains an exploration policy using rewards automatically generated from the sensitivity of a privileged task policy to errors in estimated properties. We also introduce an uncertainty-based mechanism to determine when to transition from exploration to task execution, ensuring sufficient property estimation accuracy with minimal exploration time. Our method achieves a 90% success rate on the striking task with an average exploration time under 1.2 seconds\u2014significantly outperforming baselines that achieve at most 40% success or require inefficient querying and retraining in a simulator at test time. Additionally, we demonstrate that our task-informed rewards capture the relative importance of physical properties in both the striking task and the classical CartPole example. Finally, we validate our approach by demonstrating its ability to identify object properties and adjust task execution in a physical setup using the KUKA iiwa robot arm.",
        "keywords": "Interactive Perception;Manipulation;Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/7ded0652417010dfd76c7602b51f8a3d82f20833.zip",
        "author": "Marina Y. Aoyama;Joao Moura;Juan Del Aguila Ferrandis;Sethu Vijayakumar",
        "authorids": "~Marina_Y._Aoyama1;~Joao_Moura1;~Juan_Del_Aguila_Ferrandis1;~Sethu_Vijayakumar1",
        "gender": "Not Specified;M;M;M",
        "homepage": "https://web.inf.ed.ac.uk/slmc/people;https://sites.google.com/view/joaomoura;;https://homepages.inf.ed.ac.uk/svijayak/",
        "dblp": ";;;68/1347",
        "google_scholar": ";https://scholar.google.co.uk/citations?user=1L5kTRcAAAAJ;;JdRs1sQAAAAJ",
        "orcid": ";;;",
        "linkedin": "marina-aoyama-696960201;joaopousamoura/;juan-del-aguila-ferrandis/;",
        "or_profile": "~Marina_Y._Aoyama1;~Joao_Moura1;~Juan_Del_Aguila_Ferrandis1;~Sethu_Vijayakumar1",
        "aff": "University of Edinburgh;University of Edinburgh;University of Edinburgh;University of Edinburgh",
        "aff_domain": "sms.ed.ac.uk;ed.ac.uk;ed.ac.uk;ed.ac.uk",
        "position": "PhD student;Postdoc;MS student;Full Professor",
        "bibtex": "@inproceedings{\naoyama2025poke,\ntitle={Poke and Strike: Learning Task-Informed Exploration Policies},\nauthor={Marina Y. Aoyama and Joao Moura and Juan Del Aguila Ferrandis and Sethu Vijayakumar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=SUqMCzslNH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=SUqMCzslNH",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "University of Edinburgh",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ed.ac.uk",
        "aff_unique_abbr": "Edinburgh",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United Kingdom"
    },
    {
        "id": "SebHZk78aS",
        "title": "GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We present GraspMolmo, a generalizable open-vocabulary task-oriented grasping (TOG) model. GraspMolmo predicts semantically appropriate, stable grasps conditioned on a natural language instruction and a single RGB-D frame. For instance, given \"pour me some tea\", GraspMolmo selects a grasp on a teapot handle rather than its body. Unlike prior TOG methods, which are limited by small datasets, simplistic language, and uncluttered scenes, GraspMolmo learns from a large-scale synthetic dataset of 379k samples featuring cluttered environments and diverse, realistic task descriptions. We fine-tune the Molmo visual-language model on this data, enabling GraspMolmo to generalize to novel open-vocabulary instructions and objects. In challenging real-world evaluations, GraspMolmo achieves state-of-the-art results, with a 70% prediction success on complex tasks, compared to the 35% achieved by the next best alternative. \nGraspMolmo also successfully demonstrates the ability to predict semantically correct bimanual grasps zero-shot.\nWe release our synthetic dataset, code, model, and benchmarks to accelerate research in task-semantic robotic manipulation.",
        "keywords": "task-oriented grasping;functional grasping;robotic manipulation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Abhay Deshpande;Yuquan Deng;Jordi Salvador;Arijit Ray;Winson Han;Jiafei Duan;Rose Hendrix;Yuke Zhu;Ranjay Krishna",
        "authorids": "~Abhay_Deshpande1;~Yuquan_Deng1;~Jordi_Salvador3;~Arijit_Ray1;~Winson_Han1;~Jiafei_Duan1;~Rose_Hendrix1;~Yuke_Zhu1;~Ranjay_Krishna1",
        "gender": "M;;;M;M;M;F;M;M",
        "homepage": "https://abhaybd.github.io/;;;https://arijitray1993.github.io/;;https://duanjiafei.com/;;https://yukezhu.me/;http://ranjaykrishna.com",
        "dblp": ";;53/5830;164/9384;255/5528;275/9973.html;236/4851;133/1772;167/3785",
        "google_scholar": ";;https://scholar.google.de/citations?user=YuRVs2oAAAAJ;VE-ZVW0AAAAJ;;d1WCSJIAAAAJ;TIPqRC0AAAAJ;mWGyYMsAAAAJ;IcqahyAAAAAJ",
        "orcid": ";;;0000-0002-4175-0655;;;;;0000-0001-8784-2531",
        "linkedin": "abhaybd/;;;;winsonhan/;jiafei-duan-a69b11112/;;;ranjay-krishna-1a344444/",
        "or_profile": "~Abhay_Deshpande1;~Yuquan_Deng1;~Jordi_Salvador3;~Arijit_Ray1;~Winson_Han1;~Jiafei_Duan1;~Rose_Hendrix1;~Yuke_Zhu1;~Ranjay_Krishna1",
        "aff": "Allen Institute for Artificial Intelligence;;Allen Institute for AI;Google+Boston University;Ai2;University of Washington;Allen Institute for Artificial Intelligence;Computer Science Department, University of Texas, Austin;University of Washington",
        "aff_domain": "allenai.org;;allenai.org;google.com+bu.edu;allenai.org;uw.edu;allenai.org;cs.utexas.edu;cs.washington.edu",
        "position": "Predoctoral Researcher;;Research Engineer;Intern+PhD student;Researcher;PhD student;Research Engineer;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ndeshpande2025graspmolmo,\ntitle={GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation},\nauthor={Abhay Deshpande and Yuquan Deng and Jordi Salvador and Arijit Ray and Winson Han and Jiafei Duan and Rose Hendrix and Yuke Zhu and Ranjay Krishna},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=SebHZk78aS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=SebHZk78aS",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3+4;5;6;0;7;6",
        "aff_unique_norm": "Allen Institute for Artificial Intelligence;;Allen Institute for AI;Google;Boston University;AI2;University of Washington;University of Texas at Austin",
        "aff_unique_dep": ";;;Google;;;;Computer Science Department",
        "aff_unique_url": "https://allenai.org;;https://allenai.org;https://www.google.com;https://www.bu.edu;https://www.ai2.edu;https://www.washington.edu;https://www.utexas.edu",
        "aff_unique_abbr": "AI2;;AI2;Google;BU;AI2;UW;UT Austin",
        "aff_campus_unique_index": "1;2",
        "aff_campus_unique": ";Mountain View;Austin",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "SpFH8T7gjM",
        "title": "SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Building capable household and industrial robots requires mastering the control of versatile, high-degree-of-freedom (DoF) systems such as mobile manipulators. While reinforcement learning (RL) holds promise for autonomously acquiring robot control policies, scaling it to high-DoF embodiments remains challenging. Direct RL in the real world demands both safe exploration and high sample efficiency, which are difficult to achieve in practice. Sim-to-real RL, on the other hand, is often brittle due to the reality gap. This paper introduces SLAC, a method that renders real-world RL feasible for complex embodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic latent action space. SLAC trains this latent action space via a customized unsupervised skill discovery method designed to promote temporal abstraction, disentanglement, and safety, thereby facilitating efficient downstream learning. Once a latent action space is learned, SLAC uses it as the action interface for a novel off-policy RL algorithm to autonomously learn downstream tasks through real-world interactions. We evaluate SLAC against existing methods on a suite of bimanual mobile manipulation tasks, where it achieves state-of-the-art performance. Notably, SLAC learns contact-rich whole-body tasks in under an hour of real-world interactions, without relying on any demonstrations or hand-crafted behavior priors.",
        "keywords": "Real-world RL;Latent Action;Whole-body Manipulation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Jiaheng Hu;Peter Stone;Roberto Mart\u00edn-Mart\u00edn",
        "authorids": "~Jiaheng_Hu1;~Peter_Stone1;~Roberto_Mart\u00edn-Mart\u00edn1",
        "gender": "M;M;M",
        "homepage": "https://jiahenghu.github.io/;http://www.cs.utexas.edu/~pstone;https://robertomartinmartin.com/",
        "dblp": "266/4638;s/PeterStone;153/7670",
        "google_scholar": ";qnwjcfAAAAAJ;XOJE8OEAAAAJ",
        "orcid": ";0000-0002-6795-420X;0000-0002-9586-2759",
        "linkedin": ";;",
        "or_profile": "~Jiaheng_Hu1;~Peter_Stone1;~Roberto_Mart\u00edn-Mart\u00edn1",
        "aff": "University of Texas at Austin+Google Deepmind;Sony AI+University of Texas, Austin;Amazon+University of Texas at Austin",
        "aff_domain": "utexas.edu+google.com;sony.com+utexas.edu;amazon.com+utexas.edu",
        "position": "PhD student+Intern;Principal Researcher+Full Professor;Researcher+Assistant Professor",
        "bibtex": "@inproceedings{\nhu2025slac,\ntitle={{SLAC}: Simulation-Pretrained Latent Action Space for Whole-Body Real-World {RL}},\nauthor={Jiaheng Hu and Peter Stone and Roberto Mart{\\'\\i}n-Mart{\\'\\i}n},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=SpFH8T7gjM}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=SpFH8T7gjM",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;2+0;3+0",
        "aff_unique_norm": "University of Texas at Austin;DeepMind;Sony;Amazon",
        "aff_unique_dep": ";DeepMind;Sony AI;Amazon.com, Inc.",
        "aff_unique_url": "https://www.utexas.edu;https://deepmind.com;https://www.sony.com;https://www.amazon.com",
        "aff_unique_abbr": "UT Austin;DeepMind;Sony AI;Amazon",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0+1;2+0;0+0",
        "aff_country_unique": "United States;United Kingdom;Japan"
    },
    {
        "id": "Spg25qkV81",
        "title": "Versatile Loco-Manipulation through Flexible Interlimb Coordination",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "The ability to flexibly leverage limbs for loco-manipulation is essential for enabling autonomous robots to operate in unstructured environments. Yet, prior work on loco-manipulation is often constrained to specific tasks or predetermined limb configurations. In this work, we present einforcement Learning for Interlimb Coordination (ReLIC), an approach that enables versatile loco-manipulation through flexible interlimb coordination. The key to our approach is an adaptive controller that seamlessly bridges the execution of manipulation motions and the generation of stable gaits based on task demands. Through the interplay between two controller modules, ReLIC dynamically assigns each limb for manipulation or locomotion and robustly coordinates them to achieve the task success. Using efficient reinforcement learning in simulation, ReLIC learns to perform stable gaits in accordance with the manipulation goals in the real world. To solve diverse and complex tasks, we further propose to interface the learned controller with different types of task specifications, including target trajectories, contact points, and natural language instructions. Evaluated on 12 real-world tasks that require diverse and complex coordination patterns, ReLIC demonstrates its versatility and robustness by achieving a success rate of 78.9% on average.",
        "keywords": "Loco-Manipulation;Whole-Body Control;Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/b28876c861b4a4f908bed465e74580bb3974871a.zip",
        "author": "Xinghao Zhu;Yuxin Chen;Lingfeng Sun;Farzad Niroui;Simon Le Cleac'h;Jiuguang Wang;Kuan Fang",
        "authorids": "~Xinghao_Zhu1;~Yuxin_Chen7;~Lingfeng_Sun1;~Farzad_Niroui1;~Simon_Le_Cleac'h1;~Jiuguang_Wang1;~Kuan_Fang3",
        "gender": ";M;M;;;;",
        "homepage": ";http://thomaschen98.github.io;https://lingfeng.moe;;;;",
        "dblp": ";;;;;;",
        "google_scholar": ";EzoDsIMAAAAJ;Uxb6wbkAAAAJ;-58iYyQAAAAJ;;;",
        "orcid": ";;;;;;",
        "linkedin": ";thomaschen98/;;;;;",
        "or_profile": "~Xinghao_Zhu1;~Yuxin_Chen7;~Lingfeng_Sun1;~Farzad_Niroui1;~Simon_Le_Cleac'h1;~Jiuguang_Wang1;~Kuan_Fang3",
        "aff": ";University of California, Berkeley;Boston Dynamics AI Institute;The AI Institute;;;",
        "aff_domain": ";berkeley.edu;theaiinstitute.com;theaiinstitute.com;;;",
        "position": ";PhD student;Researcher;Researcher;;;",
        "bibtex": "@inproceedings{\nzhu2025versatile,\ntitle={Versatile Loco-Manipulation through Flexible Interlimb Coordination},\nauthor={Xinghao Zhu and Yuxin Chen and Lingfeng Sun and Farzad Niroui and Simon Le Cleac'h and Jiuguang Wang and Kuan Fang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Spg25qkV81}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Spg25qkV81",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3;0;0;0",
        "aff_unique_norm": ";University of California, Berkeley;Boston Dynamics AI Institute;AI Institute",
        "aff_unique_dep": ";;AI Institute;",
        "aff_unique_url": ";https://www.berkeley.edu;https://www.bostondynamics.com/;",
        "aff_unique_abbr": ";UC Berkeley;BD AI;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Berkeley",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "TFbT7kHD89",
        "title": "Constrained Style Learning from Imperfect Demonstrations under Task Optimality",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Learning from demonstration has proven effective in robotics for acquiring natural behaviors, such as stylistic motions and lifelike agility, particularly when explicitly defining style-oriented reward functions is challenging. Synthesizing stylistic motions for real-world tasks usually requires balancing task performance and imitation quality. Existing methods generally depend on expert demonstrations closely aligned with task objectives. However, practical demonstrations are often incomplete or unrealistic, causing current methods to boost style at the expense of task performance. To address this issue, we propose formulating the problem as a  constrained Markov Decision Process (CMDP). Our approach integrates a style-imitation objective with constraints to maintain near-optimal task performance. We introduce an adaptively adjustable Lagrangian multiplier to guide the agent to imitate demonstrations selectively, capturing stylistic nuances without compromising task performance. We validate our approach across multiple robotic platforms and tasks, demonstrating both robust task performance and high-fidelity style learning. On ANYmal-D hardware we show a 14.5\\% drop in mechanical energy and a more agile gait pattern, showcasing real-world benefits.",
        "keywords": "Constrained Markov Decision Process;Imitation Learning;Legged Robots",
        "primary_area": "",
        "supplementary_material": "/attachment/5acf0b508d54cc6a987adaf55617ead20fe978d2.zip",
        "author": "Kehan Wen;Chenhao Li;Junzhe He;Marco Hutter",
        "authorids": "~Kehan_Wen1;~Chenhao_Li3;~Junzhe_He1;~Marco_Hutter1",
        "gender": "M;;M;M",
        "homepage": ";https://breadli428.github.io/;;http://www.rsl.ethz.ch",
        "dblp": ";186/9145;;04/2753",
        "google_scholar": ";kw1-DxQAAAAJ;j12Ktt8AAAAJ;https://scholar.google.ch/citations?user=DO3quJYAAAAJ",
        "orcid": "0000-0003-0411-4489;;0009-0008-5928-1237;0000-0002-4285-4990",
        "linkedin": ";chenhao-li-86080b1b0/;;",
        "or_profile": "~Kehan_Wen1;~Chenhao_Li3;~Junzhe_He1;~Marco_Hutter1",
        "aff": "ETHZ - ETH Zurich;ETHZ - ETH Zurich;ETHZ - ETH Zurich;ETHZ - ETH Zurich",
        "aff_domain": "ethz.ch;ethz.ch;ethz.ch;ethz.ch",
        "position": "MS student;PhD student;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nwen2025constrained,\ntitle={Constrained Style Learning from Imperfect Demonstrations under Task Optimality},\nauthor={Kehan Wen and Chenhao Li and Junzhe He and Marco Hutter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=TFbT7kHD89}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=TFbT7kHD89",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "Th1kFSnjUW",
        "title": "MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Mobile manipulation is the fundamental challenge for robotics in assisting humans with diverse tasks and environments in everyday life.\nConventional mobile manipulation approaches often struggle to generalize across different tasks and environments due to the lack of large-scale training. However, recent advances in manipulation foundation models demonstrate impressive generalization capability on a wide range of fixed-base manipulation tasks, which are still limited to a fixed setting. Therefore, we devise a plug-in module named MoTo, which can be combined with any off-the-shelf manipulation foundation model to empower them with mobile manipulation ability. Specifically, we propose an interaction-aware navigation policy to generate agent docking points for generalized mobile manipulation.  To enable zero-shot ability, we propose an interaction keypoints framework via vision-language models (VLM) under multi-view consistency for both target object and robotic arm following instructions, where fixed-base manipulation foundation models can be employed. We further propose motion planning objectives for the mobile base and robot arm, which minimize the distance between the two keypoints and maintain the physical feasibility of trajectories. In this way, MoTo guides the agent to move to the docking points where fixed-base manipulation can be successfully performed, and leverages VLM generation and trajectory optimization to achieve mobile manipulation in a zero-shot manner, without any requirement on mobile manipulation expert data. Extensive experimental results on OVMM and real-world demonstrate that MoTo achieves success rates of 2.68% and 16.67% higher than the state-of-the-art mobile manipulation methods, respectively, without requiring additional training data.",
        "keywords": "Mobile Manipulation;VLA;VLM",
        "primary_area": "",
        "supplementary_material": "/attachment/d5a9640c5a164ce32dbc5fa0c72675250f44a8f1.zip",
        "author": "Zhenyu Wu;Angyuan Ma;Xiuwei Xu;Hang Yin;Yinan Liang;Ziwei Wang;Jiwen Lu;Haibin Yan",
        "authorids": "~Zhenyu_Wu6;~Angyuan_Ma1;~Xiuwei_Xu1;~Hang_Yin5;~Yinan_Liang1;~Ziwei_Wang2;~Jiwen_Lu1;~Haibin_Yan1",
        "gender": ";;M;;M;M;M;F",
        "homepage": "https://github.com/Gary3410;https://ivg.au.tsinghua.edu.cn/;https://xuxw98.github.io/;https://www.au.tsinghua.edu.cn/;http://ivg.au.tsinghua.edu.cn/index.php;https://ziweiwangthu.github.io/;http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/;https://smp.bupt.edu.cn/info/1154/2621.htm",
        "dblp": ";;315/9374;;359/0769;136/5574-1;http://dblp.uni-trier.de/pers/hd/l/Lu:Jiwen;27/9964",
        "google_scholar": "https://scholar.google.com/citations?hl=zh-CN;;4G627acAAAAJ;;Zpxs0Z4AAAAJ;cMTW09EAAAAJ;TN8uDQoAAAAJ;https://scholar.google.com.sg/citations?user=-AQLKlsAAAAJ",
        "orcid": "0009-0002-4827-6017;;;;;0000-0001-9225-8495;0000-0002-6121-5529;0000-0003-0811-6545",
        "linkedin": ";;;;;;;",
        "or_profile": "~Zhenyu_Wu6;~Angyuan_Ma1;~Xiuwei_Xu1;~Hang_Yin5;~Yinan_Liang1;~Ziwei_Wang2;~Jiwen_Lu1;~Haibin_Yan1",
        "aff": "Beijing University of Posts and Telecommunications;Tsinghua University+Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University;Nanyang Technological University;Tsinghua University;Beijing University of Posts and Telecommunications",
        "aff_domain": "bupt.edu.cn;mails.tsinghua.edu.cn+mails.tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;ntu.edu.sg;tsinghua.edu.cn;bupt.edu.cn",
        "position": "PhD student;PhD student+Undergrad student;PhD student;PhD student;PhD student;Assistant Professor;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nwu2025moto,\ntitle={MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation},\nauthor={Zhenyu Wu and Angyuan Ma and Xiuwei Xu and Hang Yin and Yinan Liang and Ziwei Wang and Jiwen Lu and Haibin Yan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Th1kFSnjUW}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Th1kFSnjUW",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1+1;1;1;1;2;1;0",
        "aff_unique_norm": "Beijing University of Posts and Telecommunications;Tsinghua University;Nanyang Technological University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.bupt.edu.cn/;https://www.tsinghua.edu.cn;https://www.ntu.edu.sg",
        "aff_unique_abbr": "BUPT;THU;NTU",
        "aff_campus_unique_index": "0;;0",
        "aff_campus_unique": "Beijing;",
        "aff_country_unique_index": "0;0+0;0;0;0;1;0;0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "Tl7girqoLi",
        "title": "DemoSpeedup: Accelerating Visuomotor Policies via Entropy-Guided Demonstration Acceleration",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Imitation learning has shown great promise in robotic manipulation, but the policy\u2019s execution is often unsatisfactorily slow due to commonly tardy demonstrations collected by human operators. In this work, we present DemoSpeedup, a self-supervised method to accelerate visuomotor policy execution via entropy-guided demonstration acceleration. DemoSpeedup starts from training an arbitrary generative policy (e.g., ACT or Diffusion Policy) on normal-speed demonstrations, which serves as a per-frame action entropy estimator. The key insight is that frames with lower action entropy estimates call for more consistent policy behaviors, which often indicate the demands for higher-precision operations. In contrast, frames with higher entropy estimates correspond to more casual sections, and therefore can be more safely accelerated. Thus, we segment the original demonstrations according to the estimated entropy, and accelerate them by down-sampling at rates that increase with the entropy values. Trained with the speedup demonstrations, the resulting policies execute up to 3 times faster while maintaining the task completion performance. Interestingly, these policies could even achieve higher success rates than those trained with normal-speed demonstrations, due to the benefits of reduced decision-making horizons.",
        "keywords": "Imitation learning;Manipulation;Demonstration Acceleration",
        "primary_area": "",
        "supplementary_material": "/attachment/7b140f63582349fcd24dcdef6566280f4ea650ea.zip",
        "author": "Lingxiao Guo;Zhengrong Xue;Zijing Xu;Huazhe Xu",
        "authorids": "~Lingxiao_Guo1;~Zhengrong_Xue1;~Zijing_Xu1;~Huazhe_Xu1",
        "gender": "M;;F;M",
        "homepage": ";https://steven-xzr.github.io;;http://hxu.rocks",
        "dblp": ";272/4241;;164/9006",
        "google_scholar": "https://scholar.google.cz/citations?view_op=list_works;LO3pKmwAAAAJ;;t9HPFawAAAAJ",
        "orcid": ";;0009-0005-9611-0718;",
        "linkedin": ";;;",
        "or_profile": "~Lingxiao_Guo1;~Zhengrong_Xue1;~Zijing_Xu1;~Huazhe_Xu1",
        "aff": "Shanghai Jiaotong University;Tsinghua University;University of Electronic Science and Technology of China;Tsinghua University",
        "aff_domain": "sjtu.edu;tsinghua.edu.cn;uestc.edu.cn;tsinghua.edu.cn",
        "position": "Undergrad student;PhD student;Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\nguo2025demospeedup,\ntitle={DemoSpeedup: Accelerating Visuomotor Policies via Entropy-Guided Demonstration Acceleration},\nauthor={Lingxiao Guo and Zhengrong Xue and Zijing Xu and Huazhe Xu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Tl7girqoLi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Tl7girqoLi",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Shanghai Jiao Tong University;Tsinghua University;University of Electronic Science and Technology of China",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.sjtu.edu.cn;https://www.tsinghua.edu.cn;https://www.uestc.edu.cn",
        "aff_unique_abbr": "SJTU;THU;UESTC",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "TqevdDMqrK",
        "title": "CUPID: Curating Data your Robot Loves with Influence Functions",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In robot imitation learning, policy performance is tightly coupled with the quality and composition of the demonstration data. Yet, developing a precise understanding of how individual demonstrations contribute to downstream outcomes\u2014such as closed-loop task success or failure\u2014remains a persistent challenge. Inspired by the theory of influence functions, we propose CUPID. Given a set of evaluation rollouts, CUPID estimates the influence of a training demonstration on the policy\u2019s expected return. This enables ranking and selection of demonstrations according to their impact on the policy\u2019s closed-loop performance. We use our estimator to curate data by 1) filtering out training demonstrations that harmed the policy\u2019s performance and 2) subselecting newly collected trajectories that will most help improve the policy. Extensive simulated and hardware experiments show that our approach consistently identifies which data drives test-time performance. For example, training with less than 33% of curated data can result in state-of-the-art diffusion policies on the simulated Robomimic benchmark, and we observe similar improvements in hardware experiments. Furthermore, our hardware experiments show that our influence-based estimator can identify robust strategies under distribution shift, isolate spurious correlations, and even enhance post-training of generalist policies.",
        "keywords": "Imitation Learning;Data Curation;Influence Functions",
        "primary_area": "",
        "supplementary_material": "/attachment/1c20fa190e2867c4cf720e366fa896fad05612c1.zip",
        "author": "Christopher Agia;Rohan Sinha;Jingyun Yang;Rika Antonova;Marco Pavone;Haruki Nishimura;Masha Itkina;Jeannette Bohg",
        "authorids": "~Christopher_Agia1;~Rohan_Sinha1;~Jingyun_Yang1;~Rika_Antonova1;~Marco_Pavone1;~Haruki_Nishimura2;~Masha_Itkina1;~Jeannette_Bohg1",
        "gender": "M;;M;;M;;F;",
        "homepage": "https://www.chrisagia.com/;https://www.stanford.edu/;https://yjy0625.github.io;;https://web.stanford.edu/~pavone/;;https://mashaitkina.weebly.com/;https://web.stanford.edu/~bohg/",
        "dblp": "268/3555;;;;91/3382-1.html;;239/8541;52/7377",
        "google_scholar": "t8Em5FwAAAAJ;;7XBAa2QAAAAJ;;RhOpyXcAAAAJ;;https://scholar.google.ca/citations?user=JAmTk5gAAAAJ;rjnJnEkAAAAJ",
        "orcid": "0000-0002-1208-2539;;;;;;;0000-0002-4921-7193",
        "linkedin": "agiachris/;;;;;;masha-itkina-3bb11a97/;",
        "or_profile": "~Christopher_Agia1;~Rohan_Sinha1;~Jingyun_Yang1;~Rika_Antonova1;~Marco_Pavone1;~Haruki_Nishimura2;~Masha_Itkina1;~Jeannette_Bohg1",
        "aff": "Stanford University;Stanford University;Stanford University;;NVIDIA+Stanford University;;Toyota Research Institute;Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;;nvidia.com+stanford.edu;;tri.global;stanford.edu",
        "position": "PhD student;PhD student;PhD student;;Director, Autonomous Vehicle Research+Associate Professor;;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nagia2025cupid,\ntitle={{CUPID}: Curating Data your Robot Loves with Influence Functions},\nauthor={Christopher Agia and Rohan Sinha and Jingyun Yang and Rika Antonova and Marco Pavone and Haruki Nishimura and Masha Itkina and Jeannette Bohg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=TqevdDMqrK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=TqevdDMqrK",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1;2+0;1;3;0",
        "aff_unique_norm": "Stanford University;;NVIDIA;Toyota Research Institute",
        "aff_unique_dep": ";;NVIDIA Corporation;",
        "aff_unique_url": "https://www.stanford.edu;;https://www.nvidia.com;https://www.tri.global",
        "aff_unique_abbr": "Stanford;;NVIDIA;TRI",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0+0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "Tx54fkQ3Cq",
        "title": "Humanoid Policy ~ Human Policy",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Training manipulation policies for humanoid robots with diverse data enhances their robustness and generalization across tasks and platforms. However, learning solely from robot demonstrations is labor-intensive, requiring expensive tele-operated data collection,n which is difficult to scale. This paper investigates a more scalable data source, egocentric human demonstrations, to serve as cross-embodiment training data for robot learning. We mitigate the embodiment gap between humanoids and humans from both the data and modeling perspectives. We collect an egocentric task-oriented dataset that is directly aligned with humanoid manipulation demonstrations. We then train a human-humanoid behavior policy, which we term Human Action Transformer (HAT). The state-action space of HAT is unified for both humans and humanoid robots and can be differentiably retargeted to robot actions. Co-trained with smaller-scale robot data, HAT directly models humanoid robots and humans as different embodiments without additional supervision. We show that human data improves both the generalization and robustness of HAT with significantly better data collection efficiency.",
        "keywords": "Robot Manipulation;Humanoid;Learning from Human",
        "primary_area": "",
        "supplementary_material": "/attachment/ef243958f8beec42d56ce91d6fbed12829758fa9.zip",
        "author": "Ri-Zhao Qiu;Shiqi Yang;Xuxin Cheng;Chaitanya Chawla;Jialong Li;Tairan He;Ge Yan;David J. Yoon;Ryan Hoque;Lars Paulsen;Ge Yang;Jian Zhang;Sha Yi;Guanya Shi;Xiaolong Wang",
        "authorids": "~Ri-Zhao_Qiu1;~Shiqi_Yang2;~Xuxin_Cheng2;~Chaitanya_Chawla1;~Jialong_Li3;~Tairan_He1;~Ge_Yan3;~David_J._Yoon1;~Ryan_Hoque1;~Lars_Paulsen1;~Ge_Yang1;~Jian_Zhang23;~Sha_Yi1;~Guanya_Shi1;~Xiaolong_Wang3",
        "gender": "Not Specified;M;M;M;M;M;Not Specified;M;M;M;M;M;F;M;M",
        "homepage": "https://rogerqi.github.io/;https://aaronyang1223.github.io/;https://chengxuxin.github.io;https://chaitanya1chawla.github.io;https://rexskywalkerlee.github.io/;https://tairanhe.com;https://geyan21.github.io/;;https://ryanhoque.github.io;https://www.larspaulsenportfolio.com/;http://www.episodeyang.com;;https://yswhynot.github.io;http://guanyashi.github.io;https://xiaolonw.github.io/",
        "dblp": "336/5470;;;;;263/2891.html;169/8155-6;;250/9457;;48/4561-3;;;230/4386;91/952-4",
        "google_scholar": "uH0re54AAAAJ;OQQzJb4AAAAJ;Z8vhOxYAAAAJ;;;TVWH2U8AAAAJ;ma7qW2kAAAAJ;https://scholar.google.ca/citations?user=uoH44gEAAAAJ;ywv6tDUAAAAJ;;vaQcF6kAAAAJ;;;joR1Z4UAAAAJ;Y8O9N_0AAAAJ",
        "orcid": ";0009-0009-8529-4522;;;;;;;;;0000-0001-7520-7055;;;0000-0002-9075-3705;",
        "linkedin": "rizhaoqiu/;;;https://linkedin.com/chaitanya1chawla;jialong-li-737a561a8/;tairan-he-41a904294/;ge-yan/;;https://linkedin.com/in/ryanhoque;lars-paulsen-983ab5176/;;jianzhangpurdue/;;guanya-shi-b07b43126/;",
        "or_profile": "~Ri-Zhao_Qiu1;~Shiqi_Yang2;~Xuxin_Cheng2;~Chaitanya_Chawla1;~Jialong_Li3;~Tairan_He1;~Ge_Yan3;~David_J._Yoon1;~Ryan_Hoque1;~Lars_Paulsen1;~Ge_Yang1;~Jian_Zhang23;~Sha_Yi1;~Guanya_Shi1;~Xiaolong_Wang3",
        "aff": "University of California, San Diego;University of California, San Diego;University of California, San Diego;School of Computer Science, Carnegie Mellon University;University of California, San Diego;NVIDIA+Carnegie Mellon University;Department of Computer Science, University of Washington;Apple;Apple;University of California, San Diego;Massachusetts Institute of Technology;Apple;University of California, San Diego;Carnegie Mellon University;University of California, San Diego",
        "aff_domain": "ucsd.edu;ucsd.edu;ucsd.edu;cs.cmu.edu;ucsd.edu;nvidia.com+andrew.cmu.edu;cs.washington.edu;apple.com;apple.com;ucsd.edu;mit.edu;apple.com;ucsd.edu;andrew.cmu.edu;ucsd.edu",
        "position": "PhD student;MS student;PhD student;MS student;MS student;Intern+PhD student;PhD student;Researcher;Researcher;MS student;Postdoc;AIML;Postdoc;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nqiu2025humanoid,\ntitle={Humanoid Policy {\\textasciitilde} Human Policy},\nauthor={Ri-Zhao Qiu and Shiqi Yang and Xuxin Cheng and Chaitanya Chawla and Jialong Li and Tairan He and Ge Yan and David J. Yoon and Ryan Hoque and Lars Paulsen and Ge Yang and Jian Zhang and Sha Yi and Guanya Shi and Xiaolong Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Tx54fkQ3Cq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Tx54fkQ3Cq",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            15,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1;0;2+1;3;4;4;0;5;4;0;1;0",
        "aff_unique_norm": "University of California, San Diego;Carnegie Mellon University;NVIDIA;University of Washington;Apple;Massachusetts Institute of Technology",
        "aff_unique_dep": ";School of Computer Science;NVIDIA Corporation;Department of Computer Science;Apple Inc.;",
        "aff_unique_url": "https://www.ucsd.edu;https://www.cmu.edu;https://www.nvidia.com;https://www.washington.edu;https://www.apple.com;https://web.mit.edu",
        "aff_unique_abbr": "UCSD;CMU;NVIDIA;UW;Apple;MIT",
        "aff_campus_unique_index": "0;0;0;1;0;;3;0;0;0",
        "aff_campus_unique": "San Diego;Pittsburgh;;Seattle",
        "aff_country_unique_index": "0;0;0;0;0;0+0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "U9zcbQVDGa",
        "title": "Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Large language models (LLMs) are beginning to automate reward design for dexterous manipulation. However, no prior work has considered tactile sensing, which is known to be critical for human-like dexterity. We present Text2Touch, bringing LLM-crafted rewards to the challenging task of multi-axis in-hand object rotation with real-world vision based tactile sensing in palm-up and palm-down configurations. Our prompt engineering strategy scales to over 70 environment variables, and sim-to-real distillation enables successful policy transfer to a tactile-enabled fully actuated four-fingered dexterous robot hand. Text2Touch significantly outperforms a carefully tuned human-engineered baseline, demonstrating superior rotation speed and stability while relying on reward functions that are an order of magnitude shorter and simpler. These results illustrate how LLM-designed rewards can significantly reduce the time from concept to deployable dexterous tactile skills, supporting more rapid and scalable multimodal robot learning.",
        "keywords": "Tactile Sensing;Reinforcement Learning;Large Language Models",
        "primary_area": "",
        "supplementary_material": "/attachment/6cc0ee830e7ce48b2ffde13022477e3c9ee66dbf.zip",
        "author": "Harrison Field;Max Yang;Yijiong Lin;Efi Psomopoulou;David A.W. Barton;Nathan F. Lepora",
        "authorids": "~Harrison_Field1;~Max_Yang1;~Yijiong_Lin1;~Efi_Psomopoulou1;~David_A.W._Barton1;~Nathan_F._Lepora1",
        "gender": "M;M;;F;M;",
        "homepage": "https://github.com/hpfield;https://scholar.google.com/citations?user=WQQ1vz8AAAAJ&hl=en;;https://efi-robotics.com;https://cityinthesky.co.uk/;https://www.lepora.com",
        "dblp": ";;;;;76/10010",
        "google_scholar": "https://scholar.google.co.uk/citations?hl=en;;;https://scholar.google.gr/citations?user=kVyW-LEAAAAJ;;fb2WiJgAAAAJ",
        "orcid": ";;;0000-0003-3883-4097;;",
        "linkedin": "hpfield;;;efi-psomopoulou/;;",
        "or_profile": "~Harrison_Field1;~Max_Yang1;~Yijiong_Lin1;~Efi_Psomopoulou1;~David_A.W._Barton1;~Nathan_F._Lepora1",
        "aff": "University of Bristol;University of Bristol;;University of Bristol;University of Bristol;University of Bristol",
        "aff_domain": "bristol.ac.uk;bristol.ac.uk;;bristol.ac.uk;bristol.ac.uk;bristol.ac.uk",
        "position": "PhD student;PhD student;;Assistant Professor;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nfield2025texttouch,\ntitle={Text2Touch: Tactile In-Hand Manipulation with {LLM}-Designed Reward Functions},\nauthor={Harrison Field and Max Yang and Yijiong Lin and Efi Psomopoulou and David A.W. Barton and Nathan F. Lepora},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=U9zcbQVDGa}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=U9zcbQVDGa",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;0;0",
        "aff_unique_norm": "University of Bristol;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.bristol.ac.uk;",
        "aff_unique_abbr": "Bristol;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "UTPBM4dEUS",
        "title": "Sampling-based System Identification with Active Exploration for Legged Sim2Real Learning",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Sim-to-real discrepancies hinder learning-based policies from achieving high-precision tasks in the real world. While Domain Randomization (DR) is commonly used to bridge this gap, it often relies on heuristics and can lead to overly conservative policies with degrading performance when not properly tuned. System Identification (Sys-ID) offers a targeted approach, but standard techniques rely on differentiable dynamics and/or direct torque measurement, assumptions that rarely hold for contact-rich legged systems. To this end, we present SPI-Active (Sampling-based Parameter Identification with Active Exploration), a two-stage framework that estimates physical parameters of legged robots to minimize the sim-to-real gap. SPI-Active robustly identifies key physical parameters through massive parallel sampling, minimizing state prediction errors between simulated and real-world trajectories. To further improve the informativeness of collected data, we introduce an active exploration strategy that maximizes the Fisher Information of the collected real-world trajectories via optimizing the input commands of an exploration policy. This targeted exploration leads to accurate identification and better generalization across diverse tasks. Experimental results demonstrate that SPI-Active enables precise sim-to-real transfer of learned policies to the real world, outperforming baselines by 42-63% in various locomotion tasks. Videos at the anonymous website https://anonymous-spi-active.github.io/",
        "keywords": "System Identification;Sim2Real;Legged Robots",
        "primary_area": "",
        "supplementary_material": "/attachment/bbd53fb6c30e4cc4a64747f6d75f07da0cc9b379.zip",
        "author": "Nikhil Sobanbabu;Guanqi He;Tairan He;Yuxiang Yang;Guanya Shi",
        "authorids": "~Nikhil_Sobanbabu1;~Guanqi_He1;~Tairan_He1;~Yuxiang_Yang2;~Guanya_Shi1",
        "gender": "M;M;M;M;M",
        "homepage": "https://nike353.github.io/;https://guanqihe.github.io/;https://tairanhe.com;https://yxyang.github.io;http://guanyashi.github.io",
        "dblp": ";;263/2891.html;;230/4386",
        "google_scholar": ";DH04ikwAAAAJ;TVWH2U8AAAAJ;2NQKmzIAAAAJ;joR1Z4UAAAAJ",
        "orcid": ";;;;0000-0002-9075-3705",
        "linkedin": ";;tairan-he-41a904294/;;guanya-shi-b07b43126/",
        "or_profile": "~Nikhil_Sobanbabu1;~Guanqi_He1;~Tairan_He1;~Yuxiang_Yang2;~Guanya_Shi1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University;NVIDIA+Carnegie Mellon University;Department of Computer Science, u+Google;Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;cmu.edu;nvidia.com+andrew.cmu.edu;cs.washington.edu+google.com;andrew.cmu.edu",
        "position": "MS student;MS student;Intern+PhD student;PhD student+Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nsobanbabu2025samplingbased,\ntitle={Sampling-based System Identification with Active Exploration for Legged Sim2Real Learning},\nauthor={Nikhil Sobanbabu and Guanqi He and Tairan He and Yuxiang Yang and Guanya Shi},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=UTPBM4dEUS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=UTPBM4dEUS",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1+0;2+3;0",
        "aff_unique_norm": "Carnegie Mellon University;NVIDIA;Department of Computer Science, u;Google",
        "aff_unique_dep": ";NVIDIA Corporation;;Google",
        "aff_unique_url": "https://www.cmu.edu;https://www.nvidia.com;;https://www.google.com",
        "aff_unique_abbr": "CMU;NVIDIA;;Google",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Mountain View",
        "aff_country_unique_index": "0;0;0+0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "VVhAhzr2WV",
        "title": "Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm\u2014human teleoperation\u2014remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision-modeling turned off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations.",
        "keywords": "Robot Datasets;Imitation Learning;Data Augmentation",
        "primary_area": "",
        "supplementary_material": "/attachment/32e454e81b44b2dabb5cbc92dda4b26694b655ed.zip",
        "author": "Justin Yu;Letian Fu;Huang Huang;Karim El-Refai;Rares Andrei Ambrus;Richard Cheng;Muhammad Zubair Irshad;Ken Goldberg",
        "authorids": "~Justin_Yu3;~Letian_Fu1;~Huang_Huang1;~Karim_El-Refai1;~Rares_Andrei_Ambrus1;~Richard_Cheng1;~Muhammad_Zubair_Irshad1;~Ken_Goldberg1",
        "gender": "M;M;;;M;;M;M",
        "homepage": "https://uynitsuj.github.io/;https://max-fu.github.io/;https://sites.google.com/site/huanghuang9729/home;;http://www.csc.kth.se/~raambrus/;;https://zubairirshad.com/;http://goldberg.berkeley.edu/",
        "dblp": "263/7533;;;;25/76;03/5484;290/8882;g/KennethYGoldberg",
        "google_scholar": "m5OJ4kgAAAAJ;aWot7UgAAAAJ;;OC3OLioAAAAJ;2xjjS3oAAAAJ;d_Fpj0oAAAAJ;https://scholar.google.com/citations?hl=en;https://scholar.google.com.tw/citations?user=8fztli4AAAAJ",
        "orcid": ";;;;0000-0002-3111-3812;;0000-0002-1955-6194;0000-0001-6747-9499",
        "linkedin": "yu-justin/;;;;rare%C8%99-ambru%C8%99-b04812125/;;zubair-irshad/;goldbergken/",
        "or_profile": "~Justin_Yu3;~Letian_Fu1;~Huang_Huang1;~Karim_El-Refai1;~Rares_Andrei_Ambrus1;~Richard_Cheng1;~Muhammad_Zubair_Irshad1;~Ken_Goldberg1",
        "aff": "University of California, Berkeley;University of California, Berkeley;Stanford University+Meta Facebook+University of California, Berkeley;University of California, Berkeley;Toyota Research Institute;Toyota Research Institute;Toyota Research Institute;University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;stanford.edu+meta.com+berkeley.edu;berkeley.edu;tri.global;tri.global;tri.global;berkeley.edu",
        "position": "PhD student;PhD student;Postdoc+Postdoc+PhD student;Undergrad student;Researcher;Researcher;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nyu2025realrenderreal,\ntitle={Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware},\nauthor={Justin Yu and Letian Fu and Huang Huang and Karim El-Refai and Rares Andrei Ambrus and Richard Cheng and Muhammad Zubair Irshad and Ken Goldberg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=VVhAhzr2WV}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VVhAhzr2WV",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1+2+0;0;3;3;3;0",
        "aff_unique_norm": "University of California, Berkeley;Stanford University;Meta;Toyota Research Institute",
        "aff_unique_dep": ";;Meta Platforms, Inc.;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.stanford.edu;https://meta.com;https://www.tri.global",
        "aff_unique_abbr": "UC Berkeley;Stanford;Meta;TRI",
        "aff_campus_unique_index": "0;0;1+0;0;0",
        "aff_campus_unique": "Berkeley;Stanford;",
        "aff_country_unique_index": "0;0;0+0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "VmCkEvRULX",
        "title": "Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-top Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Robotic manipulation in unstructured environments requires systems that can generalize across diverse tasks while maintaining robust and reliable performance. We introduce GVF-TAPE, a closed-loop framework that combines generative visual foresight with task-agnostic pose estimation to enable scalable robotic manipulation. GVF-TAPE employs a generative video model to predict future RGB-D frames from a single RGB side-view image and a task description, offering visual plans that guide robot actions. A decoupled pose estimation model then extracts end-effector poses from the predicted frames, translating them into executable commands via low-level controllers. By iteratively integrating video foresight and pose estimation in a closed loop, GVF-TAPE achieves real-time, adaptive manipulation across a broad range of tasks. Extensive experiments in both simulation and real-world settings demonstrate that our approach reduces reliance on task-specific action data and generalizes effectively, providing a practical and scalable solution for intelligent robotic systems",
        "keywords": "robotic manipulation;action-label-free learning;generative visual foresight",
        "primary_area": "",
        "supplementary_material": "/attachment/1d03cec94bbf1a9c877c867f20444494d9c648e0.zip",
        "author": "Chuye Zhang;Xiaoxiong Zhang;Linfang Zheng;Wei Pan;Wei Zhang",
        "authorids": "~Chuye_Zhang1;~Xiaoxiong_Zhang1;~Linfang_Zheng1;~Wei_Pan12;~Wei_Zhang40",
        "gender": "M;M;F;M;M",
        "homepage": "https://zhangchuye.github.io/;https://github.com/Xiaoxiongzzzz;;https://weisonweileen.github.io/#/;https://www.wzhanglab.site/",
        "dblp": "346/0027;;271/6388;;",
        "google_scholar": "QPXHWn0AAAAJ;;https://scholar.google.com/citations?hl=zh-CN;;HQ6j-KsAAAAJ",
        "orcid": "0009-0006-9203-4161;;;;",
        "linkedin": ";;;;",
        "or_profile": "~Chuye_Zhang1;~Xiaoxiong_Zhang1;~Linfang_Zheng1;~Wei_Pan12;~Wei_Zhang40",
        "aff": "Southern University of Science and Technology;Southern University of Science and Technology;University of Hong Kong;Southern University of Science and Technology;Southern University of Science and Technology of China",
        "aff_domain": "sustech.edu.cn;mail.sustech.edu.cn;hku.hk;sustech.edu.cn;sustech.edu.cn",
        "position": "Undergrad student;MS student;Postdoc;Undergrad student;Professor",
        "bibtex": "@inproceedings{\nzhang2025generative,\ntitle={Generative Visual Foresight Meets Task-Agnostic Pose Estimation in Robotic Table-top Manipulation},\nauthor={Chuye Zhang and Xiaoxiong Zhang and Linfang Zheng and Wei Pan and Wei Zhang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=VmCkEvRULX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VmCkEvRULX",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Southern University of Science and Technology;University of Hong Kong",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sustech.edu.cn;https://www.hku.hk",
        "aff_unique_abbr": "SUSTech;HKU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "Vo1tL9dhpk",
        "title": "Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Terrain elevation modeling for off-road navigation aims to accurately estimate changes in terrain geometry in real-time and quantify the corresponding uncertainties. Having precise estimations and uncertainties plays a crucial role in planning and control algorithms to explore safe and reliable maneuver strategies. However, existing approaches, such as Gaussian Processes (GPs) and neural network-based methods, often fail to meet these needs. They are either unable to perform in real-time due to high computational demands, underestimating sharp geometry changes, or harming elevation accuracy when learned with uncertainties. Recently, Neural Processes (NPs) have emerged as a promising approach that integrates the Bayesian uncertainty estimation of GPs with the efficiency and flexibility of neural networks. Inspired by NPs, we propose an effective NP-based method that precisely estimates sharp elevation changes and quantifies the corresponding predictive uncertainty without losing elevation accuracy. Our method leverages semantic features from LiDAR and camera sensors to improve interpolation and extrapolation accuracy in unobserved regions. Also, we introduce a local ball-query attention mechanism to effectively reduce the computational complexity of global attention by 17\\% while preserving crucial local and spatial information. We evaluate our method on off-road datasets having interesting geometric features, collected from trails, deserts, and hills. Our results demonstrate superior performance over baselines and showcase the potential of neural processes for effective and expressive terrain modeling in complex off-road environments.",
        "keywords": "Robot perception;Terrain modeling;Neural processes",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sanghun Jung;Daehoon Gwak;Byron Boots;James Hays",
        "authorids": "~Sanghun_Jung1;~Daehoon_Gwak1;~Byron_Boots1;~James_Hays1",
        "gender": "M;M;;M",
        "homepage": "https://shjung13.github.io/;;;http://www.cc.gatech.edu/~hays/",
        "dblp": "246/1662;276/7016;;57/5958",
        "google_scholar": "e7X7O8gAAAAJ;NyQ42l8AAAAJ;;vjZrDKQAAAAJ",
        "orcid": ";;;0000-0001-7016-4252",
        "linkedin": ";;;james-h-hays/",
        "or_profile": "~Sanghun_Jung1;~Daehoon_Gwak1;~Byron_Boots1;~James_Hays1",
        "aff": "University of Washington;Korea Advanced Institute of Science & Technology;;Overland AI+Georgia Institute of Technology",
        "aff_domain": "cs.washington.edu;kaist.ac.kr;;overland.ai+gatech.edu",
        "position": "PhD student;PhD student;;Principal Researcher+Associate professor",
        "bibtex": "@inproceedings{\njung2025uncertaintyaware,\ntitle={Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes},\nauthor={Sanghun Jung and Daehoon Gwak and Byron Boots and James Hays},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Vo1tL9dhpk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Vo1tL9dhpk",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3+4",
        "aff_unique_norm": "University of Washington;Korea Advanced Institute of Science and Technology;;Overland AI;Georgia Institute of Technology",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.washington.edu;https://www.kaist.ac.kr;;https://www.overland.ai;https://www.gatech.edu",
        "aff_unique_abbr": "UW;KAIST;;Overland AI;Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0+0",
        "aff_country_unique": "United States;South Korea;"
    },
    {
        "id": "VqmAvBkFhw",
        "title": "LocoFormer: Generalist Locomotion via Long-context Adaptation",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Humans and animals exhibit flexible locomotion strategies, such as learning to walk within minutes, and efficient adaptation to changes in morphology. In contrast, modern locomotion controllers are manually tuned for specific embodiments. In this paper, we present LocoFormer, a generalist policy that can control previously unseen legged and wheeled robots, even without precise knowledge of their kinematics. LocoFormer is able to adapt to changes in morphology and dynamics at test time. We find that two key choices enable adaptation. First, we train massive scale RL on procedurally generated robots with aggressive domain randomization. Second, in contrast to previous policies that are myopic with short context lengths, we extend context by orders of magnitude to span episode boundaries. We deploy the same LocoFormer to varied robots, and show robust control even with large disturbances such as weight and motor failures. In extreme scenarios, we see emergent adaptation across episodes, LocoFormer learns from falls in early episodes to improve control strategies in later ones. We believe this simple yet general recipe can be used to train foundation models for other robotic skills in the future. Videos at generalist-locomotion.github.io.",
        "keywords": "Cross-Embodied Learning;Legged Locomotion;Online Adaptation",
        "primary_area": "",
        "supplementary_material": "/attachment/c5df75e80ceaa18ae6987d97d6d552077738c88a.zip",
        "author": "Min Liu;Deepak Pathak;Ananye Agarwal",
        "authorids": "~Min_Liu7;~Deepak_Pathak1;~Ananye_Agarwal1",
        "gender": "M;M;M",
        "homepage": "https://minliu01.github.io/;https://www.cs.cmu.edu/~dpathak/;https://anag.me/",
        "dblp": ";155/9860;294/4812",
        "google_scholar": "59D29nsAAAAJ;https://scholar.google.cl/citations?user=AEsPCAUAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;",
        "linkedin": "min-liu-16164321b/;pathak22/;",
        "or_profile": "~Min_Liu7;~Deepak_Pathak1;~Ananye_Agarwal1",
        "aff": "Skild AI;Skild AI+Carnegie Mellon University;Carnegie Mellon University",
        "aff_domain": "skild.ai;skild.ai+cmu.edu;cmu.edu",
        "position": "Member of Technical Staff;Founder+Assistant Professor;PhD student",
        "bibtex": "@inproceedings{\nliu2025locoformer,\ntitle={LocoFormer: Generalist Locomotion via Long-context Adaptation},\nauthor={Min Liu and Deepak Pathak and Ananye Agarwal},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=VqmAvBkFhw}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VqmAvBkFhw",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0+1;1",
        "aff_unique_norm": "Skild AI;Carnegie Mellon University",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.cmu.edu",
        "aff_unique_abbr": ";CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "VrNSv02Xfu",
        "title": "SAVOR: Skill Affordance Learning from Visuo-Haptic Perception for Robot-Assisted Bite Acquisition",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Robot-assisted feeding requires reliable bite acquisition, a challenging task due to the complex interactions between utensils and food with diverse physical properties. These interactions are further complicated by the temporal variability of food properties\u2014for example, steak becomes firm as it cools even during a meal. To address this, we propose SAVOR, a novel approach for learning skill affordances for bite acquisition\u2014how suitable a manipulation skill (e.g., skewering, scooping) is for a given utensil-food interaction. In our formulation, skill affordances arise from the combination of tool affordances (what a utensil can do) and food affordances (what the food allows). Tool affordances are learned offline through calibration, where different utensils interact with a variety of foods to model their functional capabilities. Food affordances are characterized by physical properties such as softness, moisture, and viscosity, initially inferred through commonsense reasoning using a visually-conditioned language model and then dynamically refined through online multi-modal visuo-haptic perception using SAVOR-Net during interaction. Our method integrates these offline and online estimates to predict skill affordances in real time, enabling the robot to select the most appropriate skill for each food item. Evaluated on 20 single-item foods and 10 in-the-wild meals, our approach improves bite acquisition success by 13\\% over state-of-the-art (SOTA) category-based methods (e.g. use skewer for fruits). These results highlight the importance of modeling interaction-driven skill affordances for generalizable and effective robot-assisted bite acquisition.",
        "keywords": "Assistive Robotics;Visuo-Haptic Perception;Affordance Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/18238e74d9e975cc86eaade7598fd624d02610c6.zip",
        "author": "Zhanxin Wu;Bo Ai;Tom Silver;Tapomayukh Bhattacharjee",
        "authorids": "~Zhanxin_Wu1;~Bo_Ai1;~Tom_Silver1;~Tapomayukh_Bhattacharjee1",
        "gender": "F;M;M;M",
        "homepage": "https://zhanxinwu.com/;https://albertboai.com/;https://web.mit.edu/tslvr/www/;http://www.tapomayukh.com",
        "dblp": ";;202/1778;74/8368",
        "google_scholar": "e5ymbE8AAAAJ;KlE77HAAAAAJ;CMcsygMAAAAJ;X1zsXTgAAAAJ",
        "orcid": ";0000-0002-9009-3823;;0000-0001-9457-5726",
        "linkedin": ";bo-ai;;tapomayukh",
        "or_profile": "~Zhanxin_Wu1;~Bo_Ai1;~Tom_Silver1;~Tapomayukh_Bhattacharjee1",
        "aff": "Cornell University;University of California, San Diego;Princeton University+Cornell University;Cornell University",
        "aff_domain": "cornell.edu;ucsd.edu;princeton.edu+cornell.edu;cornell.edu",
        "position": "PhD student;PhD student;Assistant Professor+Postdoc;Assistant Professor",
        "bibtex": "@inproceedings{\nwu2025savor,\ntitle={{SAVOR}: Skill Affordance Learning from Visuo-Haptic Perception for Robot-Assisted Bite Acquisition},\nauthor={Zhanxin Wu and Bo Ai and Tom Silver and Tapomayukh Bhattacharjee},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=VrNSv02Xfu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=VrNSv02Xfu",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2+0;0",
        "aff_unique_norm": "Cornell University;University of California, San Diego;Princeton University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cornell.edu;https://www.ucsd.edu;https://www.princeton.edu",
        "aff_unique_abbr": "Cornell;UCSD;Princeton",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "XF69ltYlMU",
        "title": "Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Multi-object rearrangement is a challenging task that requires robots to reason about a physical 3D scene and the effects of a sequence of actions.\nWhile traditional task planning methods are shown to be effective for long-horizon manipulation, they require discretizing the continuous state and action space into symbolic descriptions of objects, object relationships, and actions.\nOur proposed method is instead able to take in a partially-observed point cloud observation of an initial scene and plan to a goal-satisfying configuration, without needing to discretize the set of actions or object relationships.\nTo enable this, we formulate the planning problem as an A* search over the space of possible point cloud rearrangements.\nWe sample point cloud transformations from a learned, domain-specific prior and then search for a sequence of such point cloud transformations that leads from the initial state to a goal.\nWe evaluate our method in terms of task planning success and task execution success on a real-world, multi-step table bussing environment and a simulation block stacking environment.\nWe experimentally demonstrate that our method produces successful plans and outperforms a policy-learning approach; we also perform ablations that show the importance of search in our approach.",
        "keywords": "Robot Learning;Robot Planning",
        "primary_area": "",
        "supplementary_material": "/attachment/6529685cafe6a39918edf7eca15ffbed7cfb8972.zip",
        "author": "Kallol Saha;Amber Li;Angela Rodriguez-Izquierdo;Lifan Yu;Ben Eisner;Maxim Likhachev;David Held",
        "authorids": "~Kallol_Saha1;~Amber_Li1;~Angela_Rodriguez-Izquierdo2;~Lifan_Yu1;~Ben_Eisner1;~Maxim_Likhachev1;~David_Held1",
        "gender": "M;F;F;F;M;M;M",
        "homepage": "https://kallol-saha.github.io/;https://amburger66.github.io/;;https://github.com/SilvesterYu;;https://www.cs.cmu.edu/~maxim/;http://davheld.github.io/",
        "dblp": ";;;;;;22/11147",
        "google_scholar": "D7kmZsYAAAAJ;;;;RWe-v0UAAAAJ;Ic8NqXwAAAAJ;0QtU-NsAAAAJ",
        "orcid": ";;;;;;",
        "linkedin": "kallolsaha/;amli/;angela-rodriguez-izq/;;;;",
        "or_profile": "~Kallol_Saha1;~Amber_Li1;~Angela_Rodriguez-Izquierdo2;~Lifan_Yu1;~Ben_Eisner1;~Maxim_Likhachev1;~David_Held1",
        "aff": "Carnegie Mellon University;Carnegie Mellon University;;;Carnegie Mellon University;;Carnegie Mellon University",
        "aff_domain": "andrew.cmu.edu;cmu.edu;;;cmu.edu;;cmu.edu",
        "position": "MS student;PhD student;;;PhD student;;Associate Professor",
        "bibtex": "@inproceedings{\nsaha2025planning,\ntitle={Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement},\nauthor={Kallol Saha and Amber Li and Angela Rodriguez-Izquierdo and Lifan Yu and Ben Eisner and Maxim Likhachev and David Held},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=XF69ltYlMU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XF69ltYlMU",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;1;0;1;0",
        "aff_unique_norm": "Carnegie Mellon University;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.cmu.edu;",
        "aff_unique_abbr": "CMU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "XaJkbK02Vm",
        "title": "LocoTouch: Learning Dynamic Quadrupedal Transport with Tactile Sensing",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Quadrupedal robots have demonstrated remarkable agility and robustness in traversing complex terrains. However, they struggle with dynamic object interactions, where contact must be precisely sensed and controlled. To bridge this gap, we present LocoTouch, a system that equips quadrupedal robots with tactile sensing to address a particularly challenging task in this category: long-distance transport of unsecured cylindrical objects, which typically requires custom mounting or fastening mechanisms to maintain stability. For efficient large-area tactile sensing, we design a high-density distributed tactile sensor that covers the entire back of the robot. To effectively leverage tactile feedback for robot control, we develop a simulation environment with high-fidelity tactile signals, and train tactile-aware transport policies using a two-stage learning pipeline. Furthermore, we design a novel reward function to promote robust, symmetric, and frequency-adaptive locomotion gaits. After training in simulation, LocoTouch transfers zero-shot to the real world, reliably transporting a wide range of unsecured cylindrical objects with diverse sizes, weights, and surface properties. Moreover, it remains robust over long distances, on uneven terrain, and under severe perturbations.",
        "keywords": "Tactile Quadrupedal Policy;Tactile Sim-to-Real;Legged Robots",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Changyi Lin;Yuxin Ray Song;Boda Huo;Mingyang Yu;Yikai Wang;Shiqi Liu;Yuxiang Yang;Wenhao Yu;Tingnan Zhang;Jie Tan;Yiyue Luo;Ding Zhao",
        "authorids": "~Changyi_Lin1;~Yuxin_Ray_Song1;~Boda_Huo1;~Mingyang_Yu4;~Yikai_Wang4;~Shiqi_Liu2;~Yuxiang_Yang2;~Wenhao_Yu1;~Tingnan_Zhang1;~Jie_Tan1;~Yiyue_Luo1;~Ding_Zhao1",
        "gender": "M;;M;M;M;M;M;M;M;M;F;",
        "homepage": "https://linchangyi1.github.io;https://rayxsong.com/;;;;https://shiqiliu-67.github.io/;https://yxyang.github.io;https://wenhaoyu.weebly.com/;;http://www.jie-tan.net;https://yyueluo.com/;https://safeai-lab.github.io",
        "dblp": ";;;;;;;;https://dblp.uni-trier.de/pers/hd/z/Zhang:Tingnan;81/7419;;",
        "google_scholar": "zKMxtSIAAAAJ;;;;;PiuAi5wAAAAJ;2NQKmzIAAAAJ;1bF2s2kAAAAJ;RM2vMNcAAAAJ;neGbgzYAAAAJ;;z7tPc9IAAAAJ",
        "orcid": ";;0009-0006-7798-5102;;;;;;;;;",
        "linkedin": ";rayx/;boda-huo-a795781a1/;mingyang-yu-a6498a244/;yikai-wang-a68094269/;shiqiliu2/;;;;jie-tan/;;",
        "or_profile": "~Changyi_Lin1;~Yuxin_Ray_Song1;~Boda_Huo1;~Mingyang_Yu4;~Yikai_Wang4;~Shiqi_Liu2;~Yuxiang_Yang2;~Wenhao_Yu1;~Tingnan_Zhang1;~Jie_Tan1;~Yiyue_Luo1;~Ding_Zhao1",
        "aff": "Carnegie Mellon University;Massachusetts Institute of Technology+Department of Computer Science, University of Washington;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Department of Computer Science, u+Google;Google;Google;Google;University of Washington;Carnegie Mellon University",
        "aff_domain": "cmu.edu;mit.edu+cs.washington.edu;andrew.cmu.edu;andrew.cmu.edu;andrew.cmu.edu;cmu.edu;cs.washington.edu+google.com;google.com;google.com;google.com;uw.edu;cmu.edu",
        "position": "PhD student;MS student+Undergrad student;MS student;MS student;PhD student;PhD student;PhD student+Researcher;Software Engineer;Software Engineer;Research Scientist;Principal Researcher;Associate Professor",
        "bibtex": "@inproceedings{\nlin2025locotouch,\ntitle={LocoTouch: Learning Dynamic Quadrupedal Transport with Tactile Sensing},\nauthor={Changyi Lin and Yuxin Ray Song and Boda Huo and Mingyang Yu and Yikai Wang and Shiqi Liu and Yuxiang Yang and Wenhao Yu and Tingnan Zhang and Jie Tan and Yiyue Luo and Ding Zhao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=XaJkbK02Vm}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XaJkbK02Vm",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            12,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1+2;0;0;0;0;3+4;4;4;4;2;0",
        "aff_unique_norm": "Carnegie Mellon University;Massachusetts Institute of Technology;University of Washington;Department of Computer Science, u;Google",
        "aff_unique_dep": ";;Department of Computer Science;;Google",
        "aff_unique_url": "https://www.cmu.edu;https://web.mit.edu;https://www.washington.edu;;https://www.google.com",
        "aff_unique_abbr": "CMU;MIT;UW;;Google",
        "aff_campus_unique_index": "1;2;2;2;2",
        "aff_campus_unique": ";Seattle;Mountain View",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "XjjXLxfPou",
        "title": "ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "We introduce ReWiND, a framework for learning robot manipulation tasks solely from language instructions without per-task demonstrations. Standard reinforcement learning (RL) and imitation learning methods require expert supervision through human-designed reward functions or demonstrations for every new task. In contrast, ReWiND starts from a small demonstration dataset to learn: (1) a data-efficient, language-conditioned reward function that labels the dataset with rewards, and (2) a language-conditioned policy pre-trained with offline RL using these rewards. Given an unseen task variation, ReWiND fine-tunes the pre-trained policy using the learned reward function, requiring minimal online interaction. We show that ReWiND\u2019s reward model generalizes effectively to unseen tasks, outperforming baselines by up to 2.4X in reward generalization and policy alignment metrics. Finally, we demonstrate that ReWiND enables sample-efficient adaptation to new tasks in both simulation and on a real bimanual manipulation platform, taking a step towards scalable, real-world robot learning.",
        "keywords": "Reinforcement Learning;Offline Reinforcement Learning;Reward Learning;Reward Modeling;Language",
        "primary_area": "",
        "supplementary_material": "/attachment/9dfdfda537a62c298df923f1db61392c5a21e473.zip",
        "author": "Jiahui Zhang;Yusen Luo;Abrar Anwar;Sumedh Anand Sontakke;Joseph J Lim;Jesse Thomason;Erdem Biyik;Jesse Zhang",
        "authorids": "~Jiahui_Zhang5;~Yusen_Luo1;~Abrar_Anwar1;~Sumedh_Anand_Sontakke1;~Joseph_J_Lim1;~Jesse_Thomason1;~Erdem_Biyik1;~Jesse_Zhang3",
        "gender": ";M;M;M;M;M;M;M",
        "homepage": "https://jiahui-3205.github.io/;https://github.com/AndreasL9z;http://abraranwar.github.io/;https://sumedh7.github.io/;http://people.csail.mit.edu/lim/;https://jessethomason.com/;http://people.eecs.berkeley.edu/~ebiyik/;https://jessezhang.net",
        "dblp": ";;294/1347.html;276/0127;08/3086;130/2863;194/2736;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;c6E-5tcAAAAJ;https://scholar.google.com/citations?hl=en;jTnQTBoAAAAJ;8BeTDr0AAAAJ;https://scholar.google.com.tr/citations?user=P-G3sjYAAAAJ;fSXCOfEAAAAJ",
        "orcid": ";;0000-0003-4442-4369;;;0000-0001-9199-0633;0000-0002-9516-3130;",
        "linkedin": "jiahui-zhang-2269451a3/;;abraranwar;sumedh-sontakke-0ab24210a/;;jesse-thomason-034746171/;https://linkedin.com/in/ebiyik;",
        "or_profile": "~Jiahui_Zhang5;~Yusen_Luo1;~Abrar_Anwar1;~Sumedh_Anand_Sontakke1;~Joseph_J_Lim1;~Jesse_Thomason1;~Erdem_Biyik1;~Jesse_Zhang3",
        "aff": "University of Southern California;University of Southern California;NVIDIA+University of Southern California;Amazon;Korea Advanced Institute of Science & Technology;University of Southern California+Amazon;University of Southern California;University of Washington+NVIDIA+University of Southern California",
        "aff_domain": "usc.edu;usc.edu;nvidia.com+usc.edu;amazon.com;kaist.ac.kr;usc.edu+amazon.com;usc.edu;uw.edu+nvidia.com+usc.edu",
        "position": "Researcher;MS student;Intern+PhD student;Researcher;Associate Professor;Assistant Professor+Visiting Academic;Assistant Professor;Postdoc+Intern+PhD student",
        "bibtex": "@inproceedings{\nzhang2025rewind,\ntitle={ReWi{ND}: Language-Guided Rewards Teach Robot Policies without New Demonstrations},\nauthor={Jiahui Zhang and Yusen Luo and Abrar Anwar and Sumedh Anand Sontakke and Joseph J Lim and Jesse Thomason and Erdem Biyik and Jesse Zhang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=XjjXLxfPou}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XjjXLxfPou",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1+0;2;3;0+2;0;4+1+0",
        "aff_unique_norm": "University of Southern California;NVIDIA;Amazon;Korea Advanced Institute of Science and Technology;University of Washington",
        "aff_unique_dep": ";NVIDIA Corporation;Amazon.com, Inc.;;",
        "aff_unique_url": "https://www.usc.edu;https://www.nvidia.com;https://www.amazon.com;https://www.kaist.ac.kr;https://www.washington.edu",
        "aff_unique_abbr": "USC;NVIDIA;Amazon;KAIST;UW",
        "aff_campus_unique_index": "0;0;0;0;0;0",
        "aff_campus_unique": "Los Angeles;",
        "aff_country_unique_index": "0;0;0+0;0;1;0+0;0;0+0+0",
        "aff_country_unique": "United States;South Korea"
    },
    {
        "id": "XoRtWWjXuC",
        "title": "AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We introduce AgentWorld, an interactive simulation platform for developing household mobile manipulation capabilities. Our platform combines automated scene construction that encompasses layout generation, semantic asset placement, visual material configuration, and physics simulation, with a dual-mode teleoperation system supporting both wheeled bases and humanoid locomotion policies for data collection. The resulting AgentWorld Dataset captures diverse tasks ranging from primitive actions (pick-and-place, push-pull, etc.) to multistage activities (serve drinks, heat up food, etc.) across living rooms, bedrooms, and kitchens. Through extensive benchmarking of imitation learning methods including behavior cloning, action chunking transformers, diffusion policies, and vision-language-action models, we demonstrate the dataset's effectiveness for sim-to-real transfer. The integrated system provides a comprehensive solution for scalable robotic skill acquisition in complex home environments, bridging the gap between simulation-based training and real-world deployment.",
        "keywords": "Simulation Platform;scene construction;teleoperation;simulation dataset",
        "primary_area": "",
        "supplementary_material": "/attachment/e726c3337f8b0aad329260a46cb66ab6394979cf.zip",
        "author": "Yizheng Zhang;Zhenjun Yu;JiaXin Lai;Cewu Lu;Lei Han",
        "authorids": "~Yizheng_Zhang1;~Zhenjun_Yu1;~JiaXin_Lai1;~Cewu_Lu3;~Lei_Han1",
        "gender": "M;M;;M;M",
        "homepage": ";;https://github.com/JayceLai;https://www.mvig.org/;https://www.leihan.org",
        "dblp": ";;;;75/2307-1",
        "google_scholar": ";;;https://scholar.google.com.tw/citations?user=QZVQEWAAAAAJ;Tz4_zi8AAAAJ",
        "orcid": "0000-0002-0488-9869;;;;",
        "linkedin": ";https://www.linkedin.cn/incareer/in/ACoAADkd-OQBIZghy47hQafZehw_kvmRaWC-RvY;;;",
        "or_profile": "~Yizheng_Zhang1;~Zhenjun_Yu1;~JiaXin_Lai1;~Cewu_Lu3;~Lei_Han1",
        "aff": "Tencent Robotics X;Shanghai Jiaotong University;IEG;Shanghai Jiaotong University;Tencent Robotics X",
        "aff_domain": "tencent.com;sjtu.edu.cn;tencent.com;sjtu.edu.cn;tencent.com",
        "position": "Researcher;MS student;Researcher;Full Professor;Principal Researcher",
        "bibtex": "@inproceedings{\nzhang2025agentworld,\ntitle={AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation},\nauthor={Yizheng Zhang and Zhenjun Yu and JiaXin Lai and Cewu Lu and Lei Han},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=XoRtWWjXuC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XoRtWWjXuC",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;1;0",
        "aff_unique_norm": "Tencent;Shanghai Jiao Tong University;IEG",
        "aff_unique_dep": "Tencent Robotics X;;",
        "aff_unique_url": "https://www.tencent.com;https://www.sjtu.edu.cn;",
        "aff_unique_abbr": "Tencent Robotics X;SJTU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "XrgRvBklWu",
        "title": "DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "We present DexUMI - a data collection and policy learning framework that uses the human hand as the natural interface to transfer dexterous manipulation skills to various robot hands. DexUMI incorporates hardware and software adaptations to minimize the embodiment gap between the human hand and various robot hands. The hardware adaptation bridges the kinematics gap with a wearable hand exoskeleton. It allows direct haptic feedback in manipulation data collection and adapts human motion to feasible robot hand motion. Our software adaptation bridges the visual gap by replacing the human hand in video data with high-fidelity robot hand inpainting. We demonstrate DexUMI's capabilities through comprehensive real-world experiments on two different dexterous robot hand hardware platforms, achieving an average task success rate of 86\\%.",
        "keywords": "Dexterous Manipulation;Learning from Human;Imitation Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/351c22c96ac412c469b6d0c729d86c1785ce5583.zip",
        "author": "Mengda Xu;Han Zhang;Yifan Hou;Zhenjia Xu;Linxi Fan;Manuela Veloso;Shuran Song",
        "authorids": "~Mengda_Xu1;~Han_Zhang25;~Yifan_Hou2;~Zhenjia_Xu1;~Linxi_Fan2;~Manuela_Veloso1;~Shuran_Song3",
        "gender": "M;F;M;M;;F;F",
        "homepage": "https://mengdaxu.github.io/;https://doublehan07.github.io/;https://profiles.stanford.edu/yifan-hou;https://www.zhenjiaxu.com/;;https://www.cs.cmu.edu/~mmv/;https://shurans.github.io/",
        "dblp": ";;;238/0000;154/6778;v/ManuelaMVeloso;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;cjxKmzUAAAAJ;85D2bRgAAAAJ;QE8cLMEAAAAJ;sljtWIUAAAAJ;https://scholar.google.com.tw/citations?user=2FbkAzYAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;0000-0001-8446-941X;;;;",
        "linkedin": "mengda-xu-132b57135/;doublehan/;;;;;",
        "or_profile": "~Mengda_Xu1;~Han_Zhang25;~Yifan_Hou2;~Zhenjia_Xu1;~Linxi_Fan2;~Manuela_Veloso1;~Shuran_Song3",
        "aff": "Stanford University+Columbia University;Stanford University+Tsinghua University;Stanford University;NVIDIA;NVIDIA;School of Computer Science, Carnegie Mellon University;Stanford University",
        "aff_domain": "stanford.edu+columbia.edu;stanford.edu+tsinghua.edu.cn;stanford.edu;nvidia.com;nvidia.com;cs.cmu.edu;stanford.edu",
        "position": "PhD student+PhD student;PhD student+Intern;Postdoc;Researcher;Researcher;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nxu2025dexumi,\ntitle={Dex{UMI}: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation},\nauthor={Mengda Xu and Han Zhang and Yifan Hou and Zhenjia Xu and Linxi Fan and Manuela Veloso and Shuran Song},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=XrgRvBklWu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=XrgRvBklWu",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;0+2;0;3;3;4;0",
        "aff_unique_norm": "Stanford University;Columbia University;Tsinghua University;NVIDIA;Carnegie Mellon University",
        "aff_unique_dep": ";;;NVIDIA Corporation;School of Computer Science",
        "aff_unique_url": "https://www.stanford.edu;https://www.columbia.edu;https://www.tsinghua.edu.cn;https://www.nvidia.com;https://www.cmu.edu",
        "aff_unique_abbr": "Stanford;Columbia;THU;NVIDIA;CMU",
        "aff_campus_unique_index": "0;0;0;2;0",
        "aff_campus_unique": "Stanford;;Pittsburgh",
        "aff_country_unique_index": "0+0;0+1;0;0;0;0;0",
        "aff_country_unique": "United States;China"
    },
    {
        "id": "YvsUD8C9QS",
        "title": "Mechanistic Interpretability for Steering Vision-Language-Action Models",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Vision-Language-Action (VLA) models are a promising path to realizing generalist embodied agents that can quickly adapt to new tasks, modalities, and environments. However, methods for interpreting and steering VLAs fall far short of classical robotics pipelines, which are grounded in explicit models of kinematics, dynamics, and control. This lack of mechanistic insight is a central challenge for deploying learned policies in real-world robotics, where robustness and explainability are critical. Motivated by advances in mechanistic interpretability for large language models, we introduce the first framework for interpreting and steering VLAs via their internal representations, enabling direct intervention in model behavior at inference time. We project feedforward activations within transformer layers onto the token embedding basis, identifying sparse semantic directions - such as speed and direction - that are causally linked to action selection. Leveraging these findings, we introduce a general-purpose activation steering method that modulates behavior in real time, without fine-tuning, reward signals, or environment interaction. We evaluate this method on two recent open-source VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that interpretable components of embodied VLAs can be systematically harnessed for control---establishing a new paradigm for transparent and steerable foundation models in robotics.",
        "keywords": "Mechanistic Interpretability;Vision-Language-Action Models;Foundation Models for Robotics",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Bear H\u00e4on;Kaylene Caswell Stocking;Ian Chuang;Claire Tomlin",
        "authorids": "~Bear_H\u00e4on1;~Kaylene_Caswell_Stocking1;~Ian_Chuang1;~Claire_Tomlin1",
        "gender": "M;;M;",
        "homepage": "https://bear-haon.github.io;http://people.eecs.berkeley.edu/~kaylene/;https://ian-chuang.github.io/;",
        "dblp": ";247/4099;;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;nkXU7rIAAAAJ;;",
        "orcid": ";;0000-0002-1983-9848;",
        "linkedin": "bear-haon/;kstocking/;iantc104/;",
        "or_profile": "~Bear_H\u00e4on1;~Kaylene_Caswell_Stocking1;~Ian_Chuang1;~Claire_Tomlin1",
        "aff": "University of California, Berkeley;University of California, Berkeley;;",
        "aff_domain": "berkeley.edu;berkeley.edu;;",
        "position": "MS student;PhD student;;",
        "bibtex": "@inproceedings{\nhaon2025mechanistic,\ntitle={Mechanistic Interpretability for Steering Vision-Language-Action Models},\nauthor={Bear H{\\\"a}on and Kaylene Caswell Stocking and Ian Chuang and Claire Tomlin},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=YvsUD8C9QS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=YvsUD8C9QS",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;1",
        "aff_unique_norm": "University of California, Berkeley;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.berkeley.edu;",
        "aff_unique_abbr": "UC Berkeley;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "Yy9EVIajH5",
        "title": "GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In Embodied Question Answering (EQA), agents must explore and develop a semantic understanding of an unseen environment in order to answer a situated question with confidence. This remains a challenging problem in robotics, due to the difficulties in obtaining useful semantic representations, updating these representations online, and leveraging prior world knowledge for efficient exploration and planning. Aiming to address these limitations, we propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic scene graphs (3DSGs) and task relevant images as multi-modal memory for grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen environments. We employ a hierarchical planning approach that exploits the hierarchical nature of 3DSGs for structured planning and semantic-guided exploration. We evaluate GraphEQA in simulation on two benchmark datasets, HM-EQA and OpenEQA, and demonstrate that it outperforms key baselines by completing EQA tasks with higher success rates and fewer planning steps, and further demonstrate GraphEQA in two separate real world environments. Videos and code are available at https://grapheqa.github.io.",
        "keywords": "Embodied Question Answering;Vision Language Models;Robot Planning;Real-time 3D Scene Graphs;Guided Exploration",
        "primary_area": "",
        "supplementary_material": "/attachment/1b0b3fb7c4d7592d41e0d7224cf1c68c077ae535.zip",
        "author": "Saumya Saxena;Blake Buchanan;Chris Paxton;Peiqi Liu;Bingqing Chen;Narunas Vaskevicius;Luigi Palmieri;Jonathan Francis;Oliver Kroemer",
        "authorids": "~Saumya_Saxena1;~Blake_Buchanan1;~Chris_Paxton1;~Peiqi_Liu1;~Bingqing_Chen2;~Narunas_Vaskevicius2;~Luigi_Palmieri1;~Jonathan_Francis1;~Oliver_Kroemer1",
        "gender": "F;M;M;;F;M;M;;M",
        "homepage": "https://saumyasaxena.github.io;https://blakerbuchanan.github.io/;https://cpaxton.github.io/;https://peiqi-liu.github.io/;;;https://palmieri.github.io/;;https://www.ri.cmu.edu/ri-faculty/oliver-kroemer/",
        "dblp": ";263/4614;;;;60/3997;153/7592;;04/7743",
        "google_scholar": "zvtzoPgAAAAJ;KNkPHigAAAAJ;I1mOQpAAAAAJ;https://scholar.google.com/citations?hl=en;LYt_2MgAAAAJ;https://scholar.google.com/citations?hl=en;L-3aQNcAAAAJ;;_tbXjP4AAAAJ",
        "orcid": ";;;;;;0000-0002-4908-5434;;",
        "linkedin": ";blakerbuchanan/;;;bingqing-chen-631b754a/;;lpalmieri/;;",
        "or_profile": "~Saumya_Saxena1;~Blake_Buchanan1;~Chris_Paxton1;~Peiqi_Liu1;~Bingqing_Chen2;~Narunas_Vaskevicius2;~Luigi_Palmieri1;~Jonathan_Francis1;~Oliver_Kroemer1",
        "aff": "Carnegie Mellon University;Neya Systems;Hello Robot;Hello Robot Inc;Bosch;Robert Bosch GmbH, Bosch;Bosch;;Carnegie Mellon University",
        "aff_domain": "cmu.edu;neyarobotics.com;hello-robot.com;hello-robot.com;bosch.com;de.bosch.com;bosch.com;;cmu.edu",
        "position": "PhD student;Researcher;Senior Lead, Embodied AI;Intern;Researcher;Research Scientist;Group Leader;;Associate Professor",
        "bibtex": "@inproceedings{\nsaxena2025grapheqa,\ntitle={Graph{EQA}: Using 3D Semantic Scene Graphs for Real-time Embodied Question Answering},\nauthor={Saumya Saxena and Blake Buchanan and Chris Paxton and Peiqi Liu and Bingqing Chen and Narunas Vaskevicius and Luigi Palmieri and Jonathan Francis and Oliver Kroemer},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=Yy9EVIajH5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=Yy9EVIajH5",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3;4;4;4;5;0",
        "aff_unique_norm": "Carnegie Mellon University;Neya Systems;Hello Robot;Hello Robot Inc;Robert Bosch GmbH;",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.cmu.edu;;;;https://www.bosch.com;",
        "aff_unique_abbr": "CMU;;;;Bosch;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;2;2;2;0",
        "aff_country_unique": "United States;;Germany"
    },
    {
        "id": "ZA8iXa45P2",
        "title": "Tool-as-Interface: Learning Robot Policies from Observing Human Tool Use",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Tool use is essential for enabling robots to perform complex real-world tasks, but learning such skills requires extensive datasets. While teleoperation is widely used, it is slow, delay-sensitive, and poorly suited for dynamic tasks. In contrast, human videos provide a natural way for data collection without specialized hardware, though they pose challenges on robot learning due to viewpoint variations and embodiment gaps. To address these challenges, we propose a framework that transfers tool-use knowledge from humans to robots. To improve the policy's robustness to viewpoint variations, we use two RGB cameras to reconstruct 3D scenes and apply Gaussian splatting for novel view synthesis. We reduce the embodiment gap using segmented observations and tool-centric, task-space actions to achieve embodiment-invariant visuomotor policy learning. Our method achieves a 71\\% improvement in task success and a 77\\% reduction in data collection time compared to diffusion policies trained on teleoperation with equivalent time budgets. Our method also reduces data collection time by 41\\% compared with the state-of-the-art data collection interface.",
        "keywords": "Tool Use;Data Collection;Learning from Video",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Haonan Chen;Cheng Zhu;Shuijing Liu;Yunzhu Li;Katherine Rose Driggs-Campbell",
        "authorids": "~Haonan_Chen6;~Cheng_Zhu8;~Shuijing_Liu1;~Yunzhu_Li1;~Katherine_Rose_Driggs-Campbell1",
        "gender": ";;F;M;",
        "homepage": "https://ece.illinois.edu/about/directory/grad-students/haonan2;https://github.com/Char1ie215;https://shuijing725.github.io;https://yunzhuli.github.io/;",
        "dblp": ";;211/7210;182/1831;",
        "google_scholar": ";;I4k7ukgAAAAJ;WlA92lcAAAAJ;",
        "orcid": ";;;;",
        "linkedin": ";;shuijing-liu-4089b3123;;",
        "or_profile": "~Haonan_Chen6;~Cheng_Zhu8;~Shuijing_Liu1;~Yunzhu_Li1;~Katherine_Rose_Driggs-Campbell1",
        "aff": "University of Illinois Urbana-Champaign;University of Illinois, Urbana Champaign;, University of Texas at Austin;Columbia University;",
        "aff_domain": "illinois.edu;uiuc.edu;cs.utexas.edu;columbia.edu;",
        "position": "PhD student;Undergrad student;Postdoc;Assistant Professor;",
        "bibtex": "@inproceedings{\nchen2025toolasinterface,\ntitle={Tool-as-Interface: Learning Robot Policies from Observing Human Tool Use},\nauthor={Haonan Chen and Cheng Zhu and Shuijing Liu and Yunzhu Li and Katherine Rose Driggs-Campbell},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ZA8iXa45P2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZA8iXa45P2",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;2;3",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;University of Texas at Austin;Columbia University;",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://illinois.edu;https://www.utexas.edu;https://www.columbia.edu;",
        "aff_unique_abbr": "UIUC;UT Austin;Columbia;",
        "aff_campus_unique_index": "0;0;1",
        "aff_campus_unique": "Urbana-Champaign;Austin;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "ZPJo9RJL15",
        "title": "Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Exploration is crucial for legged robots to learn agile locomotion behaviors capable of overcoming diverse obstacles. \nFor example, a robot may need to try different contact patterns and momentum profiles to successfully jump over an obstacle\u2014but encouraging such diverse exploration is inherently challenging. As a result, training these behaviors often relies on additional techniques such as extensive reward engineering, expert demonstrations, or curriculum learning. However, these approaches limit generalizability, especially when prior knowledge or demonstration data is unavailable.\nIn this work, we propose using unsupervised skill discovery as a skill-level exploration strategy to significantly reduce human engineering effort. Our learning framework enables the agent to autonomously discover diverse skills to overcome complex obstacles. To dynamically regulate the degree of exploration throughout training, we introduce a bi-level optimization process that learns a parameter to balance two distinct reward signals. We demonstrate that our method enables quadrupedal robots to acquire highly agile behaviors\u2014including crawling, climbing, leaping, and complex maneuvers such as jumping off vertical walls. Finally, we successfully deploy the learned policy on real hardware, validating its transferability to the real world.",
        "keywords": "Unsupervised Reinforcement Learning;Locomotion;Quadruped;Skill Discovery",
        "primary_area": "",
        "supplementary_material": "/attachment/d2e43caf6a522c74dbe325ca966398ddf9b52faa.zip",
        "author": "Seungeun Rho;Kartik Garg;Morgan Byrd;Sehoon Ha",
        "authorids": "~Seungeun_Rho1;~Kartik_Garg1;~Morgan_Byrd1;~Sehoon_Ha2",
        "gender": "M;;;M",
        "homepage": ";https://sites.google.com/view/kartik-garg/home;https://morganbyrd03.github.io;https://www.cc.gatech.edu/~sha9",
        "dblp": "239/5265;305/5398;;33/10491",
        "google_scholar": ";yBSRBEEAAAAJ;;Q6F3O0sAAAAJ",
        "orcid": ";0000-0002-8585-3939;;",
        "linkedin": "seungeun-rho-2943a0179/;;;",
        "or_profile": "~Seungeun_Rho1;~Kartik_Garg1;~Morgan_Byrd1;~Sehoon_Ha2",
        "aff": "Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;gatech.edu;gatech.edu",
        "position": "PhD student;MS student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nrho2025unsupervised,\ntitle={Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion},\nauthor={Seungeun Rho and Kartik Garg and Morgan Byrd and Sehoon Ha},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ZPJo9RJL15}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZPJo9RJL15",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "ZmASpafbOc",
        "title": "Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Coordinated multi-robot navigation is essential for robots to operate as a team in diverse environments. \nDuring navigation, robot teams usually need to maintain specific formations, such as circular formations to protect human teammates at the center. \nHowever, in complex scenarios such as narrow corridors, rigidly preserving predefined formations can become infeasible. \nTherefore, robot teams must be capable of dynamically splitting into smaller subteams and adaptively controlling the subteams to navigate through such scenarios while preserving formations.\nTo enable this capability, we introduce a novel method for SubTeaming and Adaptive Formation (STAF), which is built upon a unified hierarchical learning framework:\n(1) high-level deep graph cut for team splitting, (2) intermediate-level graph learning for facilitating coordinated navigation among subteams, \nand (3) low-level policy learning for controlling individual mobile robots to reach their goal positions while avoiding collisions.\nTo evaluate STAF, we conducted extensive experiments in both indoor and outdoor environments using robotics simulations and physical robot teams.\nExperimental results show that STAF enables the novel capability for subteaming and adaptive formation control, and achieves promising performance in coordinated multi-robot navigation through challenging scenarios.\nMore details are available on the project website: https://anonymous188.github.io/STAF/.",
        "keywords": "Coordinated multi-robot navigation;Hierarchical learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zihao Deng;Peng Gao;Williard Joshua Jose;Maggie Wigness;John G. Rogers III;Brian Reily;Christopher M. Reardon;Hao Zhang",
        "authorids": "~Zihao_Deng4;~Peng_Gao4;~Williard_Joshua_Jose1;~Maggie_Wigness4;~John_G._Rogers_III1;~Brian_Reily1;~Christopher_M._Reardon1;~Hao_Zhang34",
        "gender": "M;M;M;F;M;;;",
        "homepage": ";https://research.csc.ncsu.edu/corob/index.html;;;;http://brianreily.com;;",
        "dblp": ";;273/3657;147/2719;18/8371;175/1243;13/7737;",
        "google_scholar": ";oQ26N8YAAAAJ;https://scholar.google.com.ph/citations?user=G0a-5lQAAAAJ;Y83_HFsAAAAJ;;H1ekQysAAAAJ;I3L9Y6AAAAAJ;",
        "orcid": ";;0000-0003-2062-0795;0000-0003-1707-8106;;;0000-0003-1393-2256;",
        "linkedin": "zihao-deng-239b7823a;peng-gao-02773b133/;williard-jose/;maggie-wigness-907a0984/;;;;",
        "or_profile": "~Zihao_Deng4;~Peng_Gao4;~Williard_Joshua_Jose1;~Maggie_Wigness4;~John_G._Rogers_III1;~Brian_Reily1;~Christopher_M._Reardon1;~Hao_Zhang34",
        "aff": "University of Massachusetts at Amherst+University of Massachusetts at Amherst;North Carolina State University;University of Massachusetts at Amherst+Samsung Research;CCDC Army Research Laboratory+DEVCOM Army Research Laboratory;DEVCOM Army Research Laboratory;DEVCOM U.S. Army Research Laboratory;The MITRE Corporation;",
        "aff_domain": "umass.edu+umass.edu;ncsu.edu;umass.edu+samsung.com;mail.mil+army.mil;arl.devcom.army.mil;army.mil;mitre.org;",
        "position": "PhD student+MS student;Assistant Professor;PhD student+Researcher;Computer Scientist+Researcher;Researcher;Researcher;Researcher;",
        "bibtex": "@inproceedings{\ndeng2025subteaming,\ntitle={Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation},\nauthor={Zihao Deng and Peng Gao and Williard Joshua Jose and Maggie Wigness and John G. Rogers III and Brian Reily and Christopher M. Reardon and Hao Zhang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ZmASpafbOc}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZmASpafbOc",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+0;1;0+2;3+4;4;5;6;7",
        "aff_unique_norm": "University of Massachusetts Amherst;North Carolina State University;Samsung;CCDC Army Research Laboratory;United States Army Research Laboratory;U.S. Army Research Laboratory;MITRE Corporation;",
        "aff_unique_dep": ";;Samsung Research;;Army Research Laboratory;DEVCOM;;",
        "aff_unique_url": "https://www.umass.edu;https://www.ncsu.edu;https://research.samsung.com;;https://www.arl.army.mil;https://www.arl.army.mil;https://www.mitre.org;",
        "aff_unique_abbr": "UMass Amherst;NCSU;Samsung;;ARL;ARL;MITRE;",
        "aff_campus_unique_index": "0+0;0;",
        "aff_campus_unique": "Amherst;",
        "aff_country_unique_index": "0+0;0;0+1;0;0;0;0",
        "aff_country_unique": "United States;South Korea;"
    },
    {
        "id": "ZqBXnR6ppz",
        "title": "Generalist Robot Manipulation beyond Action Labeled Data",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Recent advances in generalist robot manipulation leverage pre-trained Vision\u2013Language Models (VLMs) and large-scale robot demonstrations to tackle diverse tasks in a zero-shot manner. A key challenge remains: scaling high-quality, action-labeled robot demonstration data, which existing methods rely on for robustness and generalization. To address this, we propose a method that benefits from videos without action labels\u2014featuring humans and/or robots in action\u2014enhancing open-vocabulary performance and enabling data-efficient learning of new tasks. Our method extracts dense, dynamic 3D point clouds at the hand or gripper location and uses a proposed 3D dynamics predictor for self-supervision. This predictor is then tuned to an action predictor using a smaller labeled dataset for action alignment. We show that our method not only learns from unlabeled human and robot demonstrations\u2014improving downstream generalist robot policies\u2014but also enables robots to learn new tasks without action labels (i.e., out-of-action generalization) in both real-world and simulated settings.",
        "keywords": "Vision-Language-Action Models;Learning from Videos",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Alexander Spiridonov;Jan-Nico Zaech;Nikolay Nikolov;Luc Van Gool;Danda Pani Paudel",
        "authorids": "~Alexander_Spiridonov1;~Jan-Nico_Zaech1;~Nikolay_Nikolov1;~Luc_Van_Gool1;~Danda_Pani_Paudel3",
        "gender": "M;;M;;",
        "homepage": "https://aspiridon0v.github.io/;;;;",
        "dblp": ";;;61/5017;",
        "google_scholar": ";;https://scholar.google.com/citations?hl=en;https://scholar.google.be/citations?user=TwMib_QAAAAJ;",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Alexander_Spiridonov1;~Jan-Nico_Zaech1;~Nikolay_Nikolov1;~Luc_Van_Gool1;~Danda_Pani_Paudel3",
        "aff": "ETHZ - ETH Zurich;;Institute for Computer Science, Artificial Intelligence and Technology;Sofia Un. St. Kliment Ohridski;",
        "aff_domain": "ethz.ch;;insait.ai;insait.ai;",
        "position": "MS student;;PhD student;Full Professor;",
        "bibtex": "@inproceedings{\nspiridonov2025generalist,\ntitle={Generalist Robot Manipulation beyond Action Labeled Data},\nauthor={Alexander Spiridonov and Jan-Nico Zaech and Nikolay Nikolov and Luc Van Gool and Danda Pani Paudel},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ZqBXnR6ppz}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ZqBXnR6ppz",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3;1",
        "aff_unique_norm": "ETH Zurich;;Institute for Computer Science, Artificial Intelligence and Technology;Sofia Un. St. Kliment Ohridski",
        "aff_unique_dep": ";;Computer Science, Artificial Intelligence and Technology;",
        "aff_unique_url": "https://www.ethz.ch;;;",
        "aff_unique_abbr": "ETHZ;;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0",
        "aff_country_unique": "Switzerland;"
    },
    {
        "id": "a2RMXJbkJ8",
        "title": "ARCH: Hierarchical Hybrid Learning for Long-Horizon Contact-Rich Robotic Assembly",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Generalizable long-horizon robotic assembly requires reasoning at multiple levels of abstraction. While end-to-end imitation learning (IL) is a promising approach, it typically requires large amounts of expert demonstration data and often struggles to achieve the high precision demanded by assembly tasks. Reinforcement learning (RL) approaches, on the other hand, have shown some success in high-precision assembly, but suffer from sample inefficiency, which limits their effectiveness in long-horizon tasks. To address these challenges, we propose a hierarchical modular approach, named Adaptive Robotic Compositional Hierarchy (ARCH), which enables long-horizon, high-precision robotic assembly in contact-rich settings. ARCH employs a hierarchical planning framework, including a low-level primitive library of parameterized skills and a high-level policy. The low-level primitive library includes essential skills for assembly tasks, such as grasping and inserting. These primitives consist of both RL and model-based controllers. The high-level policy, learned via IL from a handful of demonstrations, without the need for teleoperation, selects the appropriate primitive skills and instantiates them with input parameters. We extensively evaluate our approach in simulation and on a real robotic manipulation platform. We show that ARCH generalizes well to unseen objects and outperforms baseline methods in terms of success rate and data efficiency.",
        "keywords": "Long-horizon learning;hybrid learning;robotic assembly",
        "primary_area": "",
        "supplementary_material": "/attachment/125344729e2bd329a3034df086b14a220bc45c7b.pdf",
        "author": "Jiankai Sun;Aidan Curtis;Yang You;Yan Xu;Michael Koehle;Qianzhong Chen;Suning Huang;Leonidas Guibas;Sachin Chitta;Mac Schwager;Hui Li",
        "authorids": "~Jiankai_Sun6;~Aidan_Curtis2;~Yang_You2;~Yan_Xu8;michael.koehle@autodesk.com;~Qianzhong_Chen2;~Suning_Huang1;~Leonidas_Guibas1;~Sachin_Chitta1;~Mac_Schwager1;~Hui_Li6",
        "gender": ";M;M;M;;M;;M;M;M;",
        "homepage": ";;https://qq456cvb.github.io;https://decayale.github.io/;;https://qianzhong-chen.github.io/;;http://geometry.stanford.edu/;https://www.sachinchitta.com/;https://msl.stanford.edu/;",
        "dblp": "121/4211;;33/8167;;;;;g/LeonidasJGuibas;;22/7012;l/HuiLi2",
        "google_scholar": "726MCb8AAAAJ;tRJf4Q8AAAAJ;1YV1_KUAAAAJ;https://scholar.google.com/citations?pli=1;;MqU82XsAAAAJ;;https://scholar.google.com.tw/citations?user=5JlEyTAAAAAJ;;-EqbTXoAAAAJ;",
        "orcid": ";;;0000-0002-3462-7931;;;;;;;",
        "linkedin": ";;;samuel-yan-xu;;qianzhong-chen-9bab01209/;;;;;huilimit/",
        "or_profile": "~Jiankai_Sun6;~Aidan_Curtis2;~Yang_You2;~Yan_Xu8;michael.koehle@autodesk.com;~Qianzhong_Chen2;~Suning_Huang1;~Leonidas_Guibas1;~Sachin_Chitta1;~Mac_Schwager1;~Hui_Li6",
        "aff": "Stanford University;Massachusetts Institute of Technology;Stanford University;University of Michigan - Ann Arbor;;Stanford University;;Stanford University;Autodesk;Stanford University;",
        "aff_domain": "stanford.edu;mit.edu;stanford.edu;umich.edu;;stanford.edu;;stanford.edu;autodesk.com;stanford.edu;",
        "position": "PhD student;PhD student;Postdoc;Postdoc;;MS student;;Full Professor;Researcher;Associate Professor;",
        "bibtex": "@inproceedings{\nsun2025arch,\ntitle={{ARCH}: Hierarchical Hybrid Learning for Long-Horizon Contact-Rich Robotic Assembly},\nauthor={Jiankai Sun and Aidan Curtis and Yang You and Yan Xu and Michael Koehle and Qianzhong Chen and Suning Huang and Leonidas Guibas and Sachin Chitta and Mac Schwager and Hui Li},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=a2RMXJbkJ8}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=a2RMXJbkJ8",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;2;3;0;3;0;4;0;3",
        "aff_unique_norm": "Stanford University;Massachusetts Institute of Technology;University of Michigan;;Autodesk",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.stanford.edu;https://web.mit.edu;https://www.umich.edu;;https://www.autodesk.com",
        "aff_unique_abbr": "Stanford;MIT;UM;;Autodesk",
        "aff_campus_unique_index": "0;0;2;0;0;0",
        "aff_campus_unique": "Stanford;;Ann Arbor",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "a5LFUOlkIj",
        "title": "ATK: Automatic Task-driven Keypoint Selection for Robust Policy Learning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Learning visuamotor policy through imitation learning often suffers from perceptual challenges, where visual differences between training and evaluation environments degrade policy performance. Policies relying on state estimations like 6D pose, require task-specific tracking and are difficult to scale, while raw sensor-based policies may lack robustness to small visual disturbances. In this work, we leverage 2D keypoints \u2014 spatially consistent features in the image frame \u2014 as a state representation for robust policy learning, and apply it to both sim-to-real transfer and real-world imitation learning. However, the choice of which keypoints to use can vary across objects and tasks. We propose a novel method -ATK, to automatically select keypoints in a task-driven manner, such that the chosen keypoints are \nthat are predictive of optimal behavior for the given task. Our proposal optimizes for a minimal set of task-relevant keypoints that preserve policy performance and robustness. We distill expert data (either from an expert policy in simulation or a human expert) into a policy that operates on RGB images while tracking the selected keypoints. By leveraging pre-trained visual modules, our system effectively tracks keypoints and transfers policies to the real-world evaluation scenario, even given perceptual challenges like transparent objects or fine-grained manipulation, or widely varying scene appearance. We validate our approach on various robotic tasks, demonstrating that these minimal keypoint representations improve robustness to visual disturbances and environmental variations.",
        "keywords": "Imitation learning;Sim2real;Representation Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/1f35c29d7de00cfe3bbc895cbfc14cd54108ea6b.zip",
        "author": "Yunchu Zhang;Shubham Mittal;Zhengyu Zhang;Liyiming Ke;Siddhartha Srinivasa;Abhishek Gupta",
        "authorids": "~Yunchu_Zhang1;~Shubham_Mittal3;~Zhengyu_Zhang3;~Liyiming_Ke1;~Siddhartha_Srinivasa1;~Abhishek_Gupta1",
        "gender": "M;;M;F;M;M",
        "homepage": "https://yunchuzhang.github.io/;;https://zoctipus.github.io/;http://kayke.xyz/;https://goodrobot.ai;https://homes.cs.washington.edu/~abhgupta/",
        "dblp": ";;;178/8670;;18/6404-4",
        "google_scholar": ";;;EhOtO3cAAAAJ;https://scholar.google.com.tw/citations?user=RCi98EAAAAAJ;1wLVDP4AAAAJ",
        "orcid": ";;;;;",
        "linkedin": ";;;;;",
        "or_profile": "~Yunchu_Zhang1;~Shubham_Mittal3;~Zhengyu_Zhang3;~Liyiming_Ke1;~Siddhartha_Srinivasa1;~Abhishek_Gupta1",
        "aff": ";;University of Washington;Physical Intelligence;Cruise+University of Washington;University of Washington",
        "aff_domain": ";;uw.edu;physicalintelligence.company;getcruise.com+washington.edu;uw.edu",
        "position": ";;Undergrad student;Researcher;Researcher+Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nzhang2025atk,\ntitle={{ATK}: Automatic Task-driven Keypoint Selection for Robust Policy Learning},\nauthor={Yunchu Zhang and Shubham Mittal and Zhengyu Zhang and Liyiming Ke and Siddhartha Srinivasa and Abhishek Gupta},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=a5LFUOlkIj}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=a5LFUOlkIj",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;2;3+1;1",
        "aff_unique_norm": ";University of Washington;Physical Intelligence;Cruise",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";https://www.washington.edu;;https://www.cruise.com",
        "aff_unique_abbr": ";UW;;Cruise",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1+1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "a9RXjOt5bU",
        "title": "The Sound of Simulation: Learning Multimodal Sim-to-Real Robot Policies with Generative Audio",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Robots must integrate multiple sensory modalities to act effectively in the real world. Yet, learning such multimodal policies at scale remains challenging. Simulation offers a viable solution, but while vision has benefited from high-fidelity simulators, other modalities (e.g. sound) can be notoriously difficult to simulate. As a result, sim-to-real transfer has succeeded primarily in vision-based tasks, with multimodal transfer still largely unrealized. In this work, we tackle these challenges by introducing MultiGen, a framework that integrates large-scale generative models into traditional physics simulators, enabling multisensory simulation. We showcase our framework on the dynamic task of robot pouring, which inherently relies on multimodal feedback. By synthesizing realistic audio conditioned on simulation video, our method enables training on rich audiovisual trajectories---without any real robot data. We demonstrate effective zero-shot transfer to real-world pouring with novel containers and liquids, highlighting the potential of generative modeling to both simulate hard-to-model modalities and close the multimodal sim-to-real gap.",
        "keywords": "generative modeling;real2sim;sim2real;multimodal learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Renhao Wang;Haoran Geng;Tingle Li;Philipp Wu;Feishi Wang;Gopala Anumanchipalli;Trevor Darrell;Boyi Li;Pieter Abbeel;Jitendra Malik;Alexei A Efros",
        "authorids": "~Renhao_Wang1;~Haoran_Geng1;~Tingle_Li1;~Philipp_Wu1;~Feishi_Wang1;~Gopala_Anumanchipalli1;~Trevor_Darrell2;~Boyi_Li1;~Pieter_Abbeel2;~Jitendra_Malik2;~Alexei_A_Efros1",
        "gender": ";M;M;M;M;M;;F;M;M;",
        "homepage": ";https://github.com/geng-haoran/haorangeng;https://tinglok.netlify.app/;https://github.com/wuphilipp;https://scholar.google.com/citations?user=eGG8hJgAAAAJ;http://people.eecs.berkeley.edu/~gopala/;;https://sites.google.com/site/boyilics/home;https://people.eecs.berkeley.edu/~pabbeel/;https://people.eecs.berkeley.edu/~malik/;",
        "dblp": "243/7150;295/7112;248/9136;;326/8419;54/7824;;;;58/2944;",
        "google_scholar": "q4RlE2oAAAAJ;Inr-6rEAAAAJ;UGpC1zgAAAAJ;;eGG8hJgAAAAJ;VecEj6kAAAAJ;;;https://scholar.google.com.tw/citations?user=vtwH6GkAAAAJ;oY9R5YQAAAAJ;",
        "orcid": ";;;;;0000-0002-9714-7740;;;;0000-0003-3695-1580;",
        "linkedin": ";haoran-geng-422778238/;;;;;;;;;",
        "or_profile": "~Renhao_Wang1;~Haoran_Geng1;~Tingle_Li1;~Philipp_Wu1;~Feishi_Wang1;~Gopala_Anumanchipalli1;~Trevor_Darrell2;~Boyi_Li1;~Pieter_Abbeel2;~Jitendra_Malik2;~Alexei_A_Efros1",
        "aff": "University of California, Berkeley;University of California, Berkeley;University of California, Berkeley;University of California, Berkeley;University of California, Berkeley+Peking University;University of California, Berkeley;;NVIDIA Research+University of California, Berkeley;Amazon+University of California, Berkeley;Meta Facebook+University of California, Berkeley;",
        "aff_domain": "berkeley.edu;berkeley.edu;eecs.berkeley.edu;berkeley.edu;berkeley.edu+pku.edu.cn;berkeley.edu;;nvidia.com+berkeley.edu;amazon.com+berkeley.edu;fb.com+berkeley.edu;",
        "position": "PhD student;PhD student;PhD student;PhD student;Visiting student+MS student;Assistant Professor;;Researcher+Postdoc;Amazon Scholar+Professor;Director of Research+Full Professor;",
        "bibtex": "@inproceedings{\nwang2025the,\ntitle={The Sound of Simulation: Learning Multimodal Sim-to-Real Robot Policies with Generative Audio},\nauthor={Renhao Wang and Haoran Geng and Tingle Li and Philipp Wu and Feishi Wang and Gopala Anumanchipalli and Trevor Darrell and Boyi Li and Pieter Abbeel and Jitendra Malik and Alexei A Efros},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=a9RXjOt5bU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=a9RXjOt5bU",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;0+1;0;2;3+0;4+0;5+0;2",
        "aff_unique_norm": "University of California, Berkeley;Peking University;;NVIDIA;Amazon;Meta",
        "aff_unique_dep": ";;;NVIDIA Research;Amazon.com, Inc.;Meta Platforms, Inc.",
        "aff_unique_url": "https://www.berkeley.edu;http://www.pku.edu.cn;;https://www.nvidia.com/research;https://www.amazon.com;https://meta.com",
        "aff_unique_abbr": "UC Berkeley;Peking U;;NVIDIA;Amazon;Meta",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Berkeley;",
        "aff_country_unique_index": "0;0;0;0;0+1;0;0+0;0+0;0+0",
        "aff_country_unique": "United States;China;"
    },
    {
        "id": "aSUNzvEJIf",
        "title": "Fabrica: Dual-Arm Assembly of General Multi-Part Objects via Integrated Planning and Learning",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Multi-part assembly poses significant challenges for robotic systems to execute long-horizon, contact-rich manipulation with generalization across complex geometries. We present a dual-arm robotic system capable of end-to-end planning and control for autonomous assembly of general multi-part objects. For planning over long horizons, we develop hierarchies of precedence, sequence, grasp, and motion planning with automated fixture generation, enabling general multi-step assembly on any dual-arm robots. The planner is made efficient through a parallelizable design and is optimized for downstream control stability. For contact-rich assembly steps, we propose a lightweight reinforcement learning framework that trains generalist policies across object geometries, assembly directions, and grasp poses, guided by equivaraiance and residual actions obtained from the plan. These policies transfer zero-shot to the real world and achieve 80% success rates. For systematic evaluation, we propose a benchmark suite of multi-part assemblies resembling industrial and daily objects across diverse categories and geometries. By integrating efficient global planning and robust local control, we demonstrate the first system to achieve complete and generalizable real-world multi-part assembly without domain knowledge or human demonstrations.",
        "keywords": "Assembly;Planning;Reinforcement Learning;Benchmark",
        "primary_area": "",
        "supplementary_material": "/attachment/f02d4b11a8c287296f180f4077b4ddcca74c33a5.pdf",
        "author": "Yunsheng Tian;Joshua Jacob;Yijiang Huang;Jialiang Zhao;Edward Li Gu;Pingchuan Ma;Annan Zhang;Farhad Javid;Branden Romero;Sachin Chitta;Shinjiro Sueda;Hui Li;Wojciech Matusik",
        "authorids": "~Yunsheng_Tian1;~Joshua_Jacob1;~Yijiang_Huang1;~Jialiang_Zhao1;~Edward_Li_Gu1;~Pingchuan_Ma3;~Annan_Zhang1;~Farhad_Javid1;~Branden_Romero1;~Sachin_Chitta1;~Shinjiro_Sueda1;~Hui_Li6;~Wojciech_Matusik2",
        "gender": "M;M;M;M;M;M;;M;M;M;M;;M",
        "homepage": "https://www.yunshengtian.com/;;https://web.mit.edu/yijiangh/www/;https://alanz.info/;https://egu.netlify.app/;https://people.csail.mit.edu/pcma;https://www.annanzhang.com;https://www.research.autodesk.com/people/farhad-javid/;https://www.csail.mit.edu/person/brandon-romero;https://www.sachinchitta.com/;http://people.tamu.edu/~sueda/;;https://cdfg.mit.edu/wojciech",
        "dblp": "224/0723;;;204/1900;;215/4446-2;;;;;69/4137.html;l/HuiLi2;",
        "google_scholar": "sf6RjM4AAAAJ;MrhRMMkAAAAJ;6W-0MUUAAAAJ;LaW7igYAAAAJ;;EtCZmkwAAAAJ;OyatgREAAAAJ;;;;https://scholar.google.com.tw/citations?user=mBqenhsAAAAJ;;https://scholar.google.com/citations?hl=en",
        "orcid": ";0009-0001-3165-1012;0000-0003-1820-2535;;;;0000-0001-6664-9417;;;;0000-0003-4656-498X;;0000-0003-0212-5643",
        "linkedin": ";joshmjacob/;;jialiang-zhao/;edwardgu2/;;;;;;;huilimit/;wojciech-matusik-67238126/",
        "or_profile": "~Yunsheng_Tian1;~Joshua_Jacob1;~Yijiang_Huang1;~Jialiang_Zhao1;~Edward_Li_Gu1;~Pingchuan_Ma3;~Annan_Zhang1;~Farhad_Javid1;~Branden_Romero1;~Sachin_Chitta1;~Shinjiro_Sueda1;~Hui_Li6;~Wojciech_Matusik2",
        "aff": "Massachusetts Institute of Technology;Carnegie Mellon University+Massachusetts Institute of Technology;ETHZ - ETH Zurich;Massachusetts Institute of Technology;;OpenAI+Massachusetts Institute of Technology;Massachusetts Institute of Technology;;Massachusetts Institute of Technology;Autodesk;Texas A&M University - College Station;;Massachusetts Institute of Technology",
        "aff_domain": "csail.mit.edu;andrew.cmu.edu+mit.edu;ethz.ch;mit.edu;;openai.com+mit.edu;mit.edu;;mit.edu;autodesk.com;tamu.edu;;mit.edu",
        "position": "PhD student;PhD student+Intern;Postdoc;PhD student;;Researcher+PhD student;PhD student;;PhD student;Researcher;Associate Professor;;Full Professor",
        "bibtex": "@inproceedings{\ntian2025fabrica,\ntitle={Fabrica: Dual-Arm Assembly of General Multi-Part Objects via Integrated Planning and Learning},\nauthor={Yunsheng Tian and Joshua Jacob and Yijiang Huang and Jialiang Zhao and Edward Li Gu and Pingchuan Ma and Annan Zhang and Farhad Javid and Branden Romero and Sachin Chitta and Shinjiro Sueda and Hui Li and Wojciech Matusik},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=aSUNzvEJIf}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=aSUNzvEJIf",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            13,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1+0;2;0;3;4+0;0;3;0;5;6;3;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Carnegie Mellon University;ETH Zurich;;OpenAI;Autodesk;Texas A&M University",
        "aff_unique_dep": ";;;;;;",
        "aff_unique_url": "https://web.mit.edu;https://www.cmu.edu;https://www.ethz.ch;;https://openai.com;https://www.autodesk.com;https://www.tamu.edu",
        "aff_unique_abbr": "MIT;CMU;ETHZ;;OpenAI;Autodesk;TAMU",
        "aff_campus_unique_index": ";;1",
        "aff_campus_unique": ";College Station",
        "aff_country_unique_index": "0;0+0;1;0;0+0;0;0;0;0;0",
        "aff_country_unique": "United States;Switzerland;"
    },
    {
        "id": "aZwWRycAXi",
        "title": "GraspQP: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Dexterous robotic hands enable versatile interactions through the flexibility and adaptability of a multi-finger setup, allowing for a wise range of task-specific grasp configurations in diverse environments.\nHowever, access to diverse and high-quality grasp data is essential to fully exploit the capabilities of dexterous hands, be it to train grasp prediction models from point clouds, train manipulation policies, or to support high-level task planning with a broader range of action options.\nExisting approaches for dataset generation rely on sampling-based algorithms or simplified force-closure analysis, which tend to converge to power grasps and often exhibit limited diversity.\nIn this work, we propose a method to synthesize large-scale, diverse, and physically feasible grasps that additionally go beyond simple power grasps to more refined manipulation, such as pinches or tri-finger precision grasps.\nWe introduce a rigorous differentiable energy formulation of force closure, implicitly defined through a Quadratic Program (QP).\nIn addition, we present an adjusted optimization method (MALA*) that improves performance by dynamically rejecting gradient steps based on the global sample distribution.\nWe extensively evaluate our approach and demonstrate significant improvements in both grasp diversity and the stability of final grasp predictions. Finally, we provide a new, large-scale grasp dataset for the 5'700 objects from DexGraspNet, consisting of five different grippers and three different grasp types.",
        "keywords": "Grasping;Manipulation;Optimization",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Ren\u00e9 Zurbr\u00fcgg;Andrei Cramariuc;Marco Hutter",
        "authorids": "~Ren\u00e9_Zurbr\u00fcgg1;~Andrei_Cramariuc1;~Marco_Hutter1",
        "gender": "M;M;M",
        "homepage": ";;http://www.rsl.ethz.ch",
        "dblp": "292/2398;;04/2753",
        "google_scholar": "feJr7REAAAAJ;QZKCzOQAAAAJ;https://scholar.google.ch/citations?user=DO3quJYAAAAJ",
        "orcid": ";0000-0002-9301-0253;0000-0002-4285-4990",
        "linkedin": ";andrei-cramariuc-43625b163/;",
        "or_profile": "~Ren\u00e9_Zurbr\u00fcgg1;~Andrei_Cramariuc1;~Marco_Hutter1",
        "aff": "ETHZ - ETH Zurich;ETHZ - ETH Zurich;ETHZ - ETH Zurich",
        "aff_domain": "ethz.ch;ethz.ch;ethz.ch",
        "position": "PhD student;Postdoc;Associate Professor",
        "bibtex": "@inproceedings{\nzurbrugg2025graspqp,\ntitle={Grasp{QP}: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping},\nauthor={Ren{\\'e} Zurbr{\\\"u}gg and Andrei Cramariuc and Marco Hutter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=aZwWRycAXi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=aZwWRycAXi",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "ETH Zurich",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.ethz.ch",
        "aff_unique_abbr": "ETHZ",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Switzerland"
    },
    {
        "id": "b24y5SENo5",
        "title": "Latent Theory of Mind: A Decentralized Diffusion Architecture for Cooperative Manipulation",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "We present Latent Theory of Mind (LatentToM), a decentralized diffusion policy architecture for collaborative robot manipulation. Our policy allows multiple manipulators with their own perception and computation to collaborate with each other towards a common task goal with or without explicit communication. Our key innovation lies in allowing each agent to maintain two latent representations: an ego embedding specific to the robot, and a consensus embedding trained to be common to both robots, despite their different sensor streams and poses. We further let each robot train a decoder to infer the other robot's ego embedding from their consensus embedding, akin to \"theory of mind\" in latent space. Training occurs centrally, with all the policies' consensus encoders supervised by a loss inspired by sheaf theory, a mathematical theory for clustering data on a topological manifold. Specifically, we introduce a first-order cohomology loss to enforce sheaf-consistent alignment of the consensus embeddings. To preserve the expressiveness of the consensus embedding, we further propose structural constraints based on theory of mind and a directional consensus mechanism. Execution can be fully distributed, requiring no explicit communication between policies. In which case, the information is exchanged implicitly through each robot's sensor stream by observing the actions of the other robots and their effects on the scene. Alternatively, execution can leverage direct communication to share the robots' consensus embeddings, where the embeddings are shared once during each inference step and are aligned using the sheaf Laplacian. While we tested our method using two manipulators, our approach can naturally be extended to an arbitrary number of agents. In our hardware experiments, LatentToM outperforms a naive decentralized diffusion baseline, and shows comparable performance with a state-of-the-art centralized diffusion policy for bi-manual manipulation. Additionally, we show that LatentToM is naturally robust to temporary robot failure or delays, while a centralized policy may fail.",
        "keywords": "Cooperative Manipulation;Diffusion Policy;Consensus Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/003c865b3604836b70f5520fddf86dd9cc07e8d7.zip",
        "author": "Chengyang He;Gadiel Mark Sznaier Camps;Xu Liu;Mac Schwager;Guillaume Adrien Sartoretti",
        "authorids": "~Chengyang_He1;~Gadiel_Mark_Sznaier_Camps1;~Xu_Liu11;~Mac_Schwager1;~Guillaume_Adrien_Sartoretti1",
        "gender": "M;M;M;M;M",
        "homepage": ";;https://scholar.google.com/citations?user=dSIEUlEAAAAJ&hl=en;https://msl.stanford.edu/;https://marmotlab.org/",
        "dblp": ";;;22/7012;118/9066",
        "google_scholar": ";;dSIEUlEAAAAJ;-EqbTXoAAAAJ;n7NzZ0sAAAAJ",
        "orcid": "0009-0001-8188-3516;0000-0002-7863-3137;;;0000-0002-7579-9916",
        "linkedin": ";gadiel-sznaier-camps-36aaa2a8;;;",
        "or_profile": "~Chengyang_He1;~Gadiel_Mark_Sznaier_Camps1;~Xu_Liu11;~Mac_Schwager1;~Guillaume_Adrien_Sartoretti1",
        "aff": "Stanford University+National University of Singapore;Stanford University;Stanford University;Stanford University;National University of Singapore",
        "aff_domain": "stanford.edu+u.nus.edu;stanford.edu;stanford.edu;stanford.edu;nus.edu.sg",
        "position": "PhD student+PhD student;PhD student;Postdoc;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nhe2025latent,\ntitle={Latent Theory of Mind: A Decentralized Diffusion Architecture for Cooperative Manipulation},\nauthor={Chengyang He and Gadiel Mark Sznaier Camps and Xu Liu and Mac Schwager and Guillaume Adrien Sartoretti},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=b24y5SENo5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=b24y5SENo5",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;0;0;0;1",
        "aff_unique_norm": "Stanford University;National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;https://www.nus.edu.sg",
        "aff_unique_abbr": "Stanford;NUS",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0+1;0;0;0;1",
        "aff_country_unique": "United States;Singapore"
    },
    {
        "id": "b2mXmmGX8E",
        "title": "IRIS: An Immersive Robot Interaction System",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "This paper introduces IRIS, an Immersive Robot Interaction System leveraging Extended Reality (XR). Existing XR-based systems enable efficient data collection but are often challenging to reproduce and reuse due to their specificity to particular robots, objects, simulators, and environments. IRIS addresses these issues by supporting immersive interaction and data collection across diverse simulators and real-world scenarios. It visualizes arbitrary rigid and deformable objects, robots from simulation, and integrates real-time sensor-generated point clouds for real-world applications. Additionally, IRIS enhances collaborative capabilities by enabling multiple users to simultaneously interact within the same virtual scene. Extensive experiments demonstrate that IRIS offers efficient and intuitive data collection in both simulated and real-world settings.",
        "keywords": "Human-Robot Interaction;Extended Reality;Robot Learning: Imitation Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Xinkai Jiang;Qihao Yuan;Enes Ulas Dincer;Hongyi Zhou;Ge Li;Xueyin Li;Xiaogang Jia;Timo Schnizer;Nicolas Schreiber;Weiran Liao;Julius Haag;Kailai Li;Gerhard Neumann;Rudolf Lioutikov",
        "authorids": "~Xinkai_Jiang1;~Qihao_Yuan1;~Enes_Ulas_Dincer1;~Hongyi_Zhou1;~Ge_Li3;~Xueyin_Li1;~Xiaogang_Jia1;~Timo_Schnizer1;~Nicolas_Schreiber1;~Weiran_Liao3;~Julius_Haag1;~Kailai_Li2;~Gerhard_Neumann2;~Rudolf_Lioutikov1",
        "gender": "M;M;M;M;M;M;M;M;M;M;M;M;;M",
        "homepage": ";https://github.com/sunflower-leaf;;https://hongyizhoucn.github.io/;;;https://xiaogangjia.github.io/Personal_Website/;https://github.com/tsnz;https://alr.anthropomatik.kit.edu/21_261.php;https://github.com/Andrewllab;;https://kailaili.github.io/;;https://rudolf.intuitive-robots.net",
        "dblp": ";;;;;;23/10777;;;;;;;151/9451",
        "google_scholar": "1BfDuRMAAAAJ;;;W35-J2sAAAAJ;;;E7Tja9gAAAAJ;;;;;6ehGZ2sAAAAJ;;hvjV43MAAAAJ",
        "orcid": ";;;;;;;;;;0009-0008-8642-6155;0000-0002-2368-3217;;",
        "linkedin": ";;enes-ula\u015f-din\u00e7er-202492158;hongyi-zhou-9413b9242/;geli-bruce/;xueyin-li-jilin;;;nicolas-schreiber/;;;kailai-li-294657b7;;rudolf-lioutikov-74830730a/",
        "or_profile": "~Xinkai_Jiang1;~Qihao_Yuan1;~Enes_Ulas_Dincer1;~Hongyi_Zhou1;~Ge_Li3;~Xueyin_Li1;~Xiaogang_Jia1;~Timo_Schnizer1;~Nicolas_Schreiber1;~Weiran_Liao3;~Julius_Haag1;~Kailai_Li2;~Gerhard_Neumann2;~Rudolf_Lioutikov1",
        "aff": "Karlsruher Institut f\u00fcr Technologie;University of Groningen;Karlsruher Institut f\u00fcr Technologie;Karlsruher Institut f\u00fcr Technologie;Karlsruhe Institute of Technology;Karlsruher Institut f\u00fcr Technologie;Karlsruher Institut f\u00fcr Technologie;Karlsruher Institut f\u00fcr Technologie;Karlsruher Institut f\u00fcr Technologie;;Karlsruher Institut f\u00fcr Technologie;University of Groningen;;Karlsruher Institut f\u00fcr Technologie",
        "aff_domain": "kit.edu;rug.nl;kit.edu;kit.edu;kit.edu;kit.edu;kit.edu;kit.edu;kit.edu;;kit.edu;rug.nl;;kit.edu",
        "position": "PhD student;PhD student;PhD student;PhD student;PhD student;MS student;PhD student;MS student;PhD student;;Undergrad student;Assistant Professor;;Tenure-Track Professor",
        "bibtex": "@inproceedings{\njiang2025iris,\ntitle={{IRIS}: An Immersive Robot Interaction System},\nauthor={Xinkai Jiang and Qihao Yuan and Enes Ulas Dincer and Hongyi Zhou and Ge Li and Xueyin Li and Xiaogang Jia and Timo Schnizer and Nicolas Schreiber and Weiran Liao and Julius Haag and Kailai Li and Gerhard Neumann and Rudolf Lioutikov},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=b2mXmmGX8E}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=b2mXmmGX8E",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            14,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0;2;0;0;0;0;3;0;1;3;0",
        "aff_unique_norm": "Karlsruher Institut f\u00fcr Technologie;University of Groningen;Karlsruhe Institute of Technology;",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.kit.edu;https://www.rug.nl;https://www.kit.edu;",
        "aff_unique_abbr": "KIT;RUG;KIT;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;0;0;0;0;0;0;0;0;1;0",
        "aff_country_unique": "Germany;Netherlands;"
    },
    {
        "id": "b86nyIOJWq",
        "title": "exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Tactile-aware robot learning faces critical challenges in data collection and representation due to data scarcity and sparsity, and the absence of force feedback in existing systems. To address these limitations, we introduce a tactile robot learning system with both hardware and algorithm innovations. We present exUMI, an extensible data collection device that enhances the vanilla UMI with robust proprioception (via AR MoCap and rotary encoder), modular visuo-tactile sensing, and automated calibration, achieving 100% data usability. Building on an efficient collection of over 1 M tactile frames, we propose Tactile Prediction Pretraining (TPP), a representation learning framework through action-aware temporal tactile prediction, capturing contact dynamics and mitigates tactile sparsity. Real-world experiments show that TPP outperforms traditional tactile imitation learning. Our work bridges the gap between human tactile intuition and robot learning through co-designed hardware and algorithms, offering open-source resources to advance contact-rich manipulation research.",
        "keywords": "Tactile Sensing;Robot Data Collection System;Imitation Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/95c49bb7268d70c68bca741d0aa705ea19d64717.zip",
        "author": "Yue Xu;Litao Wei;Pengyu An;Qingyu Zhang;Yong-Lu Li",
        "authorids": "~Yue_Xu4;~Litao_Wei1;~Pengyu_An1;~Qingyu_Zhang7;~Yong-Lu_Li1",
        "gender": "M;M;M;;M",
        "homepage": "https://silicx.github.io;https://www.sjtu.edu.cn/;https://pengyu-an.github.io/;;https://dirtyharrylyl.github.io/",
        "dblp": ";;;;198/9345",
        "google_scholar": "N03Uc1oAAAAJ;;0GhWmzAAAAAJ;;https://scholar.google.com.hk/citations?user=UExAaVgAAAAJ",
        "orcid": "0000-0001-7489-7269;;;;0000-0003-0478-0692",
        "linkedin": ";;;;%E6%B0%B8%E9%9C%B2-%E6%9D%8E-991b99139/",
        "or_profile": "~Yue_Xu4;~Litao_Wei1;~Pengyu_An1;~Qingyu_Zhang7;~Yong-Lu_Li1",
        "aff": "Shanghai Jiaotong University;Shanghai Jiaotong University;Shanghai Jiaotong University;;Shanghai Jiaotong University",
        "aff_domain": "sjtu.edu;sjtu.edu.cn;sjtu.edu.cn;;sjtu.edu.cn",
        "position": "PhD student;Undergrad student;Undergrad student;;Assistant Professor",
        "bibtex": "@inproceedings{\nxu2025exumi,\ntitle={ex{UMI}: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation},\nauthor={Yue Xu and Litao Wei and Pengyu An and Qingyu Zhang and Yong-Lu Li},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=b86nyIOJWq}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=b86nyIOJWq",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "Shanghai Jiao Tong University;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.sjtu.edu.cn;",
        "aff_unique_abbr": "SJTU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "b8RqTaDyb3",
        "title": "From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Autonomous skill discovery aims to enable robots to acquire diverse be-haviors without explicit supervision. Learning such behaviors directly on physical hardware remains challenging due to safety and data efficiency constraints. Existing methods, including Quality-Diversity Actor-Critic (QDAC), require manually defined skill spaces and carefully tuned heuristics, limiting real-world applicability. We propose Unsupervised Real-world Skill Acquisition (URSA), an extension of QDAC that enables robots to autonomously discover and master diverse, high-performing skills directly in the real world. We demonstrate that URSA successfully discovers diverse locomotion skills on a Unitree A1 quadruped in both simulation and the real world. Our approach supports both heuristic-driven skill discovery and fully unsupervised settings. We also show that the learn skill repertoire can be reused for downstream tasks such as real-world damage adaptation, where URSA outperforms all baselines in 5 out of 9 simulated and 3 out of 5 real-world damage scenarios. Our results establish a new framework for real-world robot learning that enables continuous skill discovery with limited human intervention, representing a significant step toward more autonomous and adaptable robotic systems.",
        "keywords": "Quality-Diversity;Reinforcement Learning;Real-World Learning;Robotics",
        "primary_area": "",
        "supplementary_material": "/attachment/14374ae384c51cc3ba715d22ff5c58cb117d18c9.zip",
        "author": "Luca Grillotti;Lisa Coiffard;Oscar Pang;Maxence Faldor;Antoine Cully",
        "authorids": "~Luca_Grillotti1;~Lisa_Coiffard1;k.pang@imperial.ac.uk;~Maxence_Faldor1;~Antoine_Cully1",
        "gender": "M;F;;M;M",
        "homepage": "https://luca.grillotti.com;;;https://maxencefaldor.github.io;",
        "dblp": ";;;342/2945;https://dblp.org/pers/c/Cully:Antoine.html",
        "google_scholar": ";;;s36pCYsAAAAJ;rZtJlPQAAAAJ",
        "orcid": ";;;0000-0003-4743-9494;",
        "linkedin": ";lisa-coiffard-b60340176/;;maxencefaldor/;",
        "or_profile": "~Luca_Grillotti1;~Lisa_Coiffard1;k.pang@imperial.ac.uk;~Maxence_Faldor1;~Antoine_Cully1",
        "aff": "Sakana AI+Imperial College London;Merantix Momentum;;Imperial College London;Imperial College London",
        "aff_domain": "sakana.ai+imperial.ac.uk;merantix.com;;imperial.ac.uk;imperial.ac.uk",
        "position": "Researcher+PhD student;Researcher;;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\ngrillotti2025from,\ntitle={From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity},\nauthor={Luca Grillotti and Lisa Coiffard and Oscar Pang and Maxence Faldor and Antoine Cully},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=b8RqTaDyb3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=b8RqTaDyb3",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;2;3;1;1",
        "aff_unique_norm": "Sakana AI;Imperial College London;Merantix;",
        "aff_unique_dep": ";;Momentum;",
        "aff_unique_url": ";https://www.imperial.ac.uk;https://www.merantix.com;",
        "aff_unique_abbr": ";ICL;Merantix;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;2;1;1",
        "aff_country_unique": ";United Kingdom;Germany"
    },
    {
        "id": "bILubVwPoD",
        "title": "Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Imitation learning enables intelligent systems to acquire complex behaviors with minimal supervision. However, existing methods often focus on short-horizon skills, require large datasets, and struggle to solve long-horizon tasks or generalize across task variations and distribution shifts. We propose a novel neuro-symbolic framework that jointly learns continuous control policies and symbolic domain abstractions from a few skill demonstrations. Our method abstracts high-level task structures into a graph, discovers symbolic rules via an Answer Set Programming solver, and trains low-level controllers using diffusion policy imitation learning. A high-level oracle filters task-relevant information to focus each controller on a minimal observation and action space. Our graph-based neuro-symbolic framework enables capturing complex state transitions, including non-spatial and temporal relations, that data-driven learning or clustering techniques often fail to discover in limited demonstration datasets. We validate our approach in six domains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers of Hanoi environments, and a distinct Automated Forklift domain with two environments. The results demonstrate high data efficiency with as few as five skill demonstrations, strong zero- and few-shot generalizations, and interpretable decision making. Our code is publicly available.",
        "keywords": "Neuro-symbolic;Imitation Learning;Task and Motion Planning;Symbolic Planning;Skill Learning;Human-Robot Interaction",
        "primary_area": "",
        "supplementary_material": "/attachment/1dfa3e7419a3f87ed508f7836cc3def06ca1aecf.zip",
        "author": "Pierrick Lorang;Hong Lu;Johannes Huemer;Patrik Zips;Matthias Scheutz",
        "authorids": "~Pierrick_Lorang1;hong.lu663424@tufts.edu;~Johannes_Huemer1;~Patrik_Zips1;~Matthias_Scheutz1",
        "gender": "M;;M;M;M",
        "homepage": "https://hrilab.tufts.edu/team/;;https://www.ait.ac.at;;https://engineering.tufts.edu/cs/people/faculty/matthias-scheutz",
        "dblp": ";;;;00/2197.html",
        "google_scholar": "fuj2TwsAAAAJ;;;RGzQXegAAAAJ;https://scholar.google.com.tw/citations?user=5yT3GScAAAAJ",
        "orcid": "0000-0002-9030-2349;;;;0000-0002-0064-2789",
        "linkedin": "pierrick-lorang-099423159/;;;;",
        "or_profile": "~Pierrick_Lorang1;hong.lu663424@tufts.edu;~Johannes_Huemer1;~Patrik_Zips1;~Matthias_Scheutz1",
        "aff": "Tufts University;;AIT Austrian Institute Of Technology;AIT Austrian Institute Of Technology;Tufts University",
        "aff_domain": "tufts.edu;;ait.ac.at;ait.ac.at;tufts.edu",
        "position": "PhD student;;Researcher;Researcher;Full Professor",
        "bibtex": "@inproceedings{\nlorang2025fewshot,\ntitle={Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting},\nauthor={Pierrick Lorang and Hong Lu and Johannes Huemer and Patrik Zips and Matthias Scheutz},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=bILubVwPoD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bILubVwPoD",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;2;0",
        "aff_unique_norm": "Tufts University;;AIT Austrian Institute Of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tufts.edu;;",
        "aff_unique_abbr": "Tufts;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "bOVF8Rj33i",
        "title": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback\u2014a capability that remains difficult to replicate in robots through behavioral cloning alone, due to the suboptimality and limited diversity of human demonstrations. In this work, we present VT-Refine, a visuo-tactile policy learning framework that combines real-world demonstrations, high-fidelity tactile simulation, and reinforcement learning to tackle precise, contact-rich bimanual assembly. We begin by training a diffusion policy on a small set of demonstrations using synchronized visual and tactile inputs. This policy is then transferred to a simulated digital twin equipped with simulated tactile sensors and further refined via large-scale reinforcement learning to enhance robustness and generalization. To enable accurate sim-to-real transfer, we leverage high-resolution piezoresistive tactile sensors that provide normal force signals and can be realistically modeled in parallel using GPU-accelerated simulation. Experimental results show that VT-Refine improves assembly performance in both simulation and the real world by increasing data diversity and enabling more effective policy fine-tuning.\nOur project page is available at https://vt-refine.github.io/ .",
        "keywords": "Tactile Simulation;Bimanual Manipulation;RL Fine-Tuning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Binghao Huang;Jie Xu;Iretiayo Akinola;Wei Yang;Balakumar Sundaralingam;Rowland O'Flaherty;Dieter Fox;Xiaolong Wang;Arsalan Mousavian;Yu-Wei Chao;Yunzhu Li",
        "authorids": "~Binghao_Huang1;~Jie_Xu7;~Iretiayo_Akinola1;~Wei_Yang2;~Balakumar_Sundaralingam1;~Rowland_O'Flaherty1;~Dieter_Fox1;~Xiaolong_Wang3;~Arsalan_Mousavian1;~Yu-Wei_Chao1;~Yunzhu_Li1",
        "gender": ";M;M;M;M;M;M;M;M;M;M",
        "homepage": "https://binghao-huang.github.io/;https://people.csail.mit.edu/jiex;;http://wyang.me/;https://balakumar-s.github.io/;;https://homes.cs.washington.edu/~fox/;https://xiaolonw.github.io/;https://cs.gmu.edu/~amousavi/;http://www-personal.umich.edu/~ywchao/;https://yunzhuli.github.io/",
        "dblp": ";37/5126-28;;03/1094-19;;;f/DieterFox;91/952-4;164/8572;44/10700;182/1831",
        "google_scholar": "nqoOetAAAAAJ;3Tj5lWEAAAAJ;e1zesfMAAAAJ;6QQX88UAAAAJ;https://scholar.google.com/citations?hl=en;;DqXsbPAAAAAJ;Y8O9N_0AAAAJ;fcA9m88AAAAJ;48Y9F-YAAAAJ;WlA92lcAAAAJ",
        "orcid": ";;;0000-0003-3975-2472;;;;;;;",
        "linkedin": ";;;;;rowlandoflaherty/;;;;;",
        "or_profile": "~Binghao_Huang1;~Jie_Xu7;~Iretiayo_Akinola1;~Wei_Yang2;~Balakumar_Sundaralingam1;~Rowland_O'Flaherty1;~Dieter_Fox1;~Xiaolong_Wang3;~Arsalan_Mousavian1;~Yu-Wei_Chao1;~Yunzhu_Li1",
        "aff": "Columbia University;NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA;NVIDIA Research+Department of Computer Science;University of California, San Diego;NVIDIA;NVIDIA;Columbia University",
        "aff_domain": "columbia.edu;nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com;nvidia.com+cs.washington.edu;ucsd.edu;nvidia.com;nvidia.com;columbia.edu",
        "position": "PhD student;Researcher;Researcher;Research Scientist;Research Scientist;Researcher;Senior Director of Robotics Research+Full Professor;Assistant Professor;Research Scientist;Research Scientist;Assistant Professor",
        "bibtex": "@inproceedings{\nhuang2025vtrefine,\ntitle={{VT}-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning},\nauthor={Binghao Huang and Jie Xu and Iretiayo Akinola and Wei Yang and Balakumar Sundaralingam and Rowland O'Flaherty and Dieter Fox and Xiaolong Wang and Arsalan Mousavian and Yu-Wei Chao and Yunzhu Li},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=bOVF8Rj33i}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bOVF8Rj33i",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;1;1;1;1+2;3;1;1;0",
        "aff_unique_norm": "Columbia University;NVIDIA;Unknown Institution;University of California, San Diego",
        "aff_unique_dep": ";NVIDIA Corporation;Department of Computer Science;",
        "aff_unique_url": "https://www.columbia.edu;https://www.nvidia.com;;https://www.ucsd.edu",
        "aff_unique_abbr": "Columbia;NVIDIA;;UCSD",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "bU15EK0oqk",
        "title": "CASPER: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Assistive teleoperation, where control is shared between a human and a robot, enables efficient and intuitive human-robot collaboration in diverse and unstructured environments. A central challenge in real-world assistive teleoperation is for the robot to infer a wide range of human intentions from user control inputs and to assist users with correct actions. Existing methods are either confined to simple, predefined scenarios or restricted to task-specific data distributions at training, limiting their support for real-world assistance.\nWe introduce Casper, an assistive teleoperation system that leverages commonsense knowledge embedded in pre-trained visual language models (VLMs) for real-time intent inference and flexible skill execution. Casper incorporates an open-world perception module for a generalized understanding of novel objects and scenes, a VLM-powered intent inference mechanism that leverages commonsense reasoning to interpret snippets of teleoperated user input, and a skill library that expands the scope of prior assistive teleoperation systems to support diverse, long-horizon mobile manipulation tasks. Extensive empirical evaluation, including human studies and system ablations, demonstrates that Casper improves task performance, reduces human cognitive load, and achieves higher user satisfaction than direct teleoperation and assistive teleoperation baselines. More information is available at https://casper-corl25.github.io/",
        "keywords": "Assistive Teleoperation;Mobile Manipulation;Vision Language Models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Huihan Liu;Rutav Shah;Shuijing Liu;Jack Pittenger;Mingyo Seo;Yuchen Cui;Yonatan Bisk;Roberto Mart\u00edn-Mart\u00edn;Yuke Zhu",
        "authorids": "~Huihan_Liu1;~Rutav_Shah1;~Shuijing_Liu1;jackpittenger@utexas.edu;~Mingyo_Seo1;~Yuchen_Cui1;~Yonatan_Bisk1;~Roberto_Mart\u00edn-Mart\u00edn1;~Yuke_Zhu1",
        "gender": ";M;F;;;F;M;M;M",
        "homepage": ";https://shahrutav.github.io;https://shuijing725.github.io;;https://mingyoseo.com;https://yuchencui.cc;http://www.YonatanBisk.com;https://robertomartinmartin.com/;https://yukezhu.me/",
        "dblp": ";;211/7210;;;201/5416.html;38/9282;153/7670;133/1772",
        "google_scholar": ";;I4k7ukgAAAAJ;;;qQz2cm8AAAAJ;bWoGh8UAAAAJ;XOJE8OEAAAAJ;mWGyYMsAAAAJ",
        "orcid": ";;;;;0000-0001-7417-1222;0000-0002-2111-9081;0000-0002-9586-2759;",
        "linkedin": ";rutav-shah-01a2941a7;shuijing-liu-4089b3123;;;;yonatanbisk/;;",
        "or_profile": "~Huihan_Liu1;~Rutav_Shah1;~Shuijing_Liu1;jackpittenger@utexas.edu;~Mingyo_Seo1;~Yuchen_Cui1;~Yonatan_Bisk1;~Roberto_Mart\u00edn-Mart\u00edn1;~Yuke_Zhu1",
        "aff": ";University of Texas at Austin;, University of Texas at Austin;;University of Texas at Austin;University of California, Los Angeles;Carnegie Mellon University;Amazon+University of Texas at Austin;Computer Science Department, University of Texas, Austin",
        "aff_domain": ";utexas.edu;cs.utexas.edu;;utexas.edu;ucla.edu;cmu.edu;amazon.com+utexas.edu;cs.utexas.edu",
        "position": ";PhD student;Postdoc;;PhD student;Assistant Professor;Assistant Professor;Researcher+Assistant Professor;Associate Professor",
        "bibtex": "@inproceedings{\nliu2025casper,\ntitle={{CASPER}: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models},\nauthor={Huihan Liu and Rutav Shah and Shuijing Liu and Jack Pittenger and Mingyo Seo and Yuchen Cui and Yonatan Bisk and Roberto Mart{\\'\\i}n-Mart{\\'\\i}n and Yuke Zhu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=bU15EK0oqk}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bU15EK0oqk",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;0;1;2;3;4+1;1",
        "aff_unique_norm": ";University of Texas at Austin;University of California, Los Angeles;Carnegie Mellon University;Amazon",
        "aff_unique_dep": ";;;;Amazon.com, Inc.",
        "aff_unique_url": ";https://www.utexas.edu;https://www.ucla.edu;https://www.cmu.edu;https://www.amazon.com",
        "aff_unique_abbr": ";UT Austin;UCLA;CMU;Amazon",
        "aff_campus_unique_index": "1;1;1;2;1;1",
        "aff_campus_unique": ";Austin;Los Angeles",
        "aff_country_unique_index": "1;1;1;1;1;1+1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "bi8o9p6h2R",
        "title": "Improving Efficiency of Sampling-based Motion Planning via Message-Passing Monte Carlo",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Sampling-based motion planning methods, while effective in high-dimensional spaces, often suffer from inefficiencies due to irregular sampling distributions, leading to suboptimal exploration of the configuration space. In this paper, we propose an approach that enhances the efficiency of these methods by utilizing low-discrepancy distributions generated through Message-Passing Monte Carlo (MPMC). MPMC leverages Graph Neural Networks (GNNs) to generate point sets that uniformly cover the space, with uniformity assessed using the the $\\mathcal{L}_p$-discrepancy measure, which quantifies the irregularity of sample distributions. By improving the uniformity of the point sets, our approach significantly reduces computational overhead and the number of samples required for solving motion planning problems. Experimental results demonstrate that our method outperforms traditional sampling techniques in terms of planning efficiency.",
        "keywords": "Graph Neural Networks;Discrepancy Theory;Motion Planning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Makram Chahine;T. Konstantin Rusch;Zach J Patterson;Daniela Rus",
        "authorids": "~Makram_Chahine1;~T._Konstantin_Rusch1;~Zach_J_Patterson1;~Daniela_Rus1",
        "gender": "Not Specified;;M;F",
        "homepage": "https://www.mit.edu/~chahine/;https://camail.org/;https://cyphilab.github.io/;https://www.csail.mit.edu/person/daniela-rus",
        "dblp": "271/6229;266/1519;;r/DanielaRus",
        "google_scholar": "UzM0rckAAAAJ;9LajlSsAAAAJ;wDqCShMAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;0000-0003-4371-7442;",
        "linkedin": "mc8/;;;",
        "or_profile": "~Makram_Chahine1;~T._Konstantin_Rusch1;~Zach_J_Patterson1;~Daniela_Rus1",
        "aff": "Massachusetts Institute of Technology;Max Planck Institute for Intelligent Systems, Max-Planck Institute+ELLIS Institute T\u00fcbingen+Massachusetts Institute of Technology;Case Western Reserve University;Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;tuebingen.mpg.de+tue.ellis.eu+mit.edu;case.edu;mit.edu",
        "position": "PhD student;Assistant Professor+Principal Researcher+Postdoc;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nchahine2025improving,\ntitle={Improving Efficiency of Sampling-based Motion Planning via Message-Passing Monte Carlo},\nauthor={Makram Chahine and T. Konstantin Rusch and Zach J Patterson and Daniela Rus},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=bi8o9p6h2R}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bi8o9p6h2R",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1+2+0;3;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Max Planck Institute for Intelligent Systems;ELLIS Institute;Case Western Reserve University",
        "aff_unique_dep": ";Intelligent Systems;;",
        "aff_unique_url": "https://web.mit.edu;https://www.mpi-is.mpg.de;https://ellis.eu/;https://www.case.edu",
        "aff_unique_abbr": "MIT;MPI-IS;ELLIS;CWRU",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";T\u00fcbingen",
        "aff_country_unique_index": "0;1+1+0;0;0",
        "aff_country_unique": "United States;Germany"
    },
    {
        "id": "bmpDAqsJov",
        "title": "Latent Adaptive Planner for Dynamic Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "This paper presents Latent Adaptive Planner (LAP), a novel approach for dynamic nonprehensile manipulation tasks that formulates planning as latent space inference, effectively learned from human demonstration videos. \n    Our method addresses key challenges in visuomotor policy learning through a principled variational replanning framework that maintains temporal consistency while efficiently adapting to environmental changes. LAP employs Bayesian updating in latent space to incrementally refine plans as new observations become available, striking an optimal balance between computational efficiency and real-time adaptability. \n    We bridge the embodiment gap between humans and robots through model-based proportional mapping that regenerates accurate kinematic-dynamic joint states and object positions from human demonstrations. \n    Experimental evaluations across multiple complex manipulation benchmarks demonstrate that LAP achieves state-of-the-art performance, outperforming existing approaches in success rate, trajectory smoothness, and energy efficiency, particularly in dynamic adaptation scenarios. Our approach enables robots to perform complex interactions with human-like adaptability while providing an expandable framework applicable to diverse robotic platforms using the same human demonstration videos.",
        "keywords": "Imitation Learning;Robotics;Dynamic Nonprehensile Manipulation;Latent Space Planning;Classical Variational Bayes;Video-based Skill Acquisition",
        "primary_area": "",
        "supplementary_material": "/attachment/92e805c7c1e52709f5a79afc2d07b657b19727f4.zip",
        "author": "Donghun Noh;Deqian Kong;Minglu Zhao;Andrew Lizarraga;Jianwen Xie;Ying Nian Wu;Dennis Hong",
        "authorids": "~Donghun_Noh1;~Deqian_Kong1;~Minglu_Zhao1;~Andrew_Lizarraga1;~Jianwen_Xie1;~Ying_Nian_Wu1;~Dennis_Hong2",
        "gender": "M;M;;;;;",
        "homepage": ";https://sites.google.com/view/deqiankong/home;https://mingluzhao.github.io/;https://drewrl3v.github.io/;;;",
        "dblp": ";199/7131;;291/3769.html;;;",
        "google_scholar": "STfqARcAAAAJ;https://scholar.google.com/citations?hl=en;nrM4PzYAAAAJ;KUDS8uwAAAAJ;;;",
        "orcid": "0009-0008-4633-9153;;;0009-0009-3937-7925;;;",
        "linkedin": "donghunnoh/;;;andrew-lizarraga/;;;",
        "or_profile": "~Donghun_Noh1;~Deqian_Kong1;~Minglu_Zhao1;~Andrew_Lizarraga1;~Jianwen_Xie1;~Ying_Nian_Wu1;~Dennis_Hong2",
        "aff": "Dexterity, Inc;University of California, Los Angeles;University of California, Los Angeles;University of California, Los Angeles;;;",
        "aff_domain": "dexterity.ai;ucla.edu;ucla.edu;ucla.edu;;;",
        "position": "Robotics Engineer;PhD student;PhD student;PhD student;;;",
        "bibtex": "@inproceedings{\nnoh2025latent,\ntitle={Latent Adaptive Planner for Dynamic Manipulation},\nauthor={Donghun Noh and Deqian Kong and Minglu Zhao and Andrew Lizarraga and Jianwen Xie and Ying Nian Wu and Dennis Hong},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=bmpDAqsJov}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bmpDAqsJov",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;1;2;2;2",
        "aff_unique_norm": "Dexterity, Inc;University of California, Los Angeles;",
        "aff_unique_dep": ";;",
        "aff_unique_url": ";https://www.ucla.edu;",
        "aff_unique_abbr": ";UCLA;",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "brTSiML1nh",
        "title": "AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In this paper, we propose AimBot, a lightweight visual augmentation technique that provides explicit spatial cues to improve visuomotor policy learning in robotic manipulation. AimBot overlays shooting lines and scope reticles onto multi-view RGB images, offering auxiliary visual guidance that encodes the end-effector's state. The overlays are computed from depth images, camera extrinsics, and the current end-effector pose, explicitly conveying spatial relationships between the gripper and objects in the scene. AimBot incurs minimal computational overhead (less than 1 ms) and requires no changes to model architectures, as it simply replaces original RGB images with augmented counterparts. Despite its simplicity, our results show that AimBot consistently improves the performance of various visuomotor policies in both simulation and real-world settings, highlighting the benefits of spatially grounded visual feedback. More videos can be found at https://aimbot-reticle.github.io/",
        "keywords": "Robotic Manipulation;Visuomotor Policy;Imitation Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/22d47e32a123bbe303711bf32a3890b83a3c7a2d.zip",
        "author": "Yinpei Dai;Jayjun Lee;Yichi Zhang;Ziqiao Ma;Jianing Yang;Amir Zadeh;Chuan Li;Nima Fazeli;Joyce Chai",
        "authorids": "~Yinpei_Dai1;~Jayjun_Lee1;~Yichi_Zhang1;~Ziqiao_Ma1;~Jianing_Yang1;~Amir_Zadeh2;~Chuan_Li4;~Nima_Fazeli1;~Joyce_Chai2",
        "gender": "M;;M;Not Specified;M;;M;;",
        "homepage": "https://yinpeidai.github.io/;https://jayjunlee.github.io/;https://594zyc.github.io/;http://mars-tin.github.io/;https://jedyang.com/;;;https://www.mmintlab.com;",
        "dblp": "209/9564;;86/7054-1;287/7595-1.html;;;22/3837-1;;",
        "google_scholar": "EzAk5DUAAAAJ;OhompmUAAAAJ;xkBBhY8AAAAJ;WbybssYAAAAJ;https://scholar.google.com/citations?hl=en;;hoZesOwAAAAJ;;",
        "orcid": ";;0000-0003-3214-1070;0000-0002-0760-4638;;;;;",
        "linkedin": ";;yichi-zhang-354a83128/;;;;;;",
        "or_profile": "~Yinpei_Dai1;~Jayjun_Lee1;~Yichi_Zhang1;~Ziqiao_Ma1;~Jianing_Yang1;~Amir_Zadeh2;~Chuan_Li4;~Nima_Fazeli1;~Joyce_Chai2",
        "aff": "University of Michigan - Ann Arbor;University of Michigan - Ann Arbor+University of Michigan - Ann Arbor;University of Michigan;University of Michigan+International Business Machines+Adobe Research;Meta+University of Michigan - Ann Arbor;;Lambda;University of Michigan;",
        "aff_domain": "umich.edu;umich.edu+umich.edu;umich.edu;umich.edu+ibm.com+adobe.com;meta.com+umich.edu;;lambdalabs.com;umich.edu;",
        "position": "PhD student;PhD student+MS student;PhD student;PhD student+Research Intern+Research Intern;Intern+PhD student;;Researcher;Assistant Professor;",
        "bibtex": "@inproceedings{\ndai2025aimbot,\ntitle={AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies},\nauthor={Yinpei Dai and Jayjun Lee and Yichi Zhang and Ziqiao Ma and Jianing Yang and Amir Zadeh and Chuan Li and Nima Fazeli and Joyce Chai},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=brTSiML1nh}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=brTSiML1nh",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0+0;0;0+1+2;3+0;4;5;0;4",
        "aff_unique_norm": "University of Michigan;International Business Machines Corporation;Adobe;Meta;;Lambda",
        "aff_unique_dep": ";;Adobe Research;Meta Platforms, Inc.;;",
        "aff_unique_url": "https://www.umich.edu;https://www.ibm.com;https://research.adobe.com;https://meta.com;;",
        "aff_unique_abbr": "UM;IBM;Adobe;Meta;;",
        "aff_campus_unique_index": "0;0+0;;0",
        "aff_campus_unique": "Ann Arbor;",
        "aff_country_unique_index": "0;0+0;0;0+0+0;0+0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "bt1Tovn0SW",
        "title": "Training Strategies for Efficient Embodied Reasoning",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Robot chain-of-thought reasoning (CoT) -- wherein a model predicts helpful intermediate representations before choosing actions -- provides an effective method for improving the generalization and performance of robot policies, especially vision-language-action models (VLAs). While such approaches have been shown to improve performance and generalization, they suffer from core limitations, like needing specialized robot reasoning data and slow inference speeds. To design new robot reasoning approaches that address these issues, a more complete characterization of why reasoning helps policy performance is critical. We hypothesize several mechanisms by which robot reasoning improves policies -- (1) better representation learning, (2) improved learning curricularization, and (3) increased expressivity -- then devise simple variants of robot CoT reasoning to isolate and test each one. We find that learning to generate reasonings does lead to better VLA representations, while attending to the reasonings aids in actually leveraging these features for improved action prediction. Our results provide us with a better understanding of why CoT reasoning helps VLAs, which we use to introduce two simple and lightweight alternative recipes for robot reasoning. Our proposed approaches achieve significant performance gains over non-reasoning policies, state-of-the-art results on the LIBERO-90 benchmark, and a 3x inference speedup compared to standard robot reasoning.",
        "keywords": "robot reasoning;vision-language-action models",
        "primary_area": "",
        "supplementary_material": "/attachment/474f7822277274f751baca7030030ff3731bba97.zip",
        "author": "William Chen;Suneel Belkhale;Suvir Mirchandani;Karl Pertsch;Danny Driess;Oier Mees;Sergey Levine",
        "authorids": "~William_Chen1;~Suneel_Belkhale1;~Suvir_Mirchandani1;~Karl_Pertsch1;~Danny_Driess1;~Oier_Mees1;~Sergey_Levine1",
        "gender": "M;M;M;;;M;M",
        "homepage": ";https://github.com/suneelbelkhale;http://suvirpmirchandani.com;https://kpertsch.github.io/;https://dannydriess.github.io/;https://www.oiermees.com/;https://people.eecs.berkeley.edu/~svlevine/",
        "dblp": ";236/5069;287/4981;211/7137;;190/8659;80/7594",
        "google_scholar": "xUeq5EAAAAAJ;;fz7LJPIAAAAJ;https://scholar.google.com/citations?view_op=list_works;https://scholar.google.de/citations?user=wxnzyjwAAAAJ;https://scholar.google.de/citations?user=sgsLkM0AAAAJ;8R35rCwAAAAJ",
        "orcid": ";0000-0002-3963-7987;;;;;",
        "linkedin": "william-chen-a3956516b/;suneel-b-032b1a101/;;;;oier-mees-a3069488;",
        "or_profile": "~William_Chen1;~Suneel_Belkhale1;~Suvir_Mirchandani1;~Karl_Pertsch1;~Danny_Driess1;~Oier_Mees1;~Sergey_Levine1",
        "aff": "University of California, Berkeley;Stanford University;Stanford University;University of California, Berkeley+Stanford University;Physical Intelliigence;Electrical Engineering & Computer Science Department, University of California, Berkeley;University of California, Berkeley",
        "aff_domain": "berkeley.edu;stanford.edu;stanford.edu;berkeley.edu+stanford.edu;physicalintelligence.company;eecs.berkeley.edu;berkeley.edu",
        "position": "PhD student;PhD student;PhD student;Postdoc+Postdoc;Researcher;Postdoc;Associate Professor",
        "bibtex": "@inproceedings{\nchen2025training,\ntitle={Training Strategies for Efficient Embodied Reasoning},\nauthor={William Chen and Suneel Belkhale and Suvir Mirchandani and Karl Pertsch and Danny Driess and Oier Mees and Sergey Levine},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=bt1Tovn0SW}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=bt1Tovn0SW",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;0+1;2;0;0",
        "aff_unique_norm": "University of California, Berkeley;Stanford University;Physical Intelligence",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.stanford.edu;",
        "aff_unique_abbr": "UC Berkeley;Stanford;",
        "aff_campus_unique_index": "0;1;1;0+1;0;0",
        "aff_campus_unique": "Berkeley;Stanford;",
        "aff_country_unique_index": "0;0;0;0+0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "cUeY476ohd",
        "title": "MirrorDuo: Reflection-Consistent Visuomotor Learning from Mirrored Demonstration Pairs",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Image-based behaviour cloning leverages demonstrations captured from ubiquitous RGB cameras, enabling impressive visuomotor performance. However, it remains constrained by the cost of collecting sufficiently diverse demonstrations, especially for generalizing across workspace variations. We propose MirrorDuo, a mirroring-based formulation that operates on image, proprioception, and full 6-DoF end-effector action tuples, generating a mirrored counterpart for each original demonstration, effectively achieving ``collect one, get one for free.\" It can be applied as a data augmentation strategy for existing learning pipelines, such as standard behaviour cloning or diffusion policy, or as a structural prior for reflection-equivariant policy networks. By leveraging the overlap between the original and mirrored domains, MirrorDuo achieves significantly improved performance under the same data budget when demonstrations are evenly distributed across both sides of the workspace. When demonstrations are confined to one side, MirrorDuo enables efficient skill transfer to the mirrored workspace with as few as zero or just 5 demonstrations in the target arrangement.",
        "keywords": "Behavior Cloning;Data Efficiency;Robotic Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/7ac21d454036d4db519e1b88169c21e05cfa48a0.zip",
        "author": "Zheyu Zhuang;Ruiyu Wang;Giovanni Luca Marchetti;Florian T. Pokorny;Danica Kragic",
        "authorids": "~Zheyu_Zhuang1;~Ruiyu_Wang3;~Giovanni_Luca_Marchetti1;~Florian_T._Pokorny1;~Danica_Kragic1",
        "gender": "M;;M;;F",
        "homepage": ";;https://www.kth.se/profile/glma;;http://www.csc.kth.se/~danik",
        "dblp": ";;310/4949;;82/1211",
        "google_scholar": "https://scholar.google.se/citations?user=zHFrndgAAAAJ;;ePYa2qAAAAAJ;;",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Zheyu_Zhuang1;~Ruiyu_Wang3;~Giovanni_Luca_Marchetti1;~Florian_T._Pokorny1;~Danica_Kragic1",
        "aff": "KTH Royal Institute of Technology;;KTH Royal Institute of Technology;;KTH",
        "aff_domain": "kth.se;;kth.se;;kth.se",
        "position": "Postdoc;;Postdoc;;Professor",
        "bibtex": "@inproceedings{\nzhuang2025mirrorduo,\ntitle={MirrorDuo: Reflection-Consistent Visuomotor Learning from Mirrored Demonstration Pairs},\nauthor={Zheyu Zhuang and Ruiyu Wang and Giovanni Luca Marchetti and Florian T. Pokorny and Danica Kragic},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=cUeY476ohd}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cUeY476ohd",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;1;0",
        "aff_unique_norm": "KTH Royal Institute of Technology;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kth.se;",
        "aff_unique_abbr": "KTH;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Sweden;"
    },
    {
        "id": "cpmwi3Xwcr",
        "title": "RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Comprehensive, unbiased, and comparable evaluation of modern generalist policies is uniquely challenging: existing approaches for robot benchmarking typically rely on heavy standardization, either by specifying fixed evaluation tasks and environments, or by hosting centralized \"robot challenges\", and do not readily scale to evaluating generalist policies across a broad range of tasks and environments. In this work, we propose RoboArena, a new approach for scalable evaluation of generalist robot policies in the real world. Instead of standardizing evaluations around fixed tasks, environments, or locations, we propose to crowd-source evaluations across a distributed network of evaluators. Importantly, evaluators can freely choose the tasks and environments they evaluate on, enabling easy scaling of diversity, but they are required to perform double-blind evaluations over pairs of policies. Then, by aggregating preference feedback from pairwise comparisons across diverse tasks and environments, we can derive a ranking of policies. We instantiate our approach across a network of evaluators at seven academic institutions using the DROID robot platform. Through more than 600 pairwise real-robot evaluation episodes across seven generalist policies, we demonstrate that our crowd-sourced approach can more accurately rank the performance of existing generalist policies than conventional, centralized evaluation approaches, while being more scalable, resilient, and trustworthy. We open our evaluation network to the community and hope that it can enable more accessible comparisons of generalist robot policies.",
        "keywords": "Generalist Robot Policy Evaluation;Crowd-Sourced Evaluation;Robot Foundation Models",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Pranav Atreya;Karl Pertsch;Tony Lee;Moo Jin Kim;Arhan Jain;Artur Kuramshin;Cyrus Neary;Edward S. Hu;Kanav Arora;Kirsty Ellis;Luca Macesanu;Matthew Leonard;Meedeum Cho;Ozgur Aslan;Shivin Dass;Tony Wang;Xingfang Yuan;Abhishek Gupta;Dinesh Jayaraman;Glen Berseth;Kostas Daniilidis;Roberto Mart\u00edn-Mart\u00edn;Youngwoon Lee;Percy Liang;Chelsea Finn;Sergey Levine",
        "authorids": "~Pranav_Atreya1;~Karl_Pertsch1;~Tony_Lee1;~Moo_Jin_Kim1;~Arhan_Jain1;~Artur_Kuramshin1;~Cyrus_Neary1;~Edward_S._Hu1;~Kanav_Arora1;kirsty.ellis@mila.quebec;~Luca_Macesanu1;~Matthew_Leonard1;~Meedeum_Cho1;~Ozgur_Aslan1;~Shivin_Dass2;tonyw3@seas.upenn.edu;xingfy@seas.upenn.edu;~Abhishek_Gupta1;~Dinesh_Jayaraman2;~Glen_Berseth1;~Kostas_Daniilidis1;~Roberto_Mart\u00edn-Mart\u00edn1;~Youngwoon_Lee1;~Percy_Liang1;~Chelsea_Finn1;~Sergey_Levine1",
        "gender": ";;M;M;M;M;M;M;;;;M;M;;;;;M;M;M;M;M;M;;F;M",
        "homepage": "https://pranavatreya.github.io;https://kpertsch.github.io/;;https://moojink.com;;https://akuramshin.github.io/;https://www.cyrusneary.com/;https://www.edwardshu.com;;;;;https://chomeed.github.io;;;;;https://homes.cs.washington.edu/~abhgupta/;https://www.seas.upenn.edu/~dineshj/;http://fracturedplane.com/;http://www.cis.upenn.edu/~kostas;https://robertomartinmartin.com/;https://youngwoon.github.io;https://cs.stanford.edu/~pliang/;https://ai.stanford.edu/~cbfinn/;https://people.eecs.berkeley.edu/~svlevine/",
        "dblp": "317/4655;211/7137;;;367/5031;;269/9716.html;245/4627;;;;;;;;;;18/6404-4;145/3870;147/5478;d/KostasDaniilidis;153/7670;117/4767;04/1701;131/1783;80/7594",
        "google_scholar": "bQowYEYAAAAJ;https://scholar.google.com/citations?view_op=list_works;OYNdx48AAAAJ;ZKRs0oEAAAAJ;https://scholar.google.com/citations?hl=en;yHkHCLsAAAAJ;z4JrPP0AAAAJ;;;;;;;;;;;1wLVDP4AAAAJ;QxLpghAAAAAJ;https://scholar.google.ca/citations?user=-WZcuuwAAAAJ;dGs2BcIAAAAJ;XOJE8OEAAAAJ;CDPa3AgAAAAJ;pouyVyUAAAAJ;vfPE6hgAAAAJ;8R35rCwAAAAJ",
        "orcid": ";;;;;;0000-0002-5293-5663;;;;;;;;;;;;0000-0002-6888-3095;0000-0001-7351-8028;0000-0003-0498-0758;0000-0002-9586-2759;0000-0001-9918-1056;;;",
        "linkedin": "pranav-d-atreya;;tonyhlee/;moojink/;;artur-kuramshin-4b926616a/;;;kanav1arora/;;luca-macesanu/;matthew-leonard-509aa4216/;;;;;;;dinesh-jayaraman-44b31539/;glen-berseth-0523278b?trk=hp-identity-name;;;;;;",
        "or_profile": "~Pranav_Atreya1;~Karl_Pertsch1;~Tony_Lee1;~Moo_Jin_Kim1;~Arhan_Jain1;~Artur_Kuramshin1;~Cyrus_Neary1;~Edward_S._Hu1;~Kanav_Arora1;kirsty.ellis@mila.quebec;~Luca_Macesanu1;~Matthew_Leonard1;~Meedeum_Cho1;~Ozgur_Aslan1;~Shivin_Dass2;tonyw3@seas.upenn.edu;xingfy@seas.upenn.edu;~Abhishek_Gupta1;~Dinesh_Jayaraman2;~Glen_Berseth1;~Kostas_Daniilidis1;~Roberto_Mart\u00edn-Mart\u00edn1;~Youngwoon_Lee1;~Percy_Liang1;~Chelsea_Finn1;~Sergey_Levine1",
        "aff": "University of California, Berkeley;University of California, Berkeley+Stanford University;Stanford University;Stanford University;;Universit\u00e9 de Montr\u00e9al;University of British Columbia+Mila - Quebec Artificial Intelligence Institute;University of Pennsylvania;University of Washington;;University of Texas at Austin;University of Pennsylvania;Yonsei University;;;;;University of Washington;University of Pennsylvania;University of Montreal+Montreal Institute for Learning Algorithms, University of Montreal, Universit\u00e9 de Montr\u00e9al;Athena Research and Innovation Centre+University of Pennsylvania;Amazon+University of Texas at Austin;Yonsei University;Stanford University;Physical Intelligence+Stanford University;University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu+stanford.edu;stanford.edu;stanford.edu;;umontreal.ca;ubc.ca+mila.quebec;upenn.edu;cs.uw.edu;;utexas.edu;seas.upenn.edu;yonsei.ac.kr;;;;;uw.edu;upenn.edu;iro.umontreal.ca+mila.umontreal.ca;athenarc.gr+upenn.edu;amazon.com+utexas.edu;yonsei.ac.kr;stanford.edu;physicalintelligence.company+stanford.edu;berkeley.edu",
        "position": "PhD student;Postdoc+Postdoc;PhD student;PhD student;;MS student;Assistant Professor+Postdoc;PhD student;Undergrad student;;Undergrad student;PhD student;Undergrad student;;;;;Assistant Professor;Assistant Professor;Assistant Professor+Assistant Professor;Affiliated Researcher+Full Professor;Researcher+Assistant Professor;Assistant Professor;Associate Professor;Researcher+Assistant Professor;Associate Professor",
        "bibtex": "@inproceedings{\natreya2025roboarena,\ntitle={RoboArena: Distributed Real-World Evaluation of Generalist Robot Policies},\nauthor={Pranav Atreya and Karl Pertsch and Tony Lee and Moo Jin Kim and Arhan Jain and Artur Kuramshin and Cyrus Neary and Edward S. Hu and Kanav Arora and Kirsty Ellis and Luca Macesanu and Matthew Leonard and Meedeum Cho and Ozgur Aslan and Shivin Dass and Tony Wang and Xingfang Yuan and Abhishek Gupta and Dinesh Jayaraman and Glen Berseth and Kostas Daniilidis and Roberto Mart{\\'\\i}n-Mart{\\'\\i}n and Youngwoon Lee and Percy Liang and Chelsea Finn and Sergey Levine},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=cpmwi3Xwcr}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=cpmwi3Xwcr",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            26,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0+1;1;1;2;3;4+5;6;7;2;8;6;9;2;2;2;2;7;6;10+10;11+6;12+8;9;1;13+1;0",
        "aff_unique_norm": "University of California, Berkeley;Stanford University;;Universit\u00e9 de Montr\u00e9al;University of British Columbia;Quebec Artificial Intelligence Institute;University of Pennsylvania;University of Washington;University of Texas at Austin;Yonsei University;University of Montreal;Athena Research and Innovation Centre;Amazon;Physical Intelligence",
        "aff_unique_dep": ";;;;;Artificial Intelligence;;;;;;;Amazon.com, Inc.;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.stanford.edu;;https://www.umontreal.ca;https://www.ubc.ca;https://mila.quebec;https://www.upenn.edu;https://www.washington.edu;https://www.utexas.edu;https://www.yonsei.ac.kr;https://wwwumontreal.ca;https://www.athena rc.gr;https://www.amazon.com;",
        "aff_unique_abbr": "UC Berkeley;Stanford;;UdeM;UBC;Mila;UPenn;UW;UT Austin;Yonsei;UM;;Amazon;",
        "aff_campus_unique_index": "0;0+1;1;1;;3;4;;3;1;1;0",
        "aff_campus_unique": "Berkeley;Stanford;;Austin;Montreal",
        "aff_country_unique_index": "0;0+0;0;0;2;2+2;0;0;0;0;3;0;0;2+2;4+0;0+0;3;0;0;0",
        "aff_country_unique": "United States;;Canada;South Korea;Greece"
    },
    {
        "id": "dBaSaa7qi4",
        "title": "CoRI: Communication of Robot Intent for Physical Human-Robot Interaction",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Clear communication of robot intent fosters transparency and interpretability in physical human-robot interaction (pHRI), particularly during assistive tasks involving direct human-robot contact. We introduce CoRI, a pipeline that automatically generates natural language communication of a robot's upcoming actions directly from its motion plan and visual perception. Our pipeline first processes the robot's image view to identify human poses and key environmental features. It then encodes the planned 3D spatial trajectory (including velocity and force) onto this view, visually grounding the path and its dynamics. CoRI queries a vision-language model with this visual representation to interpret the planned action within the visual context before generating concise, user-directed statements, without relying on task-specific information. Results from a user study involving robot-assisted feeding, bathing, and shaving tasks across two different robots indicate that CoRI leads to statistically significant difference in communication clarity compared to a baseline communication strategy. Specifically, CoRI effectively conveys not only the robot's high-level intentions but also crucial details about its motion and any collaborative user action needed.",
        "keywords": "Robot intent generation;Human-robot communication;Assistive robotics",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Junxiang Wang;Emek Bar\u0131\u015f K\u00fc\u00e7\u00fcktabak;Rana Soltani Zarrin;Zackory Erickson",
        "authorids": "~Junxiang_Wang5;baris_kucuktabak@honda-ri.com;~Rana_Soltani_Zarrin1;~Zackory_Erickson1",
        "gender": "M;;F;M",
        "homepage": "https://jimwang418.github.io/;;;https://zackory.com",
        "dblp": ";;;",
        "google_scholar": "K7Zzm7YAAAAJ;;7NGxvsgAAAAJ;wElkTtIAAAAJ",
        "orcid": "0000-0002-7264-2489;;;",
        "linkedin": "junxiang-jim-wang/;;;",
        "or_profile": "~Junxiang_Wang5;baris_kucuktabak@honda-ri.com;~Rana_Soltani_Zarrin1;~Zackory_Erickson1",
        "aff": "Carnegie Mellon University;;Honda Research Institution US;Carnegie Mellon University",
        "aff_domain": "cmu.edu;;honda-ri.com;cmu.edu",
        "position": "PhD student;;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025cori,\ntitle={Co{RI}: Communication of Robot Intent for Physical Human-Robot Interaction},\nauthor={Junxiang Wang and Emek Bar{\\i}{\\c{s}} K{\\\"u}{\\c{c}}{\\\"u}ktabak and Rana Soltani Zarrin and Zackory Erickson},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=dBaSaa7qi4}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=dBaSaa7qi4",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;0",
        "aff_unique_norm": "Carnegie Mellon University;;Honda Research Institute",
        "aff_unique_dep": ";;Honda Research Institute",
        "aff_unique_url": "https://www.cmu.edu;;https://honda-ri.com",
        "aff_unique_abbr": "CMU;;HRI",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "dT45OMevL5",
        "title": "3DS-VLA: A 3D Spatial-Aware Vision Language Action Model for Robust Multi-Task Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Recently, 2D vision-language-action (VLA) models have made significant strides in multi-task manipulation. However, these models struggle to reason about 3D spatial relationships from 2D image inputs. Although an increasing number of 3D approaches explicitly integrate 3D information, they encounter challenges such as limited availability of large-scale 3D datasets and loss of spatial information during input processing. Meanwhile, existing policies typically focus on the perception-to-action learning paradigm, lacking an explicit understanding of the spatial and temporal relationships between the robot and its environment. To address this, we propose 3DS-VLA, which enhances pretrained 2D vision-language models (VLMs) with comprehensive 3D awareness, enabling the prediction of robust end-effector poses.\nSpecifically, we enable a 2D vision encoder to encode both 2D images and 3D spatial observation by introducing a 2D-to-3D positional alignment mechanism. This allows 3DS-VLA to leverage the large-scale pre-trained knowledge of the VLM for effective reasoning in complex 3D robotic environments. Furthermore, to better understand the spatiotemporal relationship between 3D observations and robot behavior, we guide the model to learn the introduced sequential 3D spatial constraints, which define affordance-relevant 3D keypoints on objects, ensuring robust interactions. Experiments in simulated and real-world demonstrate that 3DS-VLA outperforms previous state-of-the-art policies and showcase its generalizable capabilities across multi-task, multi-embodiment, and diverse environmental settings.",
        "keywords": "Vision-Language-Action; Robotic Manipulation; Imitation Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/f4040f62d3771aa7682b3aa3f60d80d2ed6ca04d.zip",
        "author": "Xiaoqi Li;Liang Heng;Jiaming Liu;Yan Shen;Chenyang Gu;Zhuoyang Liu;Hao Chen;Nuowei Han;Renrui Zhang;Hao Tang;Shanghang Zhang;Hao Dong",
        "authorids": "~Xiaoqi_Li3;~Liang_Heng1;~Jiaming_Liu2;~Yan_Shen3;~Chenyang_Gu1;~Zhuoyang_Liu1;~Hao_Chen38;~Nuowei_Han1;~Renrui_Zhang1;~Hao_Tang6;~Shanghang_Zhang4;~Hao_Dong3",
        "gender": ";;M;;M;M;M;M;M;M;;M",
        "homepage": ";;https://github.com/liujiaming1996;;https://gaystarc.github.io;https://minifranka.github.io/;https://chen-h01.github.io/;https://github.com/HNW-HAN;;https://ha0tang.github.io/;;https://zsdonghao.github.io",
        "dblp": ";;;;;;;;244/1748;07/5751-5;;14/1525-3.html",
        "google_scholar": ";;cPki5sUAAAAJ;;liLQ5twAAAAJ;WfZvYgoAAAAJ;https://scholar.google.com.hk/citations?hl=zh-CN;;YlL3xN4AAAAJ;9zJkeEMAAAAJ;;xLFL4sMAAAAJ",
        "orcid": ";;0000-0002-6770-4390;;;;;;;0000-0002-2077-1246;;0000-0003-2261-9122",
        "linkedin": ";;;;;;;;;hao-tang-887475138/;;",
        "or_profile": "~Xiaoqi_Li3;~Liang_Heng1;~Jiaming_Liu2;~Yan_Shen3;~Chenyang_Gu1;~Zhuoyang_Liu1;~Hao_Chen38;~Nuowei_Han1;~Renrui_Zhang1;~Hao_Tang6;~Shanghang_Zhang4;~Hao_Dong3",
        "aff": ";;Peking University;;Peking University;Peking University;Department of Computer Science and Engineering, The Chinese University of Hong Kong;Beijing University of Posts and Telecommunications;MMLab of CUHK & Shanghai AI Laboratory;Peking University;;Peking University+Peking University",
        "aff_domain": ";;pku.edu.cn;;stu.pku.edu.cn;stu.pku.edu.cn;cse.cuhk.edu.hk;bupt.edu.cn;pjlab.org.cn;pku.edu.cn;;pku.edu.cn+pku.edu.cn",
        "position": ";;PhD student;;Undergrad student;Undergrad student;PhD student;MS student;PhD student;Assistant Professor;;Associate Professor+Assistant Professor",
        "bibtex": "@inproceedings{\nli2025dsvla,\ntitle={3{DS}-{VLA}: A 3D Spatial-Aware Vision Language Action Model for Robust Multi-Task Manipulation},\nauthor={Xiaoqi Li and Liang Heng and Jiaming Liu and Yan Shen and Chenyang Gu and Zhuoyang Liu and Hao Chen and Nuowei Han and Renrui Zhang and Hao Tang and Shanghang Zhang and Hao Dong},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=dT45OMevL5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=dT45OMevL5",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            12,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;1;1;2;3;2;1;0;1+1",
        "aff_unique_norm": ";Peking University;Chinese University of Hong Kong;Beijing University of Posts and Telecommunications",
        "aff_unique_dep": ";;Department of Computer Science and Engineering;",
        "aff_unique_url": ";http://www.pku.edu.cn;https://www.cuhk.edu.hk;http://www.bupt.edu.cn/",
        "aff_unique_abbr": ";Peking U;CUHK;BUPT",
        "aff_campus_unique_index": "1;2;1;",
        "aff_campus_unique": ";Hong Kong SAR;Beijing",
        "aff_country_unique_index": "1;1;1;1;1;1;1;1+1",
        "aff_country_unique": ";China"
    },
    {
        "id": "dmXFboqSnX",
        "title": "Learning Impact-Rich Rotational Maneuvers via Centroidal Velocity Rewards and Sim-to-Real Techniques: A One-Leg Hopper Flip Case Study",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Dynamic rotational maneuvers, such as front flips, inherently involve large angular momentum generation and intense impact forces, presenting major challenges for reinforcement learning and sim-to-real transfer. In this work, we propose a general framework for learning and deploying impact-rich, rotation-intensive behaviors through centroidal velocity-based rewards and actuator-aware sim-to-real techniques. We identify that conventional link-level reward formulations fail to induce true whole-body rotation and introduce a centroidal angular velocity reward that accurately captures system-wide rotational dynamics. To bridge the sim-to-real gap under extreme conditions, we model motor operating regions (MOR) and apply transmission load regularization to ensure realistic torque commands and mechanical robustness. Using the one-leg hopper front flip as a representative case study, we demonstrate the first successful hardware realization of a full front flip. Our results highlight that incorporating centroidal dynamics and actuator constraints is critical for reliably executing highly dynamic motions.",
        "keywords": "Reinforcement Learning;Sim-to-Real Transfer;One-Leg Hopper",
        "primary_area": "",
        "supplementary_material": "/attachment/45f1baa7b387af2c626a3c1a13aa359fc3f98885.zip",
        "author": "Dongyun Kang;Gijeong Kim;JongHun Choe;Hajun Kim;Hae-Won Park",
        "authorids": "~Dongyun_Kang1;~Gijeong_Kim1;~JongHun_Choe1;~Hajun_Kim1;~Hae-Won_Park2",
        "gender": "M;M;;M;",
        "homepage": ";;;https://www.dynamicrobot.kaist.ac.kr/;https://www.dynamicrobot.kaist.ac.kr/",
        "dblp": ";;;;",
        "google_scholar": "1Do3jVUAAAAJ;ESRu5ygAAAAJ;;;q7v_ewQAAAAJ",
        "orcid": "0009-0002-3922-4063;0009-0002-9293-3504;;;",
        "linkedin": ";;;;",
        "or_profile": "~Dongyun_Kang1;~Gijeong_Kim1;~JongHun_Choe1;~Hajun_Kim1;~Hae-Won_Park2",
        "aff": "Korea Advanced Institute of Science & Technology;Korea Advanced Institute of Science & Technology;;Korea Advanced Institute of Science & Technology;Korea Advanced Institute of Science & Technology",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;;kaist.ac.kr;kaist.ac.kr",
        "position": "PhD student;PhD student;;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nkang2025learning,\ntitle={Learning Impact-Rich Rotational Maneuvers via Centroidal Velocity Rewards and Sim-to-Real Techniques: A One-Leg Hopper Flip Case Study},\nauthor={Dongyun Kang and Gijeong Kim and JongHun Choe and Hajun Kim and Hae-Won Park},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=dmXFboqSnX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=dmXFboqSnX",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.kaist.ac.kr;",
        "aff_unique_abbr": "KAIST;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "South Korea;"
    },
    {
        "id": "eLeCrM5PEO",
        "title": "Self-supervised perception for tactile skin covered dexterous hands",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We present PercepSkin, a pre-trained encoder for magnetic skin sensors distributed across the fingertips, phalanges, and palm of a dexterous robot hand. \nMagnetic tactile skins offer a flexible form factor for hand-wide coverage with fast response times, in contrast to vision-based tactile sensors that are restricted to the fingertips and limited by bandwidth. Full hand tactile perception is crucial for robot dexterity. However, a lack of general-purpose models, challenges with interpreting magnetic flux and calibration have limited the adoption of these sensors.\nPercepSkin, given a history of kinematic and tactile sensing across a hand, outputs a latent tactile embedding that can be used in any downstream task. The encoder is self-supervised via self-distillation on a variety of unlabeled hand-object  interactions using an Allegro hand sensorized with Xela uSkin.\nIn experiments across several benchmark tasks, from state estimation to policy learning, we find that pretrained PercepSkin representations are both sample efficient in learning downstream tasks and improve task performance by over 41% compared to prior work and over 56% compared to end-to-end learning.",
        "keywords": "Robot Perception;Sensing & Vision;Representation learning;Foundation models;Tactile sensing",
        "primary_area": "",
        "supplementary_material": "/attachment/b91ac81984bf9527026ce8921b9b16aa9498f86c.zip",
        "author": "Akash Sharma;Carolina Higuera;Chaithanya Krishna Bodduluri;Zixi Liu;Taosha Fan;Tess Hellebrekers;Mike Lambeta;Byron Boots;Michael Kaess;Tingfan Wu;Francois Robert Hogan;Mustafa Mukadam",
        "authorids": "~Akash_Sharma1;~Carolina_Higuera1;~Chaithanya_Krishna_Bodduluri1;~Zixi_Liu1;~Taosha_Fan1;~Tess_Hellebrekers2;~Mike_Lambeta1;~Byron_Boots1;~Michael_Kaess1;~Tingfan_Wu2;~Francois_Robert_Hogan1;~Mustafa_Mukadam1",
        "gender": "M;F;M;F;;;M;;M;M;;M",
        "homepage": "https://akashsharma02.github.io;;;;https://github.com/fantaosha;https://tesshellebrekers.com;;;https://www.cs.cmu.edu/~kaess/;;https://fhogan.github.io;http://www.mustafamukadam.com",
        "dblp": ";;;;;;;;26/6036;;190/7406.html;",
        "google_scholar": "LhKc2CsAAAAJ;https://scholar.google.es/citations?hl=es;;4eeVPBoAAAAJ;;;;;27eupmsAAAAJ;https://scholar.google.com/citations?hl=en;;yYpm9LoAAAAJ",
        "orcid": ";0000-0001-5141-0817;;;;;;;0000-0002-7590-3357;;;",
        "linkedin": ";;krishna-bck;;;;mike-maroje-lambeta;;michaelkaess/;;;mhmukadam/",
        "or_profile": "~Akash_Sharma1;~Carolina_Higuera1;~Chaithanya_Krishna_Bodduluri1;~Zixi_Liu1;~Taosha_Fan1;~Tess_Hellebrekers2;~Mike_Lambeta1;~Byron_Boots1;~Michael_Kaess1;~Tingfan_Wu2;~Francois_Robert_Hogan1;~Mustafa_Mukadam1",
        "aff": "Meta Facebook+Carnegie Mellon University;University of Washington;Meta Facebook;;;;Meta;;Carnegie Mellon University;;Meta Facebook;Amazon Robotics",
        "aff_domain": "meta.com+cs.cmu.edu;uw.edu;meta.com;;;;meta.com;;cmu.edu;;meta.com;amazon.com",
        "position": "Researcher+PhD student;PhD student;Researcher;;;;Engineer;;Associate Professor;;Researcher;Researcher",
        "bibtex": "@inproceedings{\nsharma2025selfsupervised,\ntitle={Self-supervised perception for tactile skin covered dexterous hands},\nauthor={Akash Sharma and Carolina Higuera and Chaithanya Krishna Bodduluri and Zixi Liu and Taosha Fan and Tess Hellebrekers and Mike Lambeta and Byron Boots and Michael Kaess and Tingfan Wu and Francois Robert Hogan and Mustafa Mukadam},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=eLeCrM5PEO}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=eLeCrM5PEO",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            12,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;2;0;3;3;3;0;3;1;3;0;4",
        "aff_unique_norm": "Meta;Carnegie Mellon University;University of Washington;;Amazon",
        "aff_unique_dep": "Meta Platforms, Inc.;;;;Amazon Robotics",
        "aff_unique_url": "https://meta.com;https://www.cmu.edu;https://www.washington.edu;;https://www.amazonrobotics.com",
        "aff_unique_abbr": "Meta;CMU;UW;;Amazon Robotics",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "ejV8YHTpHR",
        "title": "Leveraging Correlation Across Test Platforms for Variance-Reduced Metric Estimation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Learning-based robotic systems demand rigorous validation to assure reliable performance, but extensive real\u2010world testing is often prohibitively expensive and if conducted may still yield insufficient data for high-confidence guarantees. In this work, we introduce a general estimation framework that leverages *paired* data across test platforms, e.g., paired simulation and real\u2010world observations, to achieve better estimates of real-world metrics via the method of control variates. By incorporating cheap and abundant auxiliary measurements (for example, simulator outputs) as control variates for costly real\u2010world samples, our method provably reduces the variance of Monte Carlo estimates and thus requires significantly fewer real\u2010world samples to attain a specified confidence bound on the mean performance. We provide theoretical analysis characterizing the variance and sample-efficiency improvement, and demonstrate empirically in autonomous driving and quadruped robotics settings that our approach achieves high\u2010probability bounds with markedly reduced sample complexity. Our technique can lower the real\u2010world testing burden for validating the performance of the stack, thereby enabling more efficient and cost\u2010effective experimental evaluation of robotic systems.",
        "keywords": "metric estimation;sample efficiency;control variates",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Rachel Luo;Heng Yang;Michael Watson;Apoorva Sharma;Sushant Veer;Edward Schmerling;Marco Pavone",
        "authorids": "~Rachel_Luo1;~Heng_Yang4;mwatson@nvidia.com;~Apoorva_Sharma1;~Sushant_Veer1;~Edward_Schmerling1;~Marco_Pavone1",
        "gender": "F;M;;M;M;M;M",
        "homepage": "https://rsluo.github.io/;https://hankyang.seas.harvard.edu/;;https://web.stanford.edu/~apoorva;;;https://web.stanford.edu/~pavone/",
        "dblp": "182/0443;83/415-2;;181/4231;173/5950;143/7326;91/3382-1.html",
        "google_scholar": "9TPpYBMAAAAJ;GuKEDfixZqsC;;3bBgnTIAAAAJ;1FiIlQsAAAAJ;b4Kj6MIAAAAJ;RhOpyXcAAAAJ",
        "orcid": ";;;;;;",
        "linkedin": ";;;;;;",
        "or_profile": "~Rachel_Luo1;~Heng_Yang4;mwatson@nvidia.com;~Apoorva_Sharma1;~Sushant_Veer1;~Edward_Schmerling1;~Marco_Pavone1",
        "aff": "NVIDIA;Harvard University+NVIDIA;;NVIDIA;NVIDIA;NVIDIA;NVIDIA+Stanford University",
        "aff_domain": "nvidia.com;seas.harvard.edu+nvidia.com;;nvidia.com;nvidia.com;nvidia.com;nvidia.com+stanford.edu",
        "position": "Researcher;Assistant Professor+Researcher;;Researcher;Researcher;Researcher;Director, Autonomous Vehicle Research+Associate Professor",
        "bibtex": "@inproceedings{\nluo2025leveraging,\ntitle={Leveraging Correlation Across Test Platforms for Variance-Reduced Metric Estimation},\nauthor={Rachel Luo and Heng Yang and Michael Watson and Apoorva Sharma and Sushant Veer and Edward Schmerling and Marco Pavone},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ejV8YHTpHR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ejV8YHTpHR",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1+0;2;0;0;0;0+3",
        "aff_unique_norm": "NVIDIA;Harvard University;;Stanford University",
        "aff_unique_dep": "NVIDIA Corporation;;;",
        "aff_unique_url": "https://www.nvidia.com;https://www.harvard.edu;;https://www.stanford.edu",
        "aff_unique_abbr": "NVIDIA;Harvard;;Stanford",
        "aff_campus_unique_index": ";1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0;0+0;0;0;0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "etSYDtRO0Z",
        "title": "ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Generative models based on flow matching offer significant potential for learning robot policies, particularly in generating high-dimensional, dexterous behaviors that are conditioned on diverse observations. In this work, we introduce ManiFlow, an advanced flow matching model specifically designed to support dexterous manipulation tasks. ManiFlow improves over flow matching both in the learning procedure and in the model architecture, resulting in better robustness and efficacy. It consistently exhibits strong generalization capabilities, outperforming existing state-of-the-art robot learning methods on a wide range of benchmarks. We also demonstrate the powerful capabilities of ManiFlow in solving complex bimanual dexterous manipulation challenges.",
        "keywords": "Imitation Learning;Dexterous Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/9c191228e94c83a1641d9c49bc52c094120e479c.zip",
        "author": "Ge Yan;Jiyue Zhu;Yuquan Deng;Shiqi Yang;Ri-Zhao Qiu;Xuxin Cheng;Marius Memmel;Ranjay Krishna;Ankit Goyal;Xiaolong Wang;Dieter Fox",
        "authorids": "~Ge_Yan3;~Jiyue_Zhu1;~Yuquan_Deng1;~Shiqi_Yang2;~Ri-Zhao_Qiu1;~Xuxin_Cheng2;~Marius_Memmel1;~Ranjay_Krishna1;~Ankit_Goyal1;~Xiaolong_Wang3;~Dieter_Fox1",
        "gender": "Not Specified;Not Specified;;M;Not Specified;M;M;M;M;M;M",
        "homepage": "https://geyan21.github.io/;https://jiyuezh.github.io/;;https://aaronyang1223.github.io/;https://rogerqi.github.io/;https://chengxuxin.github.io;https://memmelma.github.io/;http://ranjaykrishna.com;http://imankgoyal.github.io/;https://xiaolonw.github.io/;https://homes.cs.washington.edu/~fox/",
        "dblp": "169/8155-6;;;;336/5470;;297/5209;167/3785;89/10051-1;91/952-4;f/DieterFox",
        "google_scholar": "ma7qW2kAAAAJ;;;OQQzJb4AAAAJ;uH0re54AAAAJ;Z8vhOxYAAAAJ;FoIK-M0AAAAJ;IcqahyAAAAAJ;RhN6jKIAAAAJ;Y8O9N_0AAAAJ;DqXsbPAAAAAJ",
        "orcid": ";0000-0002-1303-562X;;0009-0009-8529-4522;;;;0000-0001-8784-2531;;;",
        "linkedin": "ge-yan/;jiyue-zhu/;;;rizhaoqiu/;;marius-memmel-333138153/;ranjay-krishna-1a344444/;;;",
        "or_profile": "~Ge_Yan3;~Jiyue_Zhu1;~Yuquan_Deng1;~Shiqi_Yang2;~Ri-Zhao_Qiu1;~Xuxin_Cheng2;~Marius_Memmel1;~Ranjay_Krishna1;~Ankit_Goyal1;~Xiaolong_Wang3;~Dieter_Fox1",
        "aff": "Department of Computer Science, University of Washington;University of California, San Diego;;University of California, San Diego;University of California, San Diego;University of California, San Diego;University of Washington;University of Washington;NVIDIA;University of California, San Diego;NVIDIA Research+Department of Computer Science",
        "aff_domain": "cs.washington.edu;ucsd.edu;;ucsd.edu;ucsd.edu;ucsd.edu;cs.washington.edu;cs.washington.edu;nvidia.com;ucsd.edu;nvidia.com+cs.washington.edu",
        "position": "PhD student;MS student;;MS student;PhD student;PhD student;PhD student;Assistant Professor;Researcher;Assistant Professor;Senior Director of Robotics Research+Full Professor",
        "bibtex": "@inproceedings{\nyan2025maniflow,\ntitle={ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training},\nauthor={Ge Yan and Jiyue Zhu and Yuquan Deng and Shiqi Yang and Ri-Zhao Qiu and Xuxin Cheng and Marius Memmel and Ranjay Krishna and Ankit Goyal and Xiaolong Wang and Dieter Fox},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=etSYDtRO0Z}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=etSYDtRO0Z",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;1;1;1;0;0;3;1;3+4",
        "aff_unique_norm": "University of Washington;University of California, San Diego;;NVIDIA;Unknown Institution",
        "aff_unique_dep": "Department of Computer Science;;;NVIDIA Corporation;Department of Computer Science",
        "aff_unique_url": "https://www.washington.edu;https://www.ucsd.edu;;https://www.nvidia.com;",
        "aff_unique_abbr": "UW;UCSD;;NVIDIA;",
        "aff_campus_unique_index": "0;1;1;1;1;1;",
        "aff_campus_unique": "Seattle;San Diego;",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "f2Y549UzM5",
        "title": "Cost-aware Discovery of Contextual Failures using Bayesian Active Learning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Ensuring the robustness of robotic systems is crucial for their deployment in safety-critical domains. Failure discovery, or falsification, is a widely used approach for evaluating robustness, with recent advancements focusing on improving sample efficiency and generalization through probabilistic sampling techniques and learning-theoretic approaches. However, existing methods typically rely on explicitly defined analytical cost functions to characterize failures, often overlooking the underlying causes and diversity of discovered failure scenarios. In this work, we propose a novel failure discovery framework that integrates contextual reasoning in the falsification process, specifically tailored for high evaluation-cost applications. Our method incorporates expert-in-the-loop feedback to construct a probabilistic surrogate model of failures using Bayesian inference. This model is iteratively refined and leveraged to guide an active learning strategy that prioritizes the discovery of diverse failure cases. We empirically validate our approach across a range of tasks for high-cost contextual falsification in robotic manipulation and autonomous driving.",
        "keywords": "Failure discovery;Testing;Contextual failures",
        "primary_area": "",
        "supplementary_material": "/attachment/b6f0f40f857d581ef2644aa27d669cab59679078.zip",
        "author": "Anjali Parashar;Joseph Zhang;Yingke Li;Chuchu Fan",
        "authorids": "~Anjali_Parashar1;~Joseph_Zhang1;~Yingke_Li1;~Chuchu_Fan2",
        "gender": "F;;F;F",
        "homepage": "https://anjalip.mit.edu/;;https://yingkeli.me;https://chuchu.mit.edu",
        "dblp": "319/5798;;;127/1756",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;;J-dq_8EAAAAJ",
        "orcid": "0009-0002-2206-1165;;;",
        "linkedin": "anjali-parashar-434b66151;jzha/;;chuchu-fan/",
        "or_profile": "~Anjali_Parashar1;~Joseph_Zhang1;~Yingke_Li1;~Chuchu_Fan2",
        "aff": "Massachusetts Institute of Technology;Massachusetts Institute of Technology+Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu+mit.edu;mit.edu;mit.edu",
        "position": "PhD student;MS student+Undergrad student;Postdoc;Associate Professor",
        "bibtex": "@inproceedings{\nparashar2025costaware,\ntitle={Cost-aware Discovery of Contextual Failures using Bayesian Active Learning},\nauthor={Anjali Parashar and Joseph Zhang and Yingke Li and Chuchu Fan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=f2Y549UzM5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=f2Y549UzM5",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0+0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "fBRqCMqVyS",
        "title": "Enabling Long(er) Horizon Imitation for Manipulation Tasks by Modeling Subgoal Transitions",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Imitation-based policy training for long-horizon manipulation tasks involving multi-step object interactions is often susceptible to compounding action errors. Contemporary methods discover semantic subgoals embedded within the overall task, decomposing the overall task into tractable shorter-horizon goal-conditioned policy learning. However, policy deployment requires iteratively estimating $\\textit{which}$ subgoal is being pursued and $\\textit{when}$ it is achieved. We observe the brittleness of conventional $\\textit{heuristic}$-based approaches (ad hoc threshold based), particularly for long-horizon imitation, since pursuing an incorrect subgoal can lead the robot policy to experience out of distribution states. In this work, we introduce two policy architectures for modeling subgoal transitions within a policy learning loop for long-horizon tasks. The first model autoregressively predicts the likelihood of the next subgoal transition, while the second uses cross-attention (via a transformer-based architecture) and implicitly models smooth and continuous transitions. We evaluate our models on $25$ simulated tasks on Franka Kitchen, $6$ real-world table-top tasks and $18$ simulated tasks on a new corpus (Franka-Long Horizon Tasks (LHT)) focused on tasks with rich object interactions over long episode lengths. Experimental results show significant improvements in learning efficacy, task success rates and generalization to out-of-distribution settings- extending horizon lengths for imitating manipulation tasks $\\textit{from long to long(er)}$.",
        "keywords": "Long Horizon Task Execution;Goal Conditioned Policy Learning;Imitation Learning;Learning from Demonstration;Robot Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/a8cb1a3396e0dcfda7c0cc535ff8623a0fe1468c.zip",
        "author": "Shivam Jain;Sachit Sachdeva;Rohan Paul",
        "authorids": "~Shivam_Jain1;~Sachit_Sachdeva1;~Rohan_Paul1",
        "gender": "M;M;M",
        "homepage": ";https://www.linkedin.com/in/sachit-sachdeva-559769214/;https://www.cse.iitd.ac.in/~rohanpaul/",
        "dblp": ";;92/7394",
        "google_scholar": ";;",
        "orcid": ";;",
        "linkedin": "shivam-jain-822198204/;;",
        "or_profile": "~Shivam_Jain1;~Sachit_Sachdeva1;~Rohan_Paul1",
        "aff": "Indian Institute of Technology, Delhi;;Indian Institute of Technology, Delhi",
        "aff_domain": "iitd.ac.in;;iitd.ac.in",
        "position": "Undergrad student;;Associate Professor",
        "bibtex": "@inproceedings{\njain2025enabling,\ntitle={Enabling Long(er) Horizon Imitation for Manipulation Tasks by Modeling Subgoal Transitions},\nauthor={Shivam Jain and Sachit Sachdeva and Rohan Paul},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=fBRqCMqVyS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=fBRqCMqVyS",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0",
        "aff_unique_norm": "Indian Institute of Technology Delhi;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.iitdelhi.ac.in;",
        "aff_unique_abbr": "IIT Delhi;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Delhi;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "India;"
    },
    {
        "id": "gD6YV5OuW3",
        "title": "ScrewSplat: An End-to-End Method for Articulated Object Recognition",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Articulated object recognition -- the task of identifying both the geometry and kinematic joints of objects with movable parts -- is essential for enabling robots to interact with everyday objects such as doors and laptops. However, existing approaches often rely on strong assumptions, such as a known number of articulated parts; require additional inputs, such as depth images; or involve complex intermediate steps that can introduce potential errors -- limiting their practicality in real-world settings. In this paper, we introduce **ScrewSplat**, a simple end-to-end method that operates solely on RGB observations. Our approach begins by randomly initializing screw axes, which are then iteratively optimized to recover the object\u2019s underlying kinematic structure. By integrating with Gaussian Splatting, we simultaneously reconstruct the 3D geometry and segment the object into rigid, movable parts. We demonstrate that our method achieves state-of-the-art recognition accuracy across a diverse set of articulated objects, and further enables zero-shot, text-guided manipulation using the recovered kinematic model.",
        "keywords": "Articulated objects;Gaussian splatting;Screw theory",
        "primary_area": "",
        "supplementary_material": "/attachment/7bc4c2fe9782e886a3af5c9455d80588a4be04d9.zip",
        "author": "Seungyeon Kim;Junsu HA;Young Hun Kim;Yonghyeon Lee;Frank C. Park",
        "authorids": "~Seungyeon_Kim2;~Junsu_HA1;~Young_Hun_Kim1;~Yonghyeon_Lee2;~Frank_C._Park1",
        "gender": "M;;M;M;M",
        "homepage": "https://seungyeon-k.github.io/;https://sites.google.com/robotics.snu.ac.kr/fcp/people/researchers?authuser=0;https://github.com/yhun96;https://www.gabe-yhlee.com;http://robotics.snu.ac.kr",
        "dblp": "74/7997-3;;;182/6796;p/FrankChongwooPark",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;;;u-h3PJIAAAAJ",
        "orcid": "0000-0001-6708-5684;;;;0000-0002-0293-6975",
        "linkedin": "seungyeon-kim-45a20b263/;;;;",
        "or_profile": "~Seungyeon_Kim2;~Junsu_HA1;~Young_Hun_Kim1;~Yonghyeon_Lee2;~Frank_C._Park1",
        "aff": "Seoul National University;Seoul National University;Seoul National University;Korea Institute for Advanced Study+Massachusetts Institute of Technology;Seoul National University",
        "aff_domain": "snu.ac.kr;snu.ac.kr;snu.ac.kr;kias.re.kr+mit.edu;snu.ac.kr",
        "position": "Postdoc;PhD student;PhD student;Postdoc+Postdoc;Full Professor",
        "bibtex": "@inproceedings{\nkim2025screwsplat,\ntitle={ScrewSplat: An End-to-End Method for Articulated Object Recognition},\nauthor={Seungyeon Kim and Junsu HA and Young Hun Kim and Yonghyeon Lee and Frank C. Park},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=gD6YV5OuW3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=gD6YV5OuW3",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1+2;0",
        "aff_unique_norm": "Seoul National University;Korea Institute for Advanced Study;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.snu.ac.kr;http://www.kaist.edu;https://web.mit.edu",
        "aff_unique_abbr": "SNU;KIAS;MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+1;0",
        "aff_country_unique": "South Korea;United States"
    },
    {
        "id": "gyihSZwQbR",
        "title": "Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference Scoped Exploration",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Hand\u2013object motion-capture (MoCap) repositories provide abundant, contact-rich human demonstrations for scaling dexterous manipulation on robots. Yet demonstration inaccuracy and embodiment gaps between human and robot hands challenge direct policy learning. Existing pipelines adapt a three-stage workflow: retargeting, tracking, and residual correction. This multi-step process may not fully utilize demonstrations and can introduce compound errors. We introduce Reference-Scoped Exploration (RSE), a unified, single-loop optimization that integrates retargeting and tracking to train a scalable robot control policy directly from MoCap. Instead of treating demonstrations as strict ground truth, we view them as soft guidance. From raw demonstrations, we construct adaptive spatial scopes\u2014time-varying termination boundaries, and reinforcement learning promotes the policy to stay within these envelopes while minimizing control effort. This holistic approach preserves demonstration intent, lets robot-specific strategies emerge, boosts robustness to noise, and scales effortlessly with large-scale demonstrations. We distill the scaled tracking policy into a vision-based, skill-conditioned generative control policy. This distilled policy captures diverse manipulation skills within a rich latent representation, enabling generalization across various objects and real-world robotic manipulation.",
        "keywords": "Dexterous Manipulation;Reinforcement Learning;Learning from Demonstrations",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Sirui Xu;Yu-Wei Chao;Liuyu Bian;Arsalan Mousavian;Yu-Xiong Wang;Liangyan Gui;Wei Yang",
        "authorids": "~Sirui_Xu1;~Yu-Wei_Chao1;~Liuyu_Bian1;~Arsalan_Mousavian1;~Yu-Xiong_Wang1;~Liangyan_Gui1;~Wei_Yang2",
        "gender": "M;M;M;M;;F;M",
        "homepage": "https://sirui-xu.github.io;http://www-personal.umich.edu/~ywchao/;;https://cs.gmu.edu/~amousavi/;https://yxw.cs.illinois.edu/;;http://wyang.me/",
        "dblp": "194/1216-2;44/10700;357/1396;164/8572;35/10700;155/5055;03/1094-19",
        "google_scholar": "sBAgRsIAAAAJ;48Y9F-YAAAAJ;J2z-0lgAAAAJ;fcA9m88AAAAJ;T_Q-xDkAAAAJ;3aE0r9QAAAAJ;6QQX88UAAAAJ",
        "orcid": "0000-0001-5372-6321;;;;;;0000-0003-3975-2472",
        "linkedin": ";;;;;;",
        "or_profile": "~Sirui_Xu1;~Yu-Wei_Chao1;~Liuyu_Bian1;~Arsalan_Mousavian1;~Yu-Xiong_Wang1;~Liangyan_Gui1;~Wei_Yang2",
        "aff": "University of Illinois, Urbana Champaign+NVIDIA;NVIDIA;University of Illinois, Urbana Champaign;NVIDIA;School of Computer Science, Carnegie Mellon University+Department of Computer Science, University of Illinois Urbana-Champaign;UIUC;NVIDIA",
        "aff_domain": "illinois.edu+nvidia.com;nvidia.com;illinois.edu;nvidia.com;cs.cmu.edu+cs.illinois.edu;cs.illinois.edu;nvidia.com",
        "position": "PhD student+Intern;Research Scientist;PhD student;Research Scientist;PhD student+Assistant Professor;Assistant Professor;Research Scientist",
        "bibtex": "@inproceedings{\nxu2025dexplore,\ntitle={Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference Scoped Exploration},\nauthor={Sirui Xu and Yu-Wei Chao and Liuyu Bian and Arsalan Mousavian and Yu-Xiong Wang and Liangyan Gui and Wei Yang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=gyihSZwQbR}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=gyihSZwQbR",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;1;0;1;2+0;0;1",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;NVIDIA;Carnegie Mellon University",
        "aff_unique_dep": ";NVIDIA Corporation;School of Computer Science",
        "aff_unique_url": "https://illinois.edu;https://www.nvidia.com;https://www.cmu.edu",
        "aff_unique_abbr": "UIUC;NVIDIA;CMU",
        "aff_campus_unique_index": "0;0;2+0;0",
        "aff_campus_unique": "Urbana-Champaign;;Pittsburgh",
        "aff_country_unique_index": "0+0;0;0;0;0+0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "h2K52fhsDU",
        "title": "Pointing3D: A Benchmark for 3D Object Referral via Pointing Gestures",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Pointing gestures provide a natural and efficient way to communicate spatial information in human-machine interaction, yet their potential for 3D object referral remains largely under-explored. To fill this gap, we introduce the task of pointing-based 3D segmentation. In this task, given an image of a person pointing at an object and the 3D point cloud of the environment, the goal is to predict the 3D segmentation mask of the referred object. To enable the standardized evaluation of this task, we introduce POINTR3D, a curated dataset of over 65,000 frames captured with three cameras across four indoor scenes, featuring diverse pointing scenarios. Each frame is annotated with the information of the active hand, the corresponding object ID, and the 3D segmentation mask of the object. To showcase the application of the proposed dataset, we further introduce Pointing3D, a transformer-based architecture that predicts the pointing direction from RGB images and uses this prediction as a prompt to segment the referred object in the 3D point cloud. Experimental results show that Pointing3D outperforms other strong baselines we introduce and lays the groundwork for future research. The dataset, source code, and evaluation tools will be made publicly available to support further research in this area, enabling a natural human-machine interaction.",
        "keywords": "Object Referral;Pointing Gesture;3D Segmentation",
        "primary_area": "",
        "supplementary_material": "/attachment/4a4a96b8fd47c3b349c07ea1921d4566c8090d1a.zip",
        "author": "Mert Arslanoglu;Kadir Yilmaz;Cemhan Kaan \u00d6zaltan;Timm Linder;Bastian Leibe",
        "authorids": "~Mert_Arslanoglu1;~Kadir_Yilmaz1;~Cemhan_Kaan_\u00d6zaltan1;~Timm_Linder1;~Bastian_Leibe3",
        "gender": "M;M;;;M",
        "homepage": ";;;;http://www.vision.rwth-aachen.de",
        "dblp": ";;;;41/1228",
        "google_scholar": ";S1NjpAwAAAAJ;ckR_mRcAAAAJ;;ZcULDB0AAAAJ",
        "orcid": ";;;;0000-0003-4225-0051",
        "linkedin": "mert-arslano\u011flu-4a5510254/;kadir-yilmaz193/;;;",
        "or_profile": "~Mert_Arslanoglu1;~Kadir_Yilmaz1;~Cemhan_Kaan_\u00d6zaltan1;~Timm_Linder1;~Bastian_Leibe3",
        "aff": "Rheinisch Westf\u00e4lische Technische Hochschule Aachen;Rheinisch Westf\u00e4lische Technische Hochschule Aachen;Rheinisch Westf\u00e4lische Technische Hochschule Aachen;;RWTH Aachen University",
        "aff_domain": "rwth-aachen.de;rwth-aachen.de;rwth-aachen.de;;rwth-aachen.de",
        "position": "MS student;PhD student;MS student;;Full Professor",
        "bibtex": "@inproceedings{\narslanoglu2025pointingd,\ntitle={Pointing3D: A Benchmark for 3D Object Referral via Pointing Gestures},\nauthor={Mert Arslanoglu and Kadir Yilmaz and Cemhan Kaan {\\\"O}zaltan and Timm Linder and Bastian Leibe},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=h2K52fhsDU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=h2K52fhsDU",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "RWTH Aachen University;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.rwth-aachen.de;",
        "aff_unique_abbr": "RWTH;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Aachen;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "hg9YtHV8MJ",
        "title": "KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In this work, we propose a novel flow field-based motion planning method that\ndrives a robot from any initial state to a desired reference trajectory such that it converges to the trajectory's end point. \nDespite demonstrated efficacy in using Koopman operator theory for modeling dynamical systems, Koopman does not inherently enforce convergence to desired trajectories nor to specified goals, a requirement when learning from demonstrations (LfD). \nWe present KoopMotion which represents motion flow fields as dynamical systems, parameterized by Koopman Operators, and leverages the divergence properties of the learnt flow fields to obtain smooth motion fields that converge to a desired reference trajectory when the robot is placed away from the desired trajectory, and tracks the trajectory until the end point.\nTo demonstrate the effectiveness of our approach, we show evaluations of KoopMotion on the LASA human handwriting dataset, including spectral analysis.\nWe also perform experiments on a physical robot, verifying KoopMotion on a miniature autonomous surface vehicle operating in a non-static fluid flow environment.\nOur approach is highly sample efficient in both space and time, requiring only 3\\% of the LASA dataset to generate dense motion plans.\nAdditionally, KoopMotion provides a significant improvement over baselines when comparing metrics that measure spatial and temporal dynamics modeling efficacy.",
        "keywords": "motion planning;koopman operator theory;dynamical systems;learning from demonstrations",
        "primary_area": "",
        "supplementary_material": "/attachment/74204fed6790d35a00894e091b65bf0de5dd54ed.zip",
        "author": "Alice Kate Li;Thales C. Silva;Victoria Edwards;Vijay Kumar;M. Ani Hsieh",
        "authorids": "~Alice_Kate_Li1;s.c.thales@gmail.com;~Victoria_Edwards1;~Vijay_Kumar2;~M._Ani_Hsieh1",
        "gender": "F;;;;",
        "homepage": ";;https://vmedwards.github.io/;http://kumarrobotics.org;",
        "dblp": ";;;;",
        "google_scholar": ";;m5rgBkUAAAAJ;FUOEBDUAAAAJ;",
        "orcid": ";;;;",
        "linkedin": "alicekl/;;;;",
        "or_profile": "~Alice_Kate_Li1;s.c.thales@gmail.com;~Victoria_Edwards1;~Vijay_Kumar2;~M._Ani_Hsieh1",
        "aff": "University of Pennsylvania;;University of Pennsylvania;;",
        "aff_domain": "upenn.edu;;seas.upenn.edu;;",
        "position": "PhD student;;PhD student;;",
        "bibtex": "@inproceedings{\nli2025koopmotion,\ntitle={KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning},\nauthor={Alice Kate Li and Thales C. Silva and Victoria Edwards and Vijay Kumar and M. Ani Hsieh},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=hg9YtHV8MJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=hg9YtHV8MJ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;1;1",
        "aff_unique_norm": "University of Pennsylvania;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.upenn.edu;",
        "aff_unique_abbr": "UPenn;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "hh9afiQMb2",
        "title": "Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Foundation models can provide robust high-level reasoning on appropriate safety interventions in hazardous scenarios beyond a robot's training data, i.e. out-of-distribution (OOD) failures. However, due to the high inference latency of Large Vision and Language Models, current methods rely on manually defined intervention policies to enact fallbacks, thereby lacking the ability to plan generalizable, semantically safe motions. To overcome these challenges we present FORTRESS, a framework that generates and reasons about semantically safe fallback strategies in real time to prevent OOD failures. At a low frequency in nominal operations, FORTRESS uses multi-modal reasoners to identify goals and anticipate failure modes. When a runtime monitor triggers a fallback response, FORTRESS rapidly synthesizes plans to fallback goals while inferring and avoiding semantically unsafe regions in real time. By bridging open-world, multi-modal reasoning with dynamics-aware planning, we eliminate the need for hard-coded fallbacks and human safety interventions. FORTRESS outperforms on-the-fly prompting of slow reasoning models in safety classification accuracy on synthetic benchmarks and real-world ANYmal robot data, and further improves system safety and planning success in simulation and on quadrotor hardware for urban navigation. Website and code can be found at https://submfort.github.io/fortress/.",
        "keywords": "Multi-modal Reasoning in Robotics;OOD Safety;Fallback Synthesis",
        "primary_area": "",
        "supplementary_material": "/attachment/0cd4f3536cbe852d956bbe884aa73a9c97ab34b3.zip",
        "author": "Milan Ganai;Rohan Sinha;Christopher Agia;Daniel Morton;Luigi Di Lillo;Marco Pavone",
        "authorids": "~Milan_Ganai1;~Rohan_Sinha1;~Christopher_Agia1;~Daniel_Morton1;~Luigi_Di_Lillo1;~Marco_Pavone1",
        "gender": ";;M;M;M;M",
        "homepage": "https://milanganai.github.io;https://www.stanford.edu/;https://www.chrisagia.com/;;;https://web.stanford.edu/~pavone/",
        "dblp": ";;268/3555;;;91/3382-1.html",
        "google_scholar": "LCMIfaQAAAAJ;;t8Em5FwAAAAJ;cH0GOYEAAAAJ;;RhOpyXcAAAAJ",
        "orcid": ";;0000-0002-1208-2539;;0000-0001-8308-9941;",
        "linkedin": "milanganai/;;agiachris/;danielpmorton;;",
        "or_profile": "~Milan_Ganai1;~Rohan_Sinha1;~Christopher_Agia1;~Daniel_Morton1;~Luigi_Di_Lillo1;~Marco_Pavone1",
        "aff": "Stanford University;Stanford University;Stanford University;Stanford University;Swiss Re;NVIDIA+Stanford University",
        "aff_domain": "stanford.edu;stanford.edu;stanford.edu;stanford.edu;swissre.com;nvidia.com+stanford.edu",
        "position": "PhD student;PhD student;PhD student;PhD student;Head of Products & Partnerships;Director, Autonomous Vehicle Research+Associate Professor",
        "bibtex": "@inproceedings{\nganai2025realtime,\ntitle={Real-Time Out-of-Distribution Failure Prevention via Multi-Modal Reasoning},\nauthor={Milan Ganai and Rohan Sinha and Christopher Agia and Daniel Morton and Luigi Di Lillo and Marco Pavone},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=hh9afiQMb2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=hh9afiQMb2",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;1;2+0",
        "aff_unique_norm": "Stanford University;Swiss Re;NVIDIA",
        "aff_unique_dep": ";;NVIDIA Corporation",
        "aff_unique_url": "https://www.stanford.edu;;https://www.nvidia.com",
        "aff_unique_abbr": "Stanford;;NVIDIA",
        "aff_campus_unique_index": "0;0;0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "htgNQHa6Ta",
        "title": "TWIST: Teleoperated Whole-Body Imitation System",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. Yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. We present the Teleoperated Whole-Body Imitation System (TWIST), a system for humanoid teleoperation through whole-body motion imitation. We first generate reference motion clips by retargeting human motion capture data to the humanoid robot. We then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (MoCap) data improves tracking accuracy. TWIST enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills\u2014spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement\u2014using a single unified neural network controller.",
        "keywords": "humanoid robots;whole-body teleoperation;learning-based control",
        "primary_area": "",
        "supplementary_material": "/attachment/0813e7003b552f56f47d80a86ed0359b26efc525.pdf",
        "author": "Yanjie Ze;Zixuan Chen;Joao Pedro Araujo;Zi-ang Cao;Xue Bin Peng;Jiajun Wu;Karen Liu",
        "authorids": "~Yanjie_Ze1;~Zixuan_Chen9;~Joao_Pedro_Araujo1;~Zi-ang_Cao1;~Xue_Bin_Peng1;~Jiajun_Wu1;~Karen_Liu1",
        "gender": "M;M;;M;M;M;",
        "homepage": "http://yanjieze.com;https://zixuan417.github.io;https://jaraujo98.github.io/;;https://xbpeng.github.io;https://jiajunwu.com;https://cs.stanford.edu/~karenliu",
        "dblp": "312/5407;;149/9340;;;117/4768;",
        "google_scholar": "BO_b2O8AAAAJ;;;;https://scholar.google.ca/citations?user=FwxfQosAAAAJ;2efgcS0AAAAJ;i28fU0MAAAAJ",
        "orcid": ";;;;;0000-0002-4176-343X;0000-0001-5926-0905",
        "linkedin": "yanjie-ze-a71a0a247/;;;zi-ang-cao-robotics;;jiajunwu/;",
        "or_profile": "~Yanjie_Ze1;~Zixuan_Chen9;~Joao_Pedro_Araujo1;~Zi-ang_Cao1;~Xue_Bin_Peng1;~Jiajun_Wu1;~Karen_Liu1",
        "aff": "Stanford University;University of California, San Diego;Computer Science Department, Stanford University;Stanford University;Simon Fraser University;Stanford University;Computer Science Department, Stanford University",
        "aff_domain": "stanford.edu;ucsd.edu;cs.stanford.edu;stanford.edu;sfu.ca;stanford.edu;cs.stanford.edu",
        "position": "PhD student;PhD student;PhD student;MS student;Assistant Professor;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\nze2025twist,\ntitle={{TWIST}: Teleoperated Whole-Body Imitation System},\nauthor={Yanjie Ze and Zixuan Chen and Joao Pedro Araujo and Zi-ang Cao and Xue Bin Peng and Jiajun Wu and Karen Liu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=htgNQHa6Ta}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=htgNQHa6Ta",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0;2;0;0",
        "aff_unique_norm": "Stanford University;University of California, San Diego;Simon Fraser University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.stanford.edu;https://www.ucsd.edu;https://www.sfu.ca",
        "aff_unique_abbr": "Stanford;UCSD;SFU",
        "aff_campus_unique_index": "0;1;0;0;0;0",
        "aff_campus_unique": "Stanford;San Diego;",
        "aff_country_unique_index": "0;0;0;0;1;0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "iQQy1BKlGv",
        "title": "MEReQ: Max-Ent Residual-Q Inverse RL for Sample-Efficient Alignment from Intervention",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Aligning robot behavior with human preferences is crucial for deploying embodied AI agents in human-centered environments. A promising solution is interactive imitation learning from human intervention, where a human expert observes the policy's execution and provides interventions as feedback. However, existing methods often fail to utilize the prior policy efficiently to facilitate learning, thus hindering sample efficiency. In this work, we introduce Maximum-Entropy Residual-Q Inverse Reinforcement Learning, designed for sample-efficient alignment from human intervention. Instead of inferring the complete human behavior characteristics, MEReQ infers a residual reward function that captures the discrepancy between the human expert's and the prior policy's underlying reward functions. It then employs Residual Q-Learning (RQL) to align the policy with human preferences using this residual reward function. Extensive evaluations on simulated and real-world tasks demonstrate that MEReQ achieves sample-efficient policy alignment from human intervention compared to other baselines.",
        "keywords": "Interactive imitation learning;Learning from human feedback;Inverse reinforcement learning",
        "primary_area": "",
        "supplementary_material": "/attachment/ab1cb8c73e94f3aa417d7ea43fe03f1cdb363c9b.zip",
        "author": "Yuxin Chen;Chen Tang;Jianglan Wei;Chenran Li;Thomas Tian;Xiang Zhang;Wei Zhan;Peter Stone;Masayoshi Tomizuka",
        "authorids": "~Yuxin_Chen7;~Chen_Tang2;~Jianglan_Wei1;~Chenran_Li1;~Thomas_Tian1;~Xiang_Zhang20;~Wei_Zhan2;~Peter_Stone1;~Masayoshi_Tomizuka2",
        "gender": "M;M;M;;M;M;;M;",
        "homepage": "http://thomaschen98.github.io;https://chentangmark.github.io;https://jianglanwei.github.io;;https://scholar.google.com/citations?user=uY4D8-wAAAAJ&hl=en&authuser=1;https://xiang-zhang-98.github.io/;;http://www.cs.utexas.edu/~pstone;",
        "dblp": ";;;;;;;s/PeterStone;",
        "google_scholar": "EzoDsIMAAAAJ;x78TL58AAAAJ;ZP4ETh0AAAAJ;;;;;qnwjcfAAAAAJ;",
        "orcid": ";;;;;;;0000-0002-6795-420X;",
        "linkedin": "thomaschen98/;chen-tang-08377b5b/;;;;;;;",
        "or_profile": "~Yuxin_Chen7;~Chen_Tang2;~Jianglan_Wei1;~Chenran_Li1;~Thomas_Tian1;~Xiang_Zhang20;~Wei_Zhan2;~Peter_Stone1;~Masayoshi_Tomizuka2",
        "aff": "University of California, Berkeley;University of Texas at Austin;Huazhong University of Science and Technology+University of California, Berkeley;;University of California, Berkeley;;;Sony AI+University of Texas, Austin;",
        "aff_domain": "berkeley.edu;utexas.edu;hust.edu.cn+berkeley.edu;;berkeley.edu;;;sony.com+utexas.edu;",
        "position": "PhD student;Postdoc;Undergrad student+Intern;;PhD student;;;Principal Researcher+Full Professor;",
        "bibtex": "@inproceedings{\nchen2025mereq,\ntitle={{MER}eQ: Max-Ent Residual-Q Inverse {RL} for Sample-Efficient Alignment from Intervention},\nauthor={Yuxin Chen and Chen Tang and Jianglan Wei and Chenran Li and Thomas Tian and Xiang Zhang and Wei Zhan and Peter Stone and Masayoshi Tomizuka},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=iQQy1BKlGv}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=iQQy1BKlGv",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2+0;3;0;3;3;4+1;3",
        "aff_unique_norm": "University of California, Berkeley;University of Texas at Austin;Huazhong University of Science and Technology;;Sony",
        "aff_unique_dep": ";;;;Sony AI",
        "aff_unique_url": "https://www.berkeley.edu;https://www.utexas.edu;http://www.hust.edu.cn;;https://www.sony.com",
        "aff_unique_abbr": "UC Berkeley;UT Austin;HUST;;Sony AI",
        "aff_campus_unique_index": "0;1;0;0;1",
        "aff_campus_unique": "Berkeley;Austin;",
        "aff_country_unique_index": "0;0;1+0;0;3+0",
        "aff_country_unique": "United States;China;;Japan"
    },
    {
        "id": "iVbCWUDyBF",
        "title": "Search-TTA: A Multi-Modal Test-Time Adaptation Framework for Visual Search in the Wild",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "To perform autonomous visual search for environmental monitoring, a robot may leverage satellite imagery as a prior map. This can help inform coarse, high level search and exploration strategies, even when such images lack sufficient resolution to allow fine-grained, explicit visual recognition of targets. However, there are some challenges to overcome with using satellite images to direct visual search. For one, targets that are unseen in satellite images are underrepresented (compared to real life) in most existing datasets, and thus vision models trained on these datasets fail to reason effectively based on indirect visual cues. Furthermore, approaches which leverage large Vision Language Models (VLMs) for generalization may yield inaccurate outputs due to hallucination, leading to inefficient search. To address these challenges, we introduce Search-TTA, a multimodal test-time adaptation framework that can accept text and/or image input. First, we pretrain a remote sensing image encoder to align with CLIP\u2019s visual encoder to output probability distributions of target presence used for visual search. Second, our framework dynamically refines CLIP\u2019s predictions during search using a test-time adaptation mechanism. Through a feedback loop inspired by Spatial Poisson Point Processes, gradient updates (weighted by uncertainty) are used to correct (potentially inaccurate) predictions and improve search performance. To validate Search-TTA\u2019s performance, we curate a visual search dataset based on internet-scale ecological data. We find that Search-TTA improves planner performance by up to 9.7%, particularly in cases with poor initial CLIP predictions. It also achieves comparable performance to state-of-the-art VLMs. Finally, we deploy Search-TTA on a real UAV via hardware-in-the-loop testing, by simulating its operation within a large-scale simulation that provides onboard sensing.",
        "keywords": "Test-Time Adaptation;Visual Search;VLM;Ecological Monitoring",
        "primary_area": "",
        "supplementary_material": "/attachment/9a80baccccc2046164700dbcb3548d1115b7d164.zip",
        "author": "Derek Ming Siang Tan;Shailesh;Boyang Liu;Alok Raj;Qi Xuan Ang;Weiheng Dai;Tanishq Duhan;Jimmy Chiun;Yuhong Cao;Florian Shkurti;Guillaume Adrien Sartoretti",
        "authorids": "~Derek_Ming_Siang_Tan1;21je0865@iitism.ac.in;boyang.liu@u.nus.edu;22je0091@iitism.ac.in;qixuan.ang@stengg.com;~Weiheng_Dai1;~Tanishq_Duhan1;~Jimmy_Chiun1;~Yuhong_Cao1;~Florian_Shkurti1;~Guillaume_Adrien_Sartoretti1",
        "gender": "M;;;;;M;M;M;M;M;M",
        "homepage": "https://www.derektanmingsiang.com/;;;;;https://weihengdai.top;;;;http://www.cs.toronto.edu/~florian/;https://marmotlab.org/",
        "dblp": ";;;;;;;;;21/10333;118/9066",
        "google_scholar": "https://scholar.google.ca/citations?user=dVvzYaoAAAAJ;;;;;;rTz0anAAAAAJ;;;https://scholar.google.ca/citations?hl=en;n7NzZ0sAAAAJ",
        "orcid": ";;;;;;;0009-0009-5184-8291;0000-0001-8099-0689;;0000-0002-7579-9916",
        "linkedin": "derektan95;;;;;;;jimmychiun;;;",
        "or_profile": "~Derek_Ming_Siang_Tan1;21je0865@iitism.ac.in;boyang.liu@u.nus.edu;22je0091@iitism.ac.in;qixuan.ang@stengg.com;~Weiheng_Dai1;~Tanishq_Duhan1;~Jimmy_Chiun1;~Yuhong_Cao1;~Florian_Shkurti1;~Guillaume_Adrien_Sartoretti1",
        "aff": "national university of singaore, National University of Singapore;;;;;national university of singaore, National University of Singapore;national university of singaore, National University of Singapore;;National University of Singapore;University of Toronto;National University of Singapore",
        "aff_domain": "u.nus.edu;;;;;u.nus.edu;u.nus.edu;;nus.edu.sg;cs.toronto.edu;nus.edu.sg",
        "position": "PhD student;;;;;PhD student;PhD student;;Postdoc;Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ntan2025searchtta,\ntitle={Search-{TTA}: A Multi-Modal Test-Time Adaptation Framework for Visual Search in the Wild},\nauthor={Derek Ming Siang Tan and Shailesh and Boyang Liu and Alok Raj and Qi Xuan Ang and Weiheng Dai and Tanishq Duhan and Jimmy Chiun and Yuhong Cao and Florian Shkurti and Guillaume Adrien Sartoretti},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=iVbCWUDyBF}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=iVbCWUDyBF",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;1;1;0;0;1;2;3;2",
        "aff_unique_norm": "national university of singaore, National University of Singapore;;National University of Singapore;University of Toronto",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";;https://www.nus.edu.sg;https://www.utoronto.ca",
        "aff_unique_abbr": ";;NUS;U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;2;1",
        "aff_country_unique": ";Singapore;Canada"
    },
    {
        "id": "iWMM4oxMBu",
        "title": "Learn from What We HAVE: History-Aware VErifier that Reasons about Past Interactions Online",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "We introduce a novel History-Aware VErifier (HAVE) to disambiguate uncertain scenarios online by leveraging past interactions. Robots frequently encounter visually ambiguous objects whose manipulation outcomes remain uncertain until physically interacted with. While generative models alone could theoretically adapt to such ambiguity, in practice they obtain suboptimal performance in ambiguous cases, even when conditioned on action history.  To address this, we propose explicitly decoupling action generation from verification: we use an unconditional diffusion-based generator to propose multiple candidate actions and employ our history-aware verifier to select the most promising action by reasoning about past interactions. Through theoretical analysis, we demonstrate that employing a verifier significantly improves expected action quality. Empirical evaluations and analysis across multiple simulated and real-world environments including articulated objects, multi-modal doors, and uneven object pick-up confirm the effectiveness of our method and improvements over baselines.",
        "keywords": "Ambiguities;Multi-modality;Generation-Verification",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yishu Li;Xinyi Mao;Ying Yuan;Kyutae Sim;Ben Eisner;David Held",
        "authorids": "~Yishu_Li1;~Xinyi_Mao1;~Ying_Yuan2;~Kyutae_Sim1;~Ben_Eisner1;~David_Held1",
        "gender": "F;F;F;;M;M",
        "homepage": "https://liy1shu.github.io/;https://shhmxy2.github.io/;https://yingyuan0414.github.io;https://ktsim01.github.io/;;http://davheld.github.io/",
        "dblp": "247/2570;391/2581;;;;22/11147",
        "google_scholar": "https://scholar.google.com/citations?hl=zh-CN;AoK5GugAAAAJ;2IEoTWwAAAAJ;;RWe-v0UAAAAJ;0QtU-NsAAAAJ",
        "orcid": ";;;;;",
        "linkedin": ";;;kyutae-sim/;;",
        "or_profile": "~Yishu_Li1;~Xinyi_Mao1;~Ying_Yuan2;~Kyutae_Sim1;~Ben_Eisner1;~David_Held1",
        "aff": "Computer Science Department, School of Computer Science;Tsinghua University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University;Carnegie Mellon University",
        "aff_domain": "csd.cs.cmu.edu;cs.tsinghua.edu.cn;andrew.cmu.edu;cmu.edu;cmu.edu;cmu.edu",
        "position": "MS student;Undergrad student;PhD student;MS student;PhD student;Associate Professor",
        "bibtex": "@inproceedings{\nli2025learn,\ntitle={Learn from What We {HAVE}: History-Aware {VE}rifier that Reasons about Past Interactions Online},\nauthor={Yishu Li and Xinyi Mao and Ying Yuan and Kyutae Sim and Ben Eisner and David Held},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=iWMM4oxMBu}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=iWMM4oxMBu",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;2;2;2",
        "aff_unique_norm": "School of Computer Science;Tsinghua University;Carnegie Mellon University",
        "aff_unique_dep": "Computer Science Department;;",
        "aff_unique_url": ";https://www.tsinghua.edu.cn;https://www.cmu.edu",
        "aff_unique_abbr": ";THU;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;2;2;2;2",
        "aff_country_unique": ";China;United States"
    },
    {
        "id": "iq0wUf6dUy",
        "title": "Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Solving complex long-horizon robotic manipulation problems requires sophisticated high-level planning capabilities, the ability to reason about the physical world, and reactively choose appropriate motor skills. Vision-language models (VLMs) pretrained on Internet data could in principle offer a framework for tackling such problems. However, in their current form, VLMs lack both the nuanced understanding of intricate physics required for robotic manipulation and the ability to reason over long horizons to address error compounding issues.\nIn this paper, we introduce a novel test-time computation framework that enhances VLMs' physical reasoning capabilities for multi-stage manipulation tasks. At its core, our approach iteratively improves a pretrained VLM with a ``reflection'' mechanism - it uses a generative model to imagine future world states, leverages these predictions to guide action selection, and critically reflects on potential suboptimalities to refine its reasoning. Experimental results demonstrate that our method significantly outperforms several state-of-the-art commercial VLMs as well as other post-training approaches such as Monte Carlo Tree Search (MCTS). Videos are available at https://corl2025-reflectvlm.github.io.",
        "keywords": "Vision-Language Models;Long-Horizon Manipulation;Manipulation Planning",
        "primary_area": "",
        "supplementary_material": "/attachment/01f98ff4de3ad30f1dd024c8b6de158fbbfe2c5e.zip",
        "author": "Yunhai Feng;Jiaming Han;Zhuoran Yang;Xiangyu Yue;Sergey Levine;Jianlan Luo",
        "authorids": "~Yunhai_Feng1;~Jiaming_Han1;~Zhuoran_Yang1;~Xiangyu_Yue1;~Sergey_Levine1;~Jianlan_Luo1",
        "gender": ";M;M;M;M;",
        "homepage": ";https://csuhan.com;https://zhuoranyang.github.io/;http://xyue.io/;https://people.eecs.berkeley.edu/~svlevine/;https://people.eecs.berkeley.edu/~jianlanluo/",
        "dblp": ";;;207/7518;80/7594;161/1838",
        "google_scholar": ";https://scholar.google.com.hk/citations?user=vgcxKEcAAAAJ;;-xQ-C1sAAAAJ;8R35rCwAAAAJ;SJoRNbYAAAAJ",
        "orcid": ";;;;;",
        "linkedin": ";;;;;",
        "or_profile": "~Yunhai_Feng1;~Jiaming_Han1;~Zhuoran_Yang1;~Xiangyu_Yue1;~Sergey_Levine1;~Jianlan_Luo1",
        "aff": ";The Chinese University of Hong Kong+ByteDance Inc.;Yale University;The Chinese University of Hong Kong;University of California, Berkeley;University of California, Berkeley",
        "aff_domain": ";cuhk.edu.hk+bytedance.com;yale.edu;ie.cuhk.edu;berkeley.edu;eecs.berkeley.edu",
        "position": ";PhD student+Intern;Assistant Professor;Assistant Professor;Associate Professor;Postdoc",
        "bibtex": "@inproceedings{\nfeng2025reflective,\ntitle={Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation},\nauthor={Yunhai Feng and Jiaming Han and Zhuoran Yang and Xiangyu Yue and Sergey Levine and Jianlan Luo},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=iq0wUf6dUy}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=iq0wUf6dUy",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1+2;3;1;4;4",
        "aff_unique_norm": ";Chinese University of Hong Kong;ByteDance;Yale University;University of California, Berkeley",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": ";https://www.cuhk.edu.hk;https://www.bytedance.com;https://www.yale.edu;https://www.berkeley.edu",
        "aff_unique_abbr": ";CUHK;ByteDance;Yale;UC Berkeley",
        "aff_campus_unique_index": "1;1;2;2",
        "aff_campus_unique": ";Hong Kong SAR;Berkeley",
        "aff_country_unique_index": "1+1;2;1;2;2",
        "aff_country_unique": ";China;United States"
    },
    {
        "id": "irh5o90Mj1",
        "title": "Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Vision-Language-Action (VLA) models have become a cornerstone in robotic policy learning, leveraging large-scale multimodal data for robust and scalable control. However, existing VLA frameworks primarily address short-horizon tasks, and their effectiveness on long-horizon, multi-step robotic manipulation remains limited due to challenges in skill chaining and subtask dependencies. In this work, we introduce Long-VLA, the first end-to-end VLA model specifically designed for long-horizon robotic tasks. Our approach features a novel phase-aware input masking strategy that adaptively segments each subtask into moving and interaction phases, enabling the model to focus on phase-relevant sensory cues and enhancing subtask compatibility. This unified strategy preserves the scalability and data efficiency of VLA training, and our architecture-agnostic module can be seamlessly integrated into existing VLA models. We further propose the L-CALVIN benchmark to systematically evaluate long-horizon manipulation. Extensive experiments on both simulated and real-world tasks demonstrate that Long-VLA significantly outperforms prior state-of-the-art methods, establishing a new baseline for long-horizon robotic control.",
        "keywords": "VLA; Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/b94e0738e19a0865d994c989069bf3af79870935.zip",
        "author": "Yiguo Fan;Shuanghao Bai;Xinyang Tong;Pengxiang Ding;Yuyang Zhu;Hongchao Lu;Fengqi Dai;Wei Zhao;Yang Liu;Siteng Huang;Zhaoxin Fan;Badong Chen;Donglin Wang",
        "authorids": "~Yiguo_Fan2;~Shuanghao_Bai1;~Xinyang_Tong1;~Pengxiang_Ding1;~Yuyang_Zhu2;~Hongchao_Lu2;~Fengqi_Dai1;~Wei_Zhao21;~Yang_Liu97;~Siteng_Huang1;~Zhaoxin_Fan1;~Badong_Chen1;~Donglin_Wang1",
        "gender": "F;M;F;M;M;M;;M;M;;M;M;M",
        "homepage": ";https://github.com/BaiShuanghao?tab=repositories;https://github.com/yuan48;https://dingpx.github.io/;https://github.com/creater-bai?tab=overview&from=2024-08-01&to=2024-08-14;;;;https://yliu-cs.github.io;https://kyonhuang.top/;https://fanzhaoxin666.github.io/;http://gr.xjtu.edu.cn/web/chenbd/home;https://milab.westlake.edu.cn/",
        "dblp": ";364/7251;354/5795;293/7538;;;;;;251/9544.html;174/6149;95/6450;",
        "google_scholar": "VcBEZFcAAAAJ;xhd94DIAAAAJ;;https://scholar.google.com/citations?hl=en;e1wUhVUAAAAJ;https://scholar.google.com/citations?view_op=list_works;;;orbCoA8AAAAJ;mhpkWSYAAAAJ;https://scholar.google.com/citations?hl=zh-CN;mq6tPX4AAAAJ;https://scholar.google.ca/citations?user=-fo6wdwAAAAJ",
        "orcid": ";0009-0002-6047-0242;;0000-0002-4049-7467;;;;0000-0001-8452-9339;0009-0004-2757-0801;0000-0002-9735-1186;0000-0002-6324-1712;0000-0003-1710-3818;0000-0002-8188-3735",
        "linkedin": ";;;;;;;;;;;;",
        "or_profile": "~Yiguo_Fan2;~Shuanghao_Bai1;~Xinyang_Tong1;~Pengxiang_Ding1;~Yuyang_Zhu2;~Hongchao_Lu2;~Fengqi_Dai1;~Wei_Zhao21;~Yang_Liu97;~Siteng_Huang1;~Zhaoxin_Fan1;~Badong_Chen1;~Donglin_Wang1",
        "aff": "Westlake University;Xi'an Jiaotong University;Westlake University;Zhejiang University+Westlake University;Westlake University+University of Electronic Science and Technology of China;Westlake University;;Westlake University;Westlake University;Alibaba Group;Beihang University;Xi'an Jiaotong University;Westlake University",
        "aff_domain": "westlake.edu.cn;xjtu.edu.cn;westlake.edu.cn;zju.edu.cn+westlake.edu;westlake.edu+uestc.edu;westlake.edu.cn;;westlake.edu.cn;westlake.edu.cn;alibaba-inc.com;buaa.edu.cn;xjtu.edu.cn;westlake.edu.cn",
        "position": "Intern;PhD student;Researcher;PhD student+PhD student;Intern+Undergrad student;Postdoc;;Postdoc;PhD student;Researcher;Assistant Professor;Full Professor;Associate Professor",
        "bibtex": "@inproceedings{\nfan2025longvla,\ntitle={Long-{VLA}: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation},\nauthor={Yiguo Fan and Shuanghao Bai and Xinyang Tong and Pengxiang Ding and Yuyang Zhu and Hongchao Lu and Fengqi Dai and Wei Zhao and Yang Liu and Siteng Huang and Zhaoxin Fan and Badong Chen and Donglin Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=irh5o90Mj1}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=irh5o90Mj1",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            13,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;2+0;0+3;0;4;0;0;5;6;1;0",
        "aff_unique_norm": "Westlake University;Xi'an Jiao Tong University;Zhejiang University;University of Electronic Science and Technology of China;;Alibaba Group;Beihang University",
        "aff_unique_dep": ";;;;;;",
        "aff_unique_url": "https://www.westlake.edu.cn;https://www.xjtu.edu.cn;https://www.zju.edu.cn;https://www.uestc.edu.cn;;https://www.alibaba.com;http://www.buaa.edu.cn/",
        "aff_unique_abbr": "WU;XJTU;ZJU;UESTC;;Alibaba;BUAA",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0+0;0+0;0;0;0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "isrcFrgwZp",
        "title": "AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Scalable and reproducible policy evaluation has been a long-standing challenge in robot learning: evaluations are critical to assess progress and build better policies, but evaluation in the real world, especially at a scale that would provide statistically reliable results, is costly in terms of human time and hard to obtain. Evaluation of increasingly generalist robot policies requires an increasingly diverse repertoire of evaluation environments, making the evaluation bottleneck even more pronounced. To make real-world evaluation of robotic policies more practical, we propose AutoEval, a system to autonomously evaluate generalist robot policies around the clock with minimal human intervention. Users interact with AutoEval by submitting evaluation jobs to the AutoEval queue, much like how software jobs are submitted with a cluster scheduling system, and AutoEval will schedule the policies for evaluation within a framework supplying automatic success detection and automatic scene resets. We show that AutoEval can nearly fully eliminate human involvement in the evaluation process, permitting around the clock evaluations, and the evaluation results correspond closely to ground truth evaluations conducted by hand. To facilitate the evaluation of generalist policies in the robotics community, we provide public access to multiple AutoEval scenes in the popular BridgeData robot setup with WidowX robot arms. In the future, we hope that AutoEval scenes can be set up across institutions to form a diverse and distributed evaluation network.",
        "keywords": "Robot evaluation;benchmark;generalist robot policy;manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/82e52753f169e76d8870af4cc7fe5ebfd5c6ed96.zip",
        "author": "Zhiyuan Zhou;Pranav Atreya;You Liang Tan;Karl Pertsch;Sergey Levine",
        "authorids": "~Zhiyuan_Zhou2;~Pranav_Atreya1;~You_Liang_Tan1;~Karl_Pertsch1;~Sergey_Levine1",
        "gender": "M;;;;M",
        "homepage": "https://zhouzypaul.github.io;https://pranavatreya.github.io;https://youliangtan.github.io/;https://kpertsch.github.io/;https://people.eecs.berkeley.edu/~svlevine/",
        "dblp": ";317/4655;;211/7137;80/7594",
        "google_scholar": "unQVOJkAAAAJ;bQowYEYAAAAJ;;https://scholar.google.com/citations?view_op=list_works;8R35rCwAAAAJ",
        "orcid": ";;;;",
        "linkedin": "zhiyuan-paul-zhou/;pranav-d-atreya;youliang-tan/;;",
        "or_profile": "~Zhiyuan_Zhou2;~Pranav_Atreya1;~You_Liang_Tan1;~Karl_Pertsch1;~Sergey_Levine1",
        "aff": "University of California, Berkeley;University of California, Berkeley;NVIDIA;University of California, Berkeley+Stanford University;University of California, Berkeley",
        "aff_domain": "berkeley.edu;berkeley.edu;nvidia.com;berkeley.edu+stanford.edu;berkeley.edu",
        "position": "PhD student;PhD student;Researcher;Postdoc+Postdoc;Associate Professor",
        "bibtex": "@inproceedings{\nzhou2025autoeval,\ntitle={AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World},\nauthor={Zhiyuan Zhou and Pranav Atreya and You Liang Tan and Karl Pertsch and Sergey Levine},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=isrcFrgwZp}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=isrcFrgwZp",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0+2;0",
        "aff_unique_norm": "University of California, Berkeley;NVIDIA;Stanford University",
        "aff_unique_dep": ";NVIDIA Corporation;",
        "aff_unique_url": "https://www.berkeley.edu;https://www.nvidia.com;https://www.stanford.edu",
        "aff_unique_abbr": "UC Berkeley;NVIDIA;Stanford",
        "aff_campus_unique_index": "0;0;0+2;0",
        "aff_campus_unique": "Berkeley;;Stanford",
        "aff_country_unique_index": "0;0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "jPHhft5tNo",
        "title": "JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platform for MARL, by bridging the Robotarium testbed with existing MARL software infrastructure. However, MARBLER lacks support for parallelization and GPU/TPU execution, making the platform prohibitively slow compared to modern MARL environments and hindering adoption. We contribute JaxRobotarium, a  Jax-powered end-to-end simulation, learning, deployment, and benchmarking platform for the Robotarium. JaxRobotarium enables rapid training and deployment of multi-robot reinforcement learning (MRRL) policies with realistic robot dynamics and safety constraints, supporting both parallelization and hardware acceleration. Our generalizable learning interface provides an easy-to-use integration with SOTA MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight standardized coordination scenarios, including four novel scenarios that bring established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a realistic robotics setting. We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation.",
        "keywords": "Multi-Robot Learning;Sim2Real Deployment;Benchmarking;Jax",
        "primary_area": "",
        "supplementary_material": "/attachment/5513f940992695e34cf11b76af5e9ff38521c7a9.zip",
        "author": "Shalin Jain;Jiazhen Liu;Siva Kailas;Harish Ravichandar",
        "authorids": "~Shalin_Jain1;~Jiazhen_Liu4;~Siva_Kailas1;~Harish_Ravichandar1",
        "gender": "M;F;M;",
        "homepage": ";;https://skailas.github.io/;http://harishravichandar.com/",
        "dblp": ";;278/3065;237/9959",
        "google_scholar": ";x4OzGCwAAAAJ;N1pDd2IAAAAJ;d2HP6SMAAAAJ",
        "orcid": ";;0000-0002-9247-8882;0000-0002-6635-2637",
        "linkedin": "shalin-jain/;;siva-kailas-a35316138/;",
        "or_profile": "~Shalin_Jain1;~Jiazhen_Liu4;~Siva_Kailas1;~Harish_Ravichandar1",
        "aff": "Georgia Institute of Technology;Georgia Institute of Technology;College of Computing, Georgia Institute of Technology;Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;cc.gatech.edu;gatech.edu",
        "position": "MS student;PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\njain2025jaxrobotarium,\ntitle={JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes},\nauthor={Shalin Jain and Jiazhen Liu and Siva Kailas and Harish Ravichandar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=jPHhft5tNo}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=jPHhft5tNo",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.gatech.edu",
        "aff_unique_abbr": "Georgia Tech",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Atlanta",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "jU7AbGq3se",
        "title": "Steering Your Diffusion Policy with Latent Space Reinforcement Learning",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Robotic control policies learned from human demonstrations have achieved impressive results in many real-world applications. However, in scenarios where initial performance is not satisfactory, as is often the case in novel open-world settings, such behavioral cloning (BC)-learned policies typically require collecting additional human demonstrations to further improve their behavior---an expensive and time-consuming process. In contrast, reinforcement learning (RL) holds the promise of enabling autonomous online policy improvement, but often falls short of achieving this due to the large number of samples it typically requires. In this work we take steps towards enabling fast autonomous adaptation of BC-trained policies via efficient real-world RL. Focusing in particular on diffusion policies---a state-of-the-art BC methodology---we propose *diffusion steering via reinforcement learning* (DSRL): adapting the BC policy by running RL over its latent-noise space. We show that DSRL is highly sample efficient, requires only black-box access to the BC policy, and enables effective real-world autonomous policy improvement. Furthermore, DSRL avoids many of the challenges associated with finetuning diffusion policies, obviating the need to modify the weights of the base policy at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks, and for adapting pretrained generalist policies, illustrating its sample efficiency and effective performance at real-world policy improvement.",
        "keywords": "diffusion policies;reinforcement learning;finetuning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Andrew Wagenmaker;Yunchu Zhang;Mitsuhiko Nakamoto;Seohong Park;Waleed Yagoub;Anusha Nagabandi;Abhishek Gupta;Sergey Levine",
        "authorids": "~Andrew_Wagenmaker1;~Yunchu_Zhang1;~Mitsuhiko_Nakamoto1;~Seohong_Park1;~Waleed_Yagoub1;~Anusha_Nagabandi1;~Abhishek_Gupta1;~Sergey_Levine1",
        "gender": "M;M;;;Not Specified;F;M;M",
        "homepage": "https://wagenmaker.github.io;https://yunchuzhang.github.io/;https://nakamotoo.github.io/;https://seohong.me/;https://homes.cs.washington.edu/~walyag/;;https://homes.cs.washington.edu/~abhgupta/;https://people.eecs.berkeley.edu/~svlevine/",
        "dblp": "195/1036;;;227/6308;;;18/6404-4;80/7594",
        "google_scholar": "ym8AZSIAAAAJ;;wIDVzroAAAAJ;;;;1wLVDP4AAAAJ;8R35rCwAAAAJ",
        "orcid": ";;;;;;;",
        "linkedin": ";;;;waleedyagoub/;;;",
        "or_profile": "~Andrew_Wagenmaker1;~Yunchu_Zhang1;~Mitsuhiko_Nakamoto1;~Seohong_Park1;~Waleed_Yagoub1;~Anusha_Nagabandi1;~Abhishek_Gupta1;~Sergey_Levine1",
        "aff": "University of California, Berkeley;;University of California, Berkeley;University of California, Berkeley;Department of Computer Science, University of Washington;University of California, Berkeley;University of Washington;University of California, Berkeley",
        "aff_domain": "berkeley.edu;;eecs.berkeley.edu;berkeley.edu;cs.washington.edu;berkeley.edu;uw.edu;berkeley.edu",
        "position": "Postdoc;;PhD student;PhD student;Undergrad student;PhD student;Assistant Professor;Associate Professor",
        "bibtex": "@inproceedings{\nwagenmaker2025steering,\ntitle={Steering Your Diffusion Policy with Latent Space Reinforcement Learning},\nauthor={Andrew Wagenmaker and Yunchu Zhang and Mitsuhiko Nakamoto and Seohong Park and Waleed Yagoub and Anusha Nagabandi and Abhishek Gupta and Sergey Levine},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=jU7AbGq3se}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=jU7AbGq3se",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0;2;0;2;0",
        "aff_unique_norm": "University of California, Berkeley;;University of Washington",
        "aff_unique_dep": ";;Department of Computer Science",
        "aff_unique_url": "https://www.berkeley.edu;;https://www.washington.edu",
        "aff_unique_abbr": "UC Berkeley;;UW",
        "aff_campus_unique_index": "0;0;0;2;0;0",
        "aff_campus_unique": "Berkeley;;Seattle",
        "aff_country_unique_index": "0;0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "jZRz7Hvsyd",
        "title": "Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Humans naturally exhibit bilateral symmetry in their gross manipulation skills, effortlessly mirroring simple actions between left and right hands. Bimanual robots\u2014which also feature bilateral symmetry\u2014should similarly exploit this property to perform tasks with either hand. Unlike humans, who often favor a dominant hand for fine dexterous skills, robots should ideally execute ambidextrous manipulation with equal proficiency. To this end, we introduce SYMDEX (SYMmetric DEXterity), a reinforcement learning framework for ambidextrous bi-manipulation that leverages the robot's inherent bilateral symmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation tasks into per-hand subtasks and trains dedicated policies for each. By exploiting bilateral symmetry via equivariant neural networks, experience from one arm is inherently leveraged by the opposite arm. We then distill the subtask policies into a global ambidextrous policy that is independent of the hand-task assignment. We evaluate SYMDEX on six challenging simulated manipulation tasks and demonstrate successful real-world deployment on two of them. Our approach outperforms baselines on more complex, asymmetric tasks, where the left and right hands perform different roles. We further demonstrate SYMDEX\u2019s scalability by extending it to a four-arm manipulation setup, where our symmetry-aware policies enable effective multi-arm collaboration and coordination. Our results highlight how structural symmetry as inductive bias in policy learning enhances sample efficiency, robustness, and generalization across diverse dexterous manipulation tasks.",
        "keywords": "Bimanual Dexterous Manipulation;Sim-to-Real Transfer;Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zechu Li;Yufeng Jin;Daniel Ordonez-Apraez;Claudio Semini;Puze Liu;Georgia Chalvatzaki",
        "authorids": "~Zechu_Li1;~Yufeng_Jin2;~Daniel_Ordonez-Apraez1;~Claudio_Semini1;~Puze_Liu1;~Georgia_Chalvatzaki1",
        "gender": "M;M;M;M;M;F",
        "homepage": ";https://pearl-lab.com/people/yufeng-jin/;https://daniel-ordonez-apraez.netlify.app/;http://dls.iit.it;https://puzeliu.github.io/;https://www.ias.informatik.tu-darmstadt.de/Team/GeorgiaChalvatzaki",
        "dblp": ";;;;292/4069;145/3334",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;https://scholar.google.com.co/citations?user=zNBUoy4AAAAJ;;zg-FMloAAAAJ;https://scholar.google.gr/citations?user=mlho5FkAAAAJ",
        "orcid": ";;0000-0002-9793-2482;;0000-0001-6887-7704;",
        "linkedin": ";yufeng-jin-84414327b/;danfoa/;;;",
        "or_profile": "~Zechu_Li1;~Yufeng_Jin2;~Daniel_Ordonez-Apraez1;~Claudio_Semini1;~Puze_Liu1;~Georgia_Chalvatzaki1",
        "aff": "Columbia University+Massachusetts Institute of Technology+Technische Universit\u00e4t Darmstadt;Technische Universit\u00e4t Darmstadt;Universit\u00e0 degli Studi di Genova, Istituto Italiano di Tecnologia;Istituto Italiano di Tecnologia;German Research Center for AI;Technische Universit\u00e4t Darmstadt+Technische Universit\u00e4t Darmstadt",
        "aff_domain": "columbia.edu+mit.edu+tu-darmstadt.de;tu-darmstadt.de;iit.it;iit.it;dfki.de;tu-darmstadt.de+tu-darmstadt.de",
        "position": "Undergrad student+Researcher+MS student;PhD student;PhD student;Principal Researcher;Researcher;Full Professor+Assistant Professor",
        "bibtex": "@inproceedings{\nli2025morphologically,\ntitle={Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation},\nauthor={Zechu Li and Yufeng Jin and Daniel Ordonez-Apraez and Claudio Semini and Puze Liu and Georgia Chalvatzaki},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=jZRz7Hvsyd}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=jZRz7Hvsyd",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1+2;2;3;4;5;2+2",
        "aff_unique_norm": "Columbia University;Massachusetts Institute of Technology;Technische Universit\u00e4t Darmstadt;Universit\u00e0 degli Studi di Genova;Istituto Italiano di Tecnologia;German Research Center for Artificial Intelligence",
        "aff_unique_dep": ";;;;;",
        "aff_unique_url": "https://www.columbia.edu;https://web.mit.edu;https://www.tu-darmstadt.de;https://www.unige.it;https://www.iit.it;https://www.dfki.de/",
        "aff_unique_abbr": "Columbia;MIT;TUD;UniGe;IIT;DFKI",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0+1;1;2;2;1;1+1",
        "aff_country_unique": "United States;Germany;Italy"
    },
    {
        "id": "jedBaI1fgU",
        "title": "BranchOut: Capturing Realistic Multimodality in Autonomous Driving Decisions",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Modeling the nuanced, multimodal nature of human driving remains a core challenge for autonomous systems, as existing methods often fail to capture the diversity of plausible behaviors in complex real-world scenarios. In this work, we introduce a novel benchmark and end-to-end planner for modeling realistic multimodality in autonomous driving decisions. \nWe propose a Gaussian Mixture Model (GMM)-based diffusion model designed to explicitly capture human-like, multimodal driving decisions in diverse contexts. Our model achieves state-of-the-art performance on current benchmarks, but reveals weaknesses in standard evaluation practices, which rely on single ground-truth trajectories or coarse closed-loop metrics while often penalizing diverse yet plausible alternatives. To address this limitation, we further develop a human-in-the-loop simulation benchmark that enables finer-grained evaluations and measures multimodal realism in challenging driving settings. Our code, models, and benchmark data will be released to promote more accurate and human-aware evaluation of autonomous driving models.",
        "keywords": "Autonomous Driving;Human-in-the-Loop Simulation;Multi-modal Planning and Evaluation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Hee Jae Kim;Zekai Yin;Lei Lai;Jason Lee;Eshed Ohn-Bar",
        "authorids": "~Hee_Jae_Kim1;~Zekai_Yin1;~Lei_Lai1;~Jason_Lee6;~Eshed_Ohn-Bar4",
        "gender": "F;M;;M;Not Specified",
        "homepage": "https://hee-jae-kim.github.io/;https://github.com/Ezzekk;;https://jasonlee328.github.io/;https://eshed1.github.io/",
        "dblp": "130/9539;353/1164;;;121/0305",
        "google_scholar": "9i7QbK0AAAAJ;;;;p9zVBV4AAAAJ",
        "orcid": ";;;;",
        "linkedin": "https://linkedin.com/in/hee-jae-kim-aa45261a0;;;jason-lee-b3479a17b/;",
        "or_profile": "~Hee_Jae_Kim1;~Zekai_Yin1;~Lei_Lai1;~Jason_Lee6;~Eshed_Ohn-Bar4",
        "aff": "Boston University;Boston University;;Allen Institute for Artificial Intelligence+University of Washington;Boston University",
        "aff_domain": "bu.edu;bu.edu;;allenai.org+uw.edu;bu.edu",
        "position": "PhD student;MS student;;Student Researcher+MS student;Assistant Professor",
        "bibtex": "@inproceedings{\nkim2025branchout,\ntitle={BranchOut: Capturing Realistic Multimodality in Autonomous Driving Decisions},\nauthor={Hee Jae Kim and Zekai Yin and Lei Lai and Jason Lee and Eshed Ohn-Bar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=jedBaI1fgU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=jedBaI1fgU",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;2+3;0",
        "aff_unique_norm": "Boston University;;Allen Institute for Artificial Intelligence;University of Washington",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.bu.edu;;https://allenai.org;https://www.washington.edu",
        "aff_unique_abbr": "BU;;AI2;UW",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "jnpILGz9gQ",
        "title": "Streaming Flow Policy: Simplifying diffusion/flow-matching policies by treating action trajectories as flow trajectories",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Recent advances in diffusion$/$flow-matching policies have enabled imitation learning of complex, multi-modal action trajectories. However, they are computationally expensive because they sample a *trajectory of trajectories*\u2014a diffusion$/$flow trajectory of action trajectories. They discard intermediate action trajectories, and must wait for the sampling process to complete before any actions can be executed on the robot. We simplify diffusion$/$flow policies by *treating action trajectories as flow trajectories*. Instead of starting from pure noise, our algorithm samples from a narrow Gaussian around the last action. Then, it incrementally integrates a velocity field learned via flow matching to produce a sequence of actions that constitute a *single* trajectory. This enables actions to be streamed to the robot on-the-fly *during* the flow sampling process, and is well-suited for receding horizon policy execution. Despite streaming, our method retains the ability to model multi-modal behavior. We train flows that *stabilize* around demonstration trajectories to reduce distribution shift and improve imitation learning performance. Streaming flow policy outperforms prior methods while enabling faster policy execution and tighter sensorimotor loops for learning-based robot control.",
        "keywords": "imitation learning;diffusion policy;flow matching",
        "primary_area": "",
        "supplementary_material": "/attachment/e2973afa4f7edb9a5308cc877d10193b04e7590c.zip",
        "author": "Sunshine Jiang;Xiaolin Fang;Nicholas Roy;Tom\u00e1s Lozano-P\u00e9rez;Leslie Pack Kaelbling;Siddharth Ancha",
        "authorids": "~Sunshine_Jiang1;~Xiaolin_Fang1;~Nicholas_Roy1;~Tom\u00e1s_Lozano-P\u00e9rez1;~Leslie_Pack_Kaelbling1;~Siddharth_Ancha1",
        "gender": "F;;M;M;F;M",
        "homepage": "https://xinyunsunshine.github.io/;https://fang-xiaolin.github.io/;;http://people.csail.mit.edu/tlp/;http://people.csail.mit.edu/lpk/;https://siddancha.github.io",
        "dblp": ";73/7258-2;32/2668;90/752;k/LesliePackKaelbling;182/2096",
        "google_scholar": ";https://scholar.google.com/citations?hl=en;;gQOKAggAAAAJ;IcasIiwAAAAJ;",
        "orcid": ";;;;0000-0001-6054-7145;",
        "linkedin": "https://www.linkedin.com/public-profile/settings?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_self_edit_contact-info%3B3XSMfK%2FFQQ26GfCvn3ctGw%3D%3D;;;;;",
        "or_profile": "~Sunshine_Jiang1;~Xiaolin_Fang1;~Nicholas_Roy1;~Tom\u00e1s_Lozano-P\u00e9rez1;~Leslie_Pack_Kaelbling1;~Siddharth_Ancha1",
        "aff": "Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology;Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;mit.edu;mit.edu;mit.edu;mit.edu",
        "position": "Undergrad student;PhD student;Full Professor;Full Professor;Full Professor;Researcher",
        "bibtex": "@inproceedings{\njiang2025streaming,\ntitle={Streaming Flow Policy: Simplifying diffusion/flow-matching policies by treating action trajectories as flow trajectories},\nauthor={Sunshine Jiang and Xiaolin Fang and Nicholas Roy and Tom{\\'a}s Lozano-P{\\'e}rez and Leslie Pack Kaelbling and Siddharth Ancha},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=jnpILGz9gQ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=jnpILGz9gQ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;0;0",
        "aff_unique_norm": "Massachusetts Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://web.mit.edu",
        "aff_unique_abbr": "MIT",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "jr1Gjpjmqc",
        "title": "Sample-Efficient Online Control Policy Learning with Real-Time Recursive Model Updates",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Data-driven control methods need to be sample-efficient and lightweight, especially when data acquisition and computational resources are limited---such as during learning on hardware. Most modern data-driven methods require large datasets and struggle with real-time updates of models, limiting their performance in dynamic environments. Koopman theory formally represents nonlinear systems as linear models over observables, and Koopman representations can be determined from data in an optimization-friendly setting with potentially rapid model updates. In this paper, we present a highly sample-efficient, Koopman-based learning pipeline: Recursive Koopman Learning (RKL). We identify sufficient conditions for model convergence and provide formal algorithmic analysis supporting our claim that RKL is lightweight and fast, with complexity independent of dataset size. We validate our method on a simulated planar two-link arm and a hybrid nonlinear hardware system with soft actuators, showing that real-time recursive Koopman model updates improve the sample efficiency and stability of data-driven controller synthesis---requiring only <10% of the data compared to benchmarks. The high-performance C++ codebase will be open-sourced.",
        "keywords": "Koopman Operator; Control",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Zixin Zhang;James Avtges;Todd Murphey",
        "authorids": "~Zixin_Zhang7;~James_Avtges1;~Todd_Murphey1",
        "gender": "M;M;M",
        "homepage": "https://www.zixinatom990.com/;https://javtges.github.io/;https://murpheylab.github.io/",
        "dblp": ";;",
        "google_scholar": "2OepJekAAAAJ;;",
        "orcid": ";;0000-0003-2262-8176",
        "linkedin": ";;",
        "or_profile": "~Zixin_Zhang7;~James_Avtges1;~Todd_Murphey1",
        "aff": "Northwestern University;Northwestern University;",
        "aff_domain": "u.northwestern.edu;northwestern.edu;",
        "position": "PhD student;PhD student;",
        "bibtex": "@inproceedings{\nzhang2025sampleefficient,\ntitle={Sample-Efficient Online Control Policy Learning with Real-Time Recursive Model Updates},\nauthor={Zixin Zhang and James Avtges and Todd Murphey},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=jr1Gjpjmqc}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=jr1Gjpjmqc",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1",
        "aff_unique_norm": "Northwestern University;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.northwestern.edu;",
        "aff_unique_abbr": "NU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "kUA2ec94LI",
        "title": "Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Efficient robot locomotion often requires balancing task performance with energy expenditure. A common approach in reinforcement learning (RL) is to penalize energy use directly in the reward function. This requires carefully weighting the reward terms to avoid undesirable trade-offs where energy minimization harms task success or vice versa. In this work, we propose a hyperparameter-free gradient optimization method to minimize energy without conflicting with task performance. Inspired by recent works in multitask learning, our method applies policy gradient projection between task and energy objectives to promote non-conflicting updates. We evaluate this technique on standard locomotion benchmarks of DM-Control and HumanoidBench and demonstrate a reduction of $64$% energy usage while maintaining comparable task performance. Further, we conduct experiments on a Unitree GO2 quadruped showcasing Sim2Real transfer of energy efficient policies. Our method is easy to implement in standard RL pipelines with minimal code changes, and offers a principled alternative to reward shaping for energy efficient control policies.",
        "keywords": "Energy efficient locomotion;Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Skand Peri;Akhil Perincherry;Bikram Pandit;Stefan Lee",
        "authorids": "~Skand_Peri1;~Akhil_Perincherry1;~Bikram_Pandit1;~Stefan_Lee1",
        "gender": ";;M;",
        "homepage": ";;https://bikcrum.com/;",
        "dblp": ";;;",
        "google_scholar": ";;bikram.pandit;",
        "orcid": ";;0009-0007-3601-6118;",
        "linkedin": ";;bikcrum/;",
        "or_profile": "~Skand_Peri1;~Akhil_Perincherry1;~Bikram_Pandit1;~Stefan_Lee1",
        "aff": ";;;",
        "aff_domain": ";;;",
        "position": ";;;",
        "bibtex": "@inproceedings{\nperi2025nonconflicting,\ntitle={Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control},\nauthor={Skand Peri and Akhil Perincherry and Bikram Pandit and Stefan Lee},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=kUA2ec94LI}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kUA2ec94LI",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0",
        "aff_unique_norm": "",
        "aff_unique_dep": "",
        "aff_unique_url": "",
        "aff_unique_abbr": "",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "",
        "aff_country_unique": ""
    },
    {
        "id": "kXhOmN3x18",
        "title": "ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Learning real-world robotic manipulation is challenging, particularly when limited demonstrations are available. Existing methods for few-shot manipulation often rely on simulation-augmented data or pre-built modules like grasping and pose estimation, which struggle with sim-to-real gaps and lack extensibility. While large-scale imitation pre-training shows promise, adapting these general-purpose policies to specific tasks in data-scarce settings remains unexplored. To achieve this, we propose ControlVLA, a novel framework that bridges pre-trained VLA models with object-centric representations via a ControlNet-style architecture for efficient fine-tuning. Specifically, to introduce object-centric conditions without overwriting prior knowledge, ControlVLA zero-initializes a set of projection layers, allowing them to gradually adapt the pre-trained manipulation policies.\nIn real-world experiments across 6 diverse tasks, including pouring cubes and folding clothes, our method achieves a 76.7\\% success rate while requiring only 10-20 demonstrations --- a significant improvement over traditional approaches that require more than 100 demonstrations to achieve comparable success. \nAdditional experiments highlight ControlVLA's extensibility to long-horizon tasks and robustness to unseen objects and backgrounds.",
        "keywords": "Robotic manipulation;Imitation learning;Few-shot learning",
        "primary_area": "",
        "supplementary_material": "/attachment/ae304f4d0854db1a3d5a574b5a00a43c873c92fb.zip",
        "author": "Puhao Li;Yingying Wu;Ziheng Xi;Wanlin Li;Yuzhe Huang;Zhiyuan Zhang;Yinghan Chen;Jianan Wang;Song-Chun Zhu;Tengyu Liu;Siyuan Huang",
        "authorids": "~Puhao_Li1;~Yingying_Wu2;~Ziheng_Xi1;~Wanlin_Li1;~Yuzhe_Huang3;~Zhiyuan_Zhang5;~Yinghan_Chen2;~Jianan_Wang2;~Song-Chun_Zhu1;~Tengyu_Liu1;~Siyuan_Huang2",
        "gender": "M;F;M;M;M;M;M;F;M;M;M",
        "homepage": "https://xiaoyao-li.github.io/;;https://github.com/zhenqis123;;https://github.com/mrHfrombuaa;;https://pku.ai/author/yinghan-chen/;https://scholar.google.com/citations?user=mt5mvZ8AAAAJ&hl=en;https://zhusongchun.net/;https://tengyu.ai;https://siyuanhuang.com/",
        "dblp": "330/4121.html;;;;;;;49/6053,;10/10313;257/1450;62/885-1",
        "google_scholar": "https://scholar.google.at/citations?user=HTsO18AAAAAJ;;;n_mYangAAAAJ;;https://scholar.google.com/citations?hl=en;;mt5mvZ8AAAAJ;https://scholar.google.com.tw/citations?user=Al8dyb4AAAAJ;;1NN7Ee8AAAAJ",
        "orcid": "0009-0003-2696-9346;0009-0006-6738-4493;;;;;;;;0000-0003-4006-1740;",
        "linkedin": ";;;;;;;;;;",
        "or_profile": "~Puhao_Li1;~Yingying_Wu2;~Ziheng_Xi1;~Wanlin_Li1;~Yuzhe_Huang3;~Zhiyuan_Zhang5;~Yinghan_Chen2;~Jianan_Wang2;~Song-Chun_Zhu1;~Tengyu_Liu1;~Siyuan_Huang2",
        "aff": "Tsinghua University;Tsinghua University ;Tsinghua University;BIGAI:Beijing Institute for General Artificial Intelligence;Beihang University;University of California, Los Angeles;University of Cambridge+Peking University;Astribot;Beijing Institute for General Artificial Intelligence+Peking University;Beijing Institute of General Artificial Intelligence;Beijing Institute for General Artificial Intelligence",
        "aff_domain": "tsinghua.edu.cn;mails.tsinghua.edu.cn;tsinghua.edu.cn;bigai.ai;buaa.edu.cn;ucla.edu;cam.ac.uk+stu.pku.edu.cn;astribot.com;bigai.ai+pku.edu.cn;bigai.ai;bigai.ai",
        "position": "PhD student;Undergrad student;Undergrad student;Researcher;MS student;MS student;Undergrad student+Undergrad student;Researcher;Principal Researcher+Full Professor;Researcher;Researcher",
        "bibtex": "@inproceedings{\nli2025controlvla,\ntitle={Control{VLA}: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models},\nauthor={Puhao Li and Yingying Wu and Ziheng Xi and Wanlin Li and Yuzhe Huang and Zhiyuan Zhang and Yinghan Chen and Jianan Wang and Song-Chun Zhu and Tengyu Liu and Siyuan Huang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=kXhOmN3x18}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kXhOmN3x18",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1;2;3;4+5;6;7+5;8;7",
        "aff_unique_norm": "Tsinghua University;BIGAI:Beijing Institute for General Artificial Intelligence;Beihang University;University of California, Los Angeles;University of Cambridge;Peking University;Astribot;Beijing Institute for General Artificial Intelligence;Beijing Institute of General Artificial Intelligence",
        "aff_unique_dep": ";;;;;;;;",
        "aff_unique_url": "https://www.tsinghua.edu.cn;;http://www.buaa.edu.cn/;https://www.ucla.edu;https://www.cam.ac.uk;http://www.pku.edu.cn;;http://www.bigaiai.org/;http://www.bigaiai.cn",
        "aff_unique_abbr": "THU;;BUAA;UCLA;Cambridge;Peking U;;BIGAI;BIGAI",
        "aff_campus_unique_index": "1;2;",
        "aff_campus_unique": ";Los Angeles;Cambridge",
        "aff_country_unique_index": "0;0;0;0;2;3+0;0+0;0;0",
        "aff_country_unique": "China;;United States;United Kingdom"
    },
    {
        "id": "kbrb6W7bUL",
        "title": "SDS \u2013 See it, Do it, Sorted: Quadruped Skill Synthesis from Single Video Demonstration",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Imagine a robot learning locomotion skills from any single video, without labels or reward engineering. We introduce SDS (\"See it. Do it. Sorted.\"), an automated pipeline for skill acquisition from unstructured video demonstrations. Using GPT-4o, SDS applies novel prompting techniques, in the form of spatio-temporal grid-based visual encoding (Gv) and structured input decomposition (SUS). These produce executable reward functions (RF) from raw input videos. The RFs are used to train PPO policies and are optimized through closed-loop evolution, using training footage and performance metrics as self-supervised signals. SDS allows quadrupeds (e.g., Unitree Go1) to learn four gaits\u2014trot, bound, pace, and hop\u2014achieving 100% gait matching fidelity, Dynamic Time Warping (DTW) distance in the order of 10^-6, and stable locomotion with zero failures, both in simulation and the real world. SDS generalizes to morphologically different quadrupeds (e.g., ANYmal) and outperforms prior work in data efficiency, training time, and engineering effort. Our code is open-source under: https://sdsreview.github.io/SDS_ANONYM/",
        "keywords": "Skill Imitation;Robot Skill Learning;Quadrupedal Robot",
        "primary_area": "",
        "supplementary_material": "/attachment/9ceb26aaabcfbced02b3185156997aa4e96c29cc.zip",
        "author": "Maria Stamatopoulou;Jeffrey Li;Dimitrios Kanoulas",
        "authorids": "~Maria_Stamatopoulou1;jeffrey.li.20@alumni.ucl.ac.uk;~Dimitrios_Kanoulas1",
        "gender": "F;;M",
        "homepage": ";;https://dkanou.github.io",
        "dblp": ";;20/4287.html",
        "google_scholar": ";;cE8_5EsAAAAJ",
        "orcid": ";;0000-0002-3684-1472",
        "linkedin": "www.linkedin.com/in/maria-stamatopoulou-797070178;;",
        "or_profile": "~Maria_Stamatopoulou1;jeffrey.li.20@alumni.ucl.ac.uk;~Dimitrios_Kanoulas1",
        "aff": "University College London, University of London;;University College London, University of London+Athena Research and Innovation Centre",
        "aff_domain": "ucl.ac.uk;;ucl.ac.uk+athenarc.gr",
        "position": "PhD student;;Full Professor+Researcher",
        "bibtex": "@inproceedings{\nstamatopoulou2025sds,\ntitle={{SDS} {\\textendash} See it, Do it, Sorted: Quadruped Skill Synthesis from Single Video Demonstration},\nauthor={Maria Stamatopoulou and Jeffrey Li and Dimitrios Kanoulas},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=kbrb6W7bUL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kbrb6W7bUL",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0+2",
        "aff_unique_norm": "University College London;;Athena Research and Innovation Centre",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucl.ac.uk;;https://www.athena rc.gr",
        "aff_unique_abbr": "UCL;;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+2",
        "aff_country_unique": "United Kingdom;;Greece"
    },
    {
        "id": "ksOrtEgIC0",
        "title": "AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Scaling up robotic imitation learning for real-world applications requires efficient and scalable demonstration collection methods. While teleoperation is effective, it depends on costly and inflexible robot platforms. In-the-wild demonstrations offer a promising alternative, but existing collection devices have key limitations: handheld setups offer limited observational coverage, and whole-body systems often require fine-tuning with robot data due to domain gaps. To address these challenges, we present AirExo-2, a low-cost exoskeleton system for large-scale in-the-wild data collection, along with visual adaptors that transform collected data into pseudo-robot demonstrations suitable for policy learning. We further introduce RISE-2, a generalizable imitation learning policy that fuses 3D spatial and 2D semantic perception for robust manipulations. Experiments show that RISE-2 outperforms prior state-of-the-art methods on both in-domain and generalization evaluations. Trained solely on adapted in-the-wild data produced by AirExo-2, RISE-2 achieves comparable performance to policies trained with teleoperated data, highlighting the effectiveness and potential of AirExo-2 for scalable and generalizable imitation learning.",
        "keywords": "Robotic Manipulation;Scalable Data Collection;Generalizable Imitation Policy",
        "primary_area": "",
        "supplementary_material": "/attachment/7ea198b88d62795239bdf66277a8e2ce0bf1c507.zip",
        "author": "Hongjie Fang;Chenxi Wang;Yiming Wang;Jingjing Chen;Shangning Xia;Jun Lv;Zihao He;Xiyan Yi;Yunhan Guo;Xinyu Zhan;Lixin Yang;Weiming Wang;Cewu Lu;Hao-Shu Fang",
        "authorids": "~Hongjie_Fang1;~Chenxi_Wang3;~Yiming_Wang16;~Jingjing_Chen6;~Shangning_Xia1;~Jun_Lv2;~Zihao_He4;~Xiyan_Yi2;~Yunhan_Guo1;~Xinyu_Zhan1;~Lixin_Yang1;~Weiming_Wang2;~Cewu_Lu3;~Hao-Shu_Fang1",
        "gender": "M;;M;F;M;M;M;;F;Not Specified;M;M;M;",
        "homepage": "https://tonyfang.net/;;https://www.mvig.org/;https://github.com/Junxix;https://github.com/Xiashangning;https://lyuj1998.github.io/;https://alan-heoooh.github.io/;;https://github.com/EstelleGuo;https://github.com/kelvin34501;https://lixiny.github.io;;https://www.mvig.org/;",
        "dblp": ";;;;;;;;;257/1454-1;59/4517-1;86/3464;;",
        "google_scholar": "Kyio_RAAAAAJ;;;;;DtaiAjwAAAAJ;fzKelVAAAAAJ;;;WurpqEMAAAAJ;https://scholar.google.com/citations?hl=en;;https://scholar.google.com.tw/citations?user=QZVQEWAAAAAJ;OT16tRAAAAAJ",
        "orcid": ";0000-0002-9420-3495;;;;;;;;0009-0004-7859-2592;0000-0001-6366-3192;;;",
        "linkedin": ";;;;;;;;;;;;;",
        "or_profile": "~Hongjie_Fang1;~Chenxi_Wang3;~Yiming_Wang16;~Jingjing_Chen6;~Shangning_Xia1;~Jun_Lv2;~Zihao_He4;~Xiyan_Yi2;~Yunhan_Guo1;~Xinyu_Zhan1;~Lixin_Yang1;~Weiming_Wang2;~Cewu_Lu3;~Hao-Shu_Fang1",
        "aff": "Shanghai Jiaotong University;;;;Shanghai Jiaotong University;Shanghai Jiaotong University;University of California, San Diego+Shanghai Jiaotong University;;Shanghai Jiaotong University;Shanghai Jiaotong University;Shanghai Jiaotong University;Shanghai Jiaotong University;Shanghai Jiaotong University;Massachusetts Institute of Technology",
        "aff_domain": "sjtu.edu.cn;;;;sjtu.edu.cn;sjtu.edu.cn;ucsd.edu+sjtu.edu.cn;;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;sjtu.edu.cn;mit.edu",
        "position": "PhD student;;;;MS student;PhD student;Intern+Undergrad student;;Undergrad student;PhD student;Assistant Professor;Associate Professor;Full Professor;Postdoc",
        "bibtex": "@inproceedings{\nfang2025airexo,\ntitle={AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons},\nauthor={Hongjie Fang and Chenxi Wang and Yiming Wang and Jingjing Chen and Shangning Xia and Jun Lv and Zihao He and Xiyan Yi and Yunhan Guo and Xinyu Zhan and Lixin Yang and Weiming Wang and Cewu Lu and Hao-Shu Fang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ksOrtEgIC0}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ksOrtEgIC0",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            14,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;1;0;0;2+0;1;0;0;0;0;0;3",
        "aff_unique_norm": "Shanghai Jiao Tong University;;University of California, San Diego;Massachusetts Institute of Technology",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.sjtu.edu.cn;;https://www.ucsd.edu;https://web.mit.edu",
        "aff_unique_abbr": "SJTU;;UCSD;MIT",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0;2+0;0;0;0;0;0;2",
        "aff_country_unique": "China;;United States"
    },
    {
        "id": "kto4zVmo4w",
        "title": "One View, Many Worlds: Single-Image to 3D object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Estimating the 6D pose of arbitrary objects from a single reference image is a critical yet challenging task in robotics, especially considering the long-tail distribution of real-world instances. While category-level and model-based approaches have achieved notable progress, they remain limited in generalizing to unseen objects under one-shot settings. In this work, we propose a novel pipeline for fast and accurate one-shot 6D pose and scale estimation. Leveraging recent advances in single-view 3D generation, we first build high-fidelity textured meshes without requiring known object poses. To resolve scale ambiguity, we introduce a coarse-to-fine alignment module that estimates both object size and initial pose by matching 2D-3D features with depth information. We then generate a diversified set of plausible 3D models using text-guided generative augmentation and render them with Blender to synthesize large-scale, domain-randomized training data for pose estiamtion. This synthetic data bridges the domain gap and enables robust fine-tuning of pose estimators. Our method achieves state-of-the-art results on several 6D pose benchmarks, and we further validate its effectiveness on a newly collected in-the-wild dataset. Finally, we integrate our system with a dexterous hand, demonstrating its robustness in real-world robotic grasping tasks. All code, data, and models will be released to foster future research.",
        "keywords": "Unseen Object Pose Estimation;Generative Model;Robot Manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/94c17f53e4a1c93570507b2448cac1fb07ba8448.zip",
        "author": "Zheng Geng;Nan Wang;Shaocong Xu;Chongjie Ye;Bohan Li;Zhaoxi Chen;Sida Peng;Hao Zhao",
        "authorids": "~Zheng_Geng1;~Nan_Wang14;~Shaocong_Xu1;~Chongjie_Ye1;~Bohan_Li6;~Zhaoxi_Chen1;~Sida_Peng1;~Hao_Zhao1",
        "gender": "M;M;M;M;M;M;M;M",
        "homepage": ";https://bigcileng.github.io/;https://daniellli.github.io/;https://hugoycj.github.io/;https://scholar.google.com/citations?user=V-YdQiAAAAAJ&hl=zh-CN;https://frozenburning.github.io/;http://pengsida.net/;https://sites.google.com/view/fromandto",
        "dblp": ";;;319/2660;;118/8512-9;232/3246;08/3737-2.html",
        "google_scholar": ";BWfLE6EAAAAJ;PvYOrK0AAAAJ;;V-YdQiAAAAAJ;HsV0WbwAAAAJ;;ygQznUQAAAAJ",
        "orcid": "0009-0008-7400-3091;0009-0009-7540-779X;0000-0001-7525-0790;;0000-0002-6959-7517;0000-0003-3998-7044;;0000-0001-7903-581X",
        "linkedin": ";;;;;;;",
        "or_profile": "~Zheng_Geng1;~Nan_Wang14;~Shaocong_Xu1;~Chongjie_Ye1;~Bohan_Li6;~Zhaoxi_Chen1;~Sida_Peng1;~Hao_Zhao1",
        "aff": "Beijing Institute of Technology;Beijing Academy of Artificial Intelligence+Tongji University;Beijing Academy of Artificial Intelligence;The Chinese University of Hong Kong, Shenzhen;Shanghai Jiaotong University;Nanyang Technological University;Zhejiang University;Tsinghua University+Peking University",
        "aff_domain": "bit.edu.cn;baai.ac.cn+tongji.edu.cn;baai.ac.cn;cuhk.edu.cn;sjtu.edu.cn;ntu.edu.sg;zju.edu.cn;tsinghua.edu.cn+pku.edu.cn",
        "position": "Undergrad student;Researcher+MS student;Researcher;PhD student;PhD student;PhD student;Assistant Professor;Assistant Professor+Postdoc",
        "bibtex": "@inproceedings{\ngeng2025one,\ntitle={One View, Many Worlds: Single-Image to 3D object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation},\nauthor={Zheng Geng and Nan Wang and Shaocong Xu and Chongjie Ye and Bohan Li and Zhaoxi Chen and Sida Peng and Hao Zhao},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=kto4zVmo4w}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=kto4zVmo4w",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1+2;1;3;4;5;6;7+8",
        "aff_unique_norm": "Beijing Institute of Technology;Beijing Academy of Artificial Intelligence;Tongji University;Chinese University of Hong Kong;Shanghai Jiao Tong University;Nanyang Technological University;Zhejiang University;Tsinghua University;Peking University",
        "aff_unique_dep": ";;;;;;;;",
        "aff_unique_url": "http://www.bit.edu.cn/;https://www.baaic.cn;https://www.tongji.edu.cn;https://www.cuhk.edu.cn;https://www.sjtu.edu.cn;https://www.ntu.edu.sg;https://www.zju.edu.cn;https://www.tsinghua.edu.cn;http://www.pku.edu.cn",
        "aff_unique_abbr": "BIT;BAAI;Tongji;CUHK;SJTU;NTU;ZJU;THU;Peking U",
        "aff_campus_unique_index": ";1;",
        "aff_campus_unique": ";Shenzhen",
        "aff_country_unique_index": "0;0+0;0;0;0;1;0;0+0",
        "aff_country_unique": "China;Singapore"
    },
    {
        "id": "lJWUourMTT",
        "title": "Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Kinodynamic motion planning is concerned with computing collision-free trajectories while abiding by the robot's dynamic constraints. This critical problem is often tackled using sampling-based planners (SBPs) that explore the robot's high-dimensional state space by constructing a search tree via action propagations. Although SBPs can offer global guarantees on completeness and solution quality, their performance is often hindered by slow exploration due to uninformed action sampling. Learning-based approaches can yield significantly faster runtimes, yet they fail to generalize to out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety, thus limiting their deployment on physical robots. We present Diffusion Tree (DiTree): a provably-generalizable framework leveraging diffusion policies (DPs) as informed samplers to efficiently guide state-space search within SBPs. DiTree  combines DP's ability to model complex distributions of expert trajectories, conditioned on local observations, with the completeness of SBPs, to yield provably-safe solutions within a few action propagation iterations for complex dynamical systems. We demonstrate DiTree's power with an implementation combining the popular RRT planner with a DP action sampler trained on a single environment. In comprehensive evaluations on OOD scenarios, DiTree has comparable runtimes to a standalone DP (4x faster than classical SBPs), while improving the success rate over DP and SBPs (on average).",
        "keywords": "Kinodynamic motion planning;diffusion models;nonlinear systems",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yaniv Hassidof;Tom Jurgenson;Kiril Solovey",
        "authorids": "~Yaniv_Hassidof1;~Tom_Jurgenson1;~Kiril_Solovey2",
        "gender": "M;;M",
        "homepage": "https://yanivhass.github.io/;;https://kirilsol.github.io",
        "dblp": ";https://dblp.uni-trier.de/pers/hd/j/Jurgenson:Tom;99/10966.htm",
        "google_scholar": ";1YjIvioAAAAJ;b7sIEHcAAAAJ",
        "orcid": "0009-0001-3761-4327;;",
        "linkedin": "yaniv-hassidof-7885191b8/;;",
        "or_profile": "~Yaniv_Hassidof1;~Tom_Jurgenson1;~Kiril_Solovey2",
        "aff": "Technion - Israel Institute of Technology;Technion;Technion - Israel Institute of Technology, Technion",
        "aff_domain": "campus.technion.ac.il;technion.ac.il;technion.ac.il",
        "position": "MS student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nhassidof2025trainonce,\ntitle={Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees},\nauthor={Yaniv Hassidof and Tom Jurgenson and Kiril Solovey},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=lJWUourMTT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=lJWUourMTT",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il/en/",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "lRKGIidrYZ",
        "title": "Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Building robotic agents capable of operating across diverse environments and object types remains a significant challenge, often requiring extensive data collection. This is particularly restrictive in robotics, where each data point must be physically executed in the real world. Consequently, there is a critical need for alternative data sources for robotics and frameworks that enable learning from such data. In this work, we present Point Policy, a new method for learning robot policies exclusively from offline human demonstration videos without any teleoperation data. Point Policy leverages state-of-the-art vision models and policy architectures to translate human hand poses into robot poses while capturing object states through semantically meaningful key points. This approach yields a morphology-agnostic representation that facilitates effective policy learning. Through experiments on a diverse set of real-world tasks, we demonstrate that Point Policy significantly outperforms prior methods for policy learning from human videos, performing well not only within the training distribution but also generalizing to novel object instances and cluttered environments. Videos of the robot are best viewed at anon-point-policy.github.io.",
        "keywords": "Robot Learning;Imitation Learning;Robot Perception;Sensing & Vision",
        "primary_area": "",
        "supplementary_material": "/attachment/d03686899b2fb3f7793533ddab758ffed386d5d4.zip",
        "author": "Siddhant Haldar;Lerrel Pinto",
        "authorids": "~Siddhant_Haldar1;~Lerrel_Pinto1",
        "gender": "M;M",
        "homepage": "https://siddhanthaldar.github.io/;https://www.lerrelpinto.com/",
        "dblp": "227/2282;168/8304",
        "google_scholar": "-h_bkRgAAAAJ;pmVPj94AAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Siddhant_Haldar1;~Lerrel_Pinto1",
        "aff": "New York University;New York University",
        "aff_domain": "nyu.edu;cs.nyu.edu",
        "position": "PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nhaldar2025point,\ntitle={Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation},\nauthor={Siddhant Haldar and Lerrel Pinto},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=lRKGIidrYZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=lRKGIidrYZ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "New York University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.nyu.edu",
        "aff_unique_abbr": "NYU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "mjYKNIRqpy",
        "title": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In this paper, we propose a training-free framework for vision-and-language navigation (VLN). Existing zero-shot VLN methods are mainly designed for discrete environments or involve unsupervised training in continuous simulator environments, which makes it challenging to generalize and deploy them in real-world scenarios. To achieve a training-free framework in continuous environments, our framework formulates navigation guidance as graph constraint optimization by decomposing instructions into explicit spatial constraints. The constraint-driven paradigm decodes spatial semantics through constraint solving, enabling zero-shot adaptation to unseen environments. Specifically, we construct a spatial constraint library covering all types of spatial relationship mentioned in VLN instructions. The human instruction is decomposed into a directed acyclic graph, with waypoint nodes, object nodes and edges, which are used as queries to retrieve the library to build the graph constraints. The graph constraint optimization is solved by the constraint solver to determine the positions of waypoints, obtaining the robot's navigation path and final goal. To handle cases of no solution or multiple solutions, we construct the navigation tree and the backtracking mechanism. Extensive experiments on standard benchmarks demonstrate significant improvements in success rate and navigation efficiency compared to state-of-the-art zero-shot VLN methods. We further conduct real-world experiments to show our framework can effectively generalize to new environments and instruction sets, paving the way for more robust and autonomous navigation framework.",
        "keywords": "Vision-Language Navigation;Graph Constraint",
        "primary_area": "",
        "supplementary_material": "/attachment/05d8c3479971c7c2225791d0c1cb1efb795772b7.zip",
        "author": "Hang Yin;Haoyu Wei;Xiuwei Xu;Wenxuan Guo;Jie Zhou;Jiwen Lu",
        "authorids": "~Hang_Yin5;~Haoyu_Wei3;~Xiuwei_Xu1;~Wenxuan_Guo3;~Jie_Zhou3;~Jiwen_Lu1",
        "gender": ";M;M;M;M;M",
        "homepage": "https://www.au.tsinghua.edu.cn/;https://ivg.au.tsinghua.edu.cn/Jiwen_Lu/students.html;https://xuxw98.github.io/;https://github.com/GWxuan;https://www.tsinghua.edu.cn/publish/auen/1713/2011/20110506105532098625469/20110506105532098625469_.html;http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/",
        "dblp": ";;315/9374;;00/5012-1;http://dblp.uni-trier.de/pers/hd/l/Lu:Jiwen",
        "google_scholar": ";;4G627acAAAAJ;yj--nvoAAAAJ;;TN8uDQoAAAAJ",
        "orcid": ";;;;;0000-0002-6121-5529",
        "linkedin": ";;;;;",
        "or_profile": "~Hang_Yin5;~Haoyu_Wei3;~Xiuwei_Xu1;~Wenxuan_Guo3;~Jie_Zhou3;~Jiwen_Lu1",
        "aff": "Tsinghua University;Beihang University;Tsinghua University;Tsinghua University;Tsinghua University;Tsinghua University",
        "aff_domain": "tsinghua.edu.cn;buaa.edu;tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn;tsinghua.edu.cn",
        "position": "PhD student;Undergrad student;PhD student;PhD student;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nyin2025gcvln,\ntitle={{GC}-{VLN}: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation},\nauthor={Hang Yin and Haoyu Wei and Xiuwei Xu and Wenxuan Guo and Jie Zhou and Jiwen Lu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=mjYKNIRqpy}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=mjYKNIRqpy",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0;0;0",
        "aff_unique_norm": "Tsinghua University;Beihang University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tsinghua.edu.cn;http://www.buaa.edu.cn/",
        "aff_unique_abbr": "THU;BUAA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "China"
    },
    {
        "id": "nMiyWyNhQx",
        "title": "Human-like Navigation in a World Built for Humans",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "When navigating in a man-made environment they haven\u2019t visited before\u2014like an office building\u2014humans employ behaviors such as reading signs and asking others for directions. These behaviors help humans reach their destinations efficiently by reducing the need to search through large areas. Existing robot navigation systems lack the ability to execute such behaviors and are thus highly inefficient at navigating within large environments. We present ReasonNav, a modular navigation system which integrates these human-like navigation skills by leveraging the reasoning capabilites of a vision-language model (VLM). We design compact input and output abstractions based on navigation landmarks, allowing the VLM to focus on language understanding and reasoning. We evaluate ReasonNav on real and simulated navigation tasks and show that the agent successfully employs higher-order reasoning to navigate efficiently in large, complex buildings.",
        "keywords": "navigation;reasoning;vison-language model",
        "primary_area": "",
        "supplementary_material": "/attachment/ee6f3207c1cf04ae4618328650300e1caa1b197c.zip",
        "author": "Bhargav Chandaka;Gloria Xinyue Wang;Haozhe Chen;Henry Che;Albert J. Zhai;Shenlong Wang",
        "authorids": "~Bhargav_Chandaka1;~Gloria_Xinyue_Wang1;~Haozhe_Chen5;~Henry_Che1;~Albert_J._Zhai1;~Shenlong_Wang1",
        "gender": "M;;M;M;;M",
        "homepage": "https://bchandaka.github.io/;;;https://hungdche.github.io/;;https://shenlong.web.illinois.edu/",
        "dblp": "349/4876;;;347/2919;;117/4842",
        "google_scholar": ";;;op5oHxcAAAAJ;;QFpswmcAAAAJ",
        "orcid": ";;;;;",
        "linkedin": ";;haozhe-chen-53343a26a/;henry-che/;;shenlong-wang-3496023b",
        "or_profile": "~Bhargav_Chandaka1;~Gloria_Xinyue_Wang1;~Haozhe_Chen5;~Henry_Che1;~Albert_J._Zhai1;~Shenlong_Wang1",
        "aff": "University of Illinois, Urbana Champaign;;University of Illinois, Urbana Champaign;UIUC+Waabi.ai;;University of Illinois, Urbana Champaign",
        "aff_domain": "illinois.edu;;illinois.edu;illinois.edu+waabi.ai;;illinois.edu",
        "position": "MS student;;MS student;MS student+Research Intern;;Assistant Professor",
        "bibtex": "@inproceedings{\nchandaka2025humanlike,\ntitle={Human-like Navigation in a World Built for Humans},\nauthor={Bhargav Chandaka and Gloria Xinyue Wang and Haozhe Chen and Henry Che and Albert J. Zhai and Shenlong Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=nMiyWyNhQx}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=nMiyWyNhQx",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0+2;1;0",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;;Waabi.ai",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://illinois.edu;;",
        "aff_unique_abbr": "UIUC;;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "najxw1MlZH",
        "title": "QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Panoramic cameras, capturing comprehensive 360-degree environmental data, are suitable for quadruped robots in surrounding perception and interaction with complex environments. However, the scarcity of high-quality panoramic training data \u2014 caused by inherent kinematic constraints and complex sensor calibration challenges \u2014 fundamentally limits the development of robust perception systems tailored to these embodied platforms. To address this issue, we propose QuaDreamer\u2014the first panoramic data generation engine specifically designed for quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of quadruped robots to generate highly controllable, realistic panoramic videos, providing a data source for downstream tasks. Specifically, to effectively capture the unique vertical vibration characteristics exhibited during quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts controllable vertical signals through frequency-domain feature filtering and provides high-quality prompts. To facilitate high-quality panoramic video generation under jitter signal control, we propose a Scene-Object Controller (SOC) that effectively manages object motion and boosts background jitter control through the attention mechanism. To address panoramic distortions in wide-FoV video generation, we propose the Panoramic Enhancer (PE) \u2013 a dual-stream architecture that synergizes frequency-texture refinement for local detail enhancement with spatial-structure correction for global geometric consistency. We further demonstrate that the generated video sequences can serve as training data for the quadruped robot's panoramic visual perception model, enhancing the performance of multi-object tracking in 360-degree scenes. The source code and model weights will be publicly available.",
        "keywords": "Panoramic Video Generation;World Model;Quadruped Robots",
        "primary_area": "",
        "supplementary_material": "/attachment/8b51841ea2ce6216a2b6f07190900f120464828e.pdf",
        "author": "Sheng Wu;Fei Teng;Hao Shi;Qi Jiang;Kai Luo;Kaiwei Wang;Kailun Yang",
        "authorids": "~Sheng_Wu4;~Fei_Teng11;~Hao_Shi2;~Qi_Jiang2;~Kai_Luo3;~Kaiwei_Wang1;~Kailun_Yang1",
        "gender": "M;;M;M;;M;M",
        "homepage": "https://losehu.com;;;;;https://person.zju.edu.cn/en/0009214;https://yangkailun.com/",
        "dblp": ";;123/2798;;;45/11142;190/9526",
        "google_scholar": ";;https://scholar.google.co.nz/citations?user=0EI9msQAAAAJ;;;https://scholar.google.com.hk/citations?user=B6xWNvgAAAAJ;pKFqWhgAAAAJ",
        "orcid": ";;0000-0003-0184-2245;0000-0002-7484-3030;;;0000-0002-1090-667X",
        "linkedin": ";;;;;;yangkailun/",
        "or_profile": "~Sheng_Wu4;~Fei_Teng11;~Hao_Shi2;~Qi_Jiang2;~Kai_Luo3;~Kaiwei_Wang1;~Kailun_Yang1",
        "aff": "Hunan University;;Zhejiang University;Zhejiang University;;Zhejiang University;Hunan University",
        "aff_domain": "hnu.edu.cn;;zju.edu.cn;zju.edu.cn;;zju.edu.cn;hnu.edu.cn",
        "position": "MS student;;PhD student;PhD student;;Full Professor;Full Professor",
        "bibtex": "@inproceedings{\nwu2025quadreamer,\ntitle={QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots},\nauthor={Sheng Wu and Fei Teng and Hao Shi and Qi Jiang and Kai Luo and Kaiwei Wang and Kailun Yang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=najxw1MlZH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=najxw1MlZH",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;2;1;2;0",
        "aff_unique_norm": "Hunan University;;Zhejiang University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "http://www.hunu.edu.cn/;;https://www.zju.edu.cn",
        "aff_unique_abbr": "HNU;;ZJU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "nryBWao01j",
        "title": "Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle Avoidance for Autonomous Racing",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Diffusion models hold great potential in robotics due to their ability to capture complex, high-dimensional data distributions. However, their lack of constraint-awareness limits their deployment in safety-critical applications. We propose Constraint-Aware Diffusion Guidance (CoDiG), a data-efficient and general-purpose framework that integrates barrier functions into the denoising process, guiding diffusion sampling toward constraint-satisfying outputs. CoDiG enables constraint satisfaction even with limited training data and generalizes across tasks. We evaluate our framework in the challenging setting of miniature autonomous racing, where real-time obstacle avoidance is essential. Real-world experiments show that CoDiG generates safe outputs efficiently under dynamic conditions, highlighting its potential for broader robotic applications.",
        "keywords": "Diffusion Guidance;Constraint-Aware Sampling;Real-Time Obstacle Avoidance;Autonomous Racing;Safe Control",
        "primary_area": "",
        "supplementary_material": "/attachment/c0de470c9df4dc17ee2de5e0a83e0459477f28dc.zip",
        "author": "Hao Ma;Sabrina Bodmer;Andrea Carron;Melanie Zeilinger;Michael Muehlebach",
        "authorids": "~Hao_Ma6;~Sabrina_Bodmer1;~Andrea_Carron1;~Melanie_Zeilinger1;~Michael_Muehlebach1",
        "gender": "M;F;;F;",
        "homepage": "https://is.mpg.de/person/hma2;;;;https://sites.google.com/view/mmuehlebach/",
        "dblp": ";;125/5537;41/7142;142/1129",
        "google_scholar": ";CY-KjesAAAAJ;;;uTfYBAsAAAAJ",
        "orcid": ";;;0000-0003-4570-7571;",
        "linkedin": ";;;;",
        "or_profile": "~Hao_Ma6;~Sabrina_Bodmer1;~Andrea_Carron1;~Melanie_Zeilinger1;~Michael_Muehlebach1",
        "aff": "ETHZ - ETH Zurich;ETHZ - ETH Zurich;ETHZ - ETH Zurich;ETHZ - ETH Zurich;Max-Planck Institute",
        "aff_domain": "ethz.ch;ethz.ch;ethz.ch;ethz.ch;mpg.de",
        "position": "PhD student;PhD student;Lecturer;Associate Professor;Principal Researcher",
        "bibtex": "@inproceedings{\nma2025constraintaware,\ntitle={Constraint-Aware Diffusion Guidance for Robotics: Real-Time Obstacle Avoidance for Autonomous Racing},\nauthor={Hao Ma and Sabrina Bodmer and Andrea Carron and Melanie Zeilinger and Michael Muehlebach},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=nryBWao01j}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=nryBWao01j",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;1",
        "aff_unique_norm": "ETH Zurich;Max-Planck-Gesellschaft zur F\u00f6rderung der Wissenschaften e.V.",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;https://www.mpg.de",
        "aff_unique_abbr": "ETHZ;MPG",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;1",
        "aff_country_unique": "Switzerland;Germany"
    },
    {
        "id": "o0LBjJxUeS",
        "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Reasoning over long sequences of observations and actions is essential for many robotic tasks. \nYet, learning effective long-context policies from demonstrations remains challenging. \nAs context length increases, training becomes increasingly expensive due to rising memory demands, and policy performance often degrades as a result of spurious correlations.\nRecent methods typically sidestep these issues by truncating context length, discarding historical information that may be critical for subsequent decisions.\nIn this paper, we propose an alternative approach that explicitly regularizes the retention of past information.\nWe first revisit the copycat problem in imitation learning and identify an opposite challenge in recent diffusion policies: rather than over-relying on prior actions, they often fail to capture essential dependencies between past and future actions.\nTo address this, we introduce Past-Token Prediction (PTP), an auxiliary task in which the policy learns to predict past action tokens alongside future ones.\nThis regularization significantly improves temporal modeling in the policy head, with minimal reliance on visual representations.\nBuilding on this observation, we further introduce a multistage training strategy: pre-train the visual encoder with short contexts, and fine-tune the policy head using cached long-context embeddings. \nThis strategy preserves the benefits of PTP while greatly reducing memory and computational overhead.\nFinally, we extend PTP into a self-verification mechanism at test time, enabling the policy to score and select candidates consistent with past actions during inference.\nExperiments across four real-world and six simulated tasks demonstrate that our proposed method improves the performance of long-context diffusion policies by 3\u00d7 and accelerates policy training by more than 10\u00d7.\nVideos are available at https://ptp-robot.github.io.",
        "keywords": "history-conditioned policy learning",
        "primary_area": "",
        "supplementary_material": "/attachment/cf0be1b1b46ef688873e211d1e65194e8476b254.zip",
        "author": "Marcel Torne Villasevil;Andy Tang;Yuejiang Liu;Chelsea Finn",
        "authorids": "~Marcel_Torne_Villasevil1;~Andy_Tang1;~Yuejiang_Liu1;~Chelsea_Finn1",
        "gender": "M;;;F",
        "homepage": "https://marceltorne.github.io;https://andyta.ng/;https://sites.google.com/view/yuejiangliu;https://ai.stanford.edu/~cbfinn/",
        "dblp": "352/5363;;202/5799;131/1783",
        "google_scholar": "ITlelQ8AAAAJ;;https://scholar.google.com/citations?hl=en;vfPE6hgAAAAJ",
        "orcid": ";0000-0001-9164-1687;;",
        "linkedin": "marceltorne/;;;",
        "or_profile": "~Marcel_Torne_Villasevil1;~Andy_Tang1;~Yuejiang_Liu1;~Chelsea_Finn1",
        "aff": "Stanford University;Computer Science Department, Stanford University;Stanford University;Physical Intelligence+Stanford University",
        "aff_domain": "stanford.edu;cs.stanford.edu;cs.stanford.edu;physicalintelligence.company+stanford.edu",
        "position": "PhD student;Undergrad student;Postdoc;Researcher+Assistant Professor",
        "bibtex": "@inproceedings{\nvillasevil2025learning,\ntitle={Learning Long-Context Diffusion Policies via Past-Token Prediction},\nauthor={Marcel Torne Villasevil and Andy Tang and Yuejiang Liu and Chelsea Finn},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=o0LBjJxUeS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=o0LBjJxUeS",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1+0",
        "aff_unique_norm": "Stanford University;Physical Intelligence",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.stanford.edu;",
        "aff_unique_abbr": "Stanford;",
        "aff_campus_unique_index": "0;0;0;0",
        "aff_campus_unique": "Stanford;",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "o2w2iiMyEU",
        "title": "LaDi-WM: A Latent Diffusion-Based World Model for Predictive Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Predictive manipulation has recently gained considerable attention in the Embodied AI community due to its potential to improve robot policy performance by leveraging predicted states. However, generating accurate future visual states of robot-object interactions from world models remains a well-known challenge, particularly in achieving high-quality pixel-level representations. To this end, we propose LaDi-WM, a world model that predicts the latent space of future states using diffusion modeling. Specifically, LaDi-WM leverages the well-established latent space aligned with pre-trained Visual Foundation Models (VFMs), which comprises both geometric features (DINO-based) and semantic features (CLIP-based). We find that predicting the evolution of the latent space is easier to learn and more generalizable than directly predicting pixel-level images. Building on LaDi-WM, we design a diffusion policy that iteratively refines output actions by incorporating forecasted states, thereby generating more consistent and accurate results. Extensive experiments on both synthetic and real-world benchmarks demonstrate that LaDi-WM significantly enhances policy performance by 27.9\\% on the LIBERO-LONG benchmark and 20\\% on the real-world scenario. Furthermore, our world model and policies achieve impressive generalizability in real-world experiments. The full source code will be publicly available.",
        "keywords": "Robot Learning: Model Learning;Control & Dynamics;Grasping & Manipulation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yuhang Huang;Jiazhao Zhang;Shilong Zou;Xinwang Liu;Ruizhen Hu;Kai Xu",
        "authorids": "~Yuhang_Huang1;~Jiazhao_Zhang2;~Shilong_Zou1;~Xinwang_Liu1;~Ruizhen_Hu1;~Kai_Xu5",
        "gender": "M;M;M;M;F;",
        "homepage": ";https://jzhzhang.github.io/;https://github.com/ZouShilong1024;https://xinwangliu.github.io/;https://csse.szu.edu.cn/staff/ruizhenhu/;",
        "dblp": ";243/2982;;45/6569-2.html;14/11270;",
        "google_scholar": "https://scholar.google.com/citations?hl=zh-CN;zOZgpXwAAAAJ;;A56vWC4AAAAJ;https://scholar.google.ca/citations?user=MloRITsAAAAJ;",
        "orcid": ";;;;;",
        "linkedin": ";;;;;",
        "or_profile": "~Yuhang_Huang1;~Jiazhao_Zhang2;~Shilong_Zou1;~Xinwang_Liu1;~Ruizhen_Hu1;~Kai_Xu5",
        "aff": "National University of Defense Technology;Peking University;National University of Defense Technology;National University of Defense Technology;Shenzhen University;",
        "aff_domain": "nudt.edu.cn;pku.edu.cn;nudt.edu.cn;nudt.edu.cn;szu.edu.cn;",
        "position": "PhD student;PhD student;MS student;Full Professor;Full Professor;",
        "bibtex": "@inproceedings{\nhuang2025ladiwm,\ntitle={LaDi-{WM}: A Latent Diffusion-Based World Model for Predictive Manipulation},\nauthor={Yuhang Huang and Jiazhao Zhang and Shilong Zou and Xinwang Liu and Ruizhen Hu and Kai Xu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=o2w2iiMyEU}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=o2w2iiMyEU",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0;2;3",
        "aff_unique_norm": "National University of Defense Technology;Peking University;Shenzhen University;",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "http://www.nudt.edu.cn/;http://www.pku.edu.cn;https://www.szu.edu.cn;",
        "aff_unique_abbr": "NUDT;Peking U;SZU;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "China;"
    },
    {
        "id": "oDUbsdc0Ru",
        "title": "Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Cloth manipulation is challenging due to its highly complex dynamics, near-infinite degrees of freedom, and frequent self-occlusions, which complicate both state estimation and dynamics modeling. Inspired by recent advances in generative models, we hypothesize that these expressive models can effectively capture intricate cloth configurations and deformation patterns from data. Therefore, we propose a diffusion-based generative approach for both perception and dynamics modeling. Specifically, we formulate state estimation as reconstructing full cloth states from partial observations and dynamics modeling as predicting future states given the current state and robot actions. Leveraging a transformer-based diffusion model, our method achieves accurate state reconstruction and reduces long-horizon dynamics prediction errors by an order of magnitude compared to prior approaches. We integrate our dynamics models with model-predictive control and show that our framework enables effective cloth folding on real robotic systems, demonstrating the potential of generative models for deformable object manipulation under partial observability and complex dynamics.",
        "keywords": "Model Learning;Deformable Object Manipulation;Perception",
        "primary_area": "",
        "supplementary_material": "/attachment/1470b67a19a8bbee2426e8e8c8df120c1e2a9bf5.zip",
        "author": "Tongxuan Tian;Haoyang Li;Bo Ai;Xiaodi Yuan;Zhiao Huang;Hao Su",
        "authorids": "~Tongxuan_Tian1;~Haoyang_Li6;~Bo_Ai1;~Xiaodi_Yuan1;~Zhiao_Huang1;~Hao_Su1",
        "gender": "M;M;M;F;M;M",
        "homepage": ";https://haoyangli16.github.io/;https://albertboai.com/;https://rabbit-hu.github.io/;;http://ai.ucsd.edu/~haosu",
        "dblp": ";;;;172/1410;09/4945-1",
        "google_scholar": ";;KlE77HAAAAAJ;i-QiNPIAAAAJ;;1P8Zu04AAAAJ",
        "orcid": ";;0000-0002-9009-3823;0009-0003-2320-2297;;",
        "linkedin": "tongxuan-tian/;haoyang-li-174990297/;bo-ai;xiaodi-yuan-117b681aa/;;",
        "or_profile": "~Tongxuan_Tian1;~Haoyang_Li6;~Bo_Ai1;~Xiaodi_Yuan1;~Zhiao_Huang1;~Hao_Su1",
        "aff": "University of Virginia, Charlottesville;University of California, San Diego;University of California, San Diego;University of California, San Diego;;University of California, San Diego",
        "aff_domain": "virginia.edu;ucsd.edu;ucsd.edu;ucsd.edu;;ucsd.edu",
        "position": "MS student;Researcher;PhD student;PhD student;;Associate Professor",
        "bibtex": "@inproceedings{\ntian2025diffusion,\ntitle={Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation},\nauthor={Tongxuan Tian and Haoyang Li and Bo Ai and Xiaodi Yuan and Zhiao Huang and Hao Su},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=oDUbsdc0Ru}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=oDUbsdc0Ru",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;1;2;1",
        "aff_unique_norm": "University of Virginia;University of California, San Diego;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.virginia.edu;https://www.ucsd.edu;",
        "aff_unique_abbr": "UVA;UCSD;",
        "aff_campus_unique_index": "0;1;1;1;1",
        "aff_campus_unique": "Charlottesville;San Diego;",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "oGcC8nMOit",
        "title": "Cross-Sensor Touch Generation",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Today's visuo-tactile sensors come in many shapes and sizes, making it challenging to develop general-purpose tactile representations. This is because most models are tied to a specific sensor design. To address this challenge, we propose two approaches to cross-sensor image generation. The first is an end-to-end method that leverages paired data (Touch2Touch). The second method builds an intermediate depth representation and does not require paired data (T2D2: Touch-to-Depth-to-Touch). Both methods enable the use of sensor-specific models across multiple sensors via the cross-sensor touch generation process. Together, these models offer flexible solutions for sensor translation, depending on data availability and application needs. We demonstrate their effectiveness on downstream tasks such as cup stacking and tool insertion, where models originally designed for one sensor are successfully transferred to another using in-hand pose estimation.",
        "keywords": "Tactile Sensing;Cross-Modal Generation;Manipulation;Representation Learning;Soft Bubbles;GelSlims;Digits",
        "primary_area": "",
        "supplementary_material": "/attachment/2db9b95fb609689b26616a35566d5d9131e967e4.zip",
        "author": "Samanta Rodriguez;Yiming Dou;Miquel Oller;Andrew Owens;Nima Fazeli",
        "authorids": "~Samanta_Rodriguez1;~Yiming_Dou1;~Miquel_Oller1;~Andrew_Owens1;~Nima_Fazeli1",
        "gender": "F;M;;M;",
        "homepage": ";https://www.yimingdou.com/;;http://andrewowens.com;https://www.mmintlab.com",
        "dblp": ";333/2362.html;;85/2697;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;IMsjP1sAAAAJ;N8LKz0kAAAAJ;9hX-JksAAAAJ;",
        "orcid": ";;;;",
        "linkedin": ";;;;",
        "or_profile": "~Samanta_Rodriguez1;~Yiming_Dou1;~Miquel_Oller1;~Andrew_Owens1;~Nima_Fazeli1",
        "aff": "University of Michigan - Ann Arbor;Cornell University+University of Michigan - Ann Arbor;University of Michigan - Ann Arbor;Cornell University+University of Michigan;University of Michigan",
        "aff_domain": "umich.edu;cornell.edu+umich.edu;umich.edu;cornell.edu+umich.edu;umich.edu",
        "position": "PhD student;PhD student+PhD student;PhD student;Associate Professor+Assistant Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nrodriguez2025crosssensor,\ntitle={Cross-Sensor Touch Generation},\nauthor={Samanta Rodriguez and Yiming Dou and Miquel Oller and Andrew Owens and Nima Fazeli},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=oGcC8nMOit}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=oGcC8nMOit",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1+0;0;1+0;0",
        "aff_unique_norm": "University of Michigan;Cornell University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.umich.edu;https://www.cornell.edu",
        "aff_unique_abbr": "UM;Cornell",
        "aff_campus_unique_index": "0;0;0;",
        "aff_campus_unique": "Ann Arbor;",
        "aff_country_unique_index": "0;0+0;0;0+0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "oOCa85Z1Ho",
        "title": "Steerable Scene Generation with Post Training and Inference-Time Search",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Training robots in simulation requires diverse 3D scenes that reflect the specific challenges of downstream tasks. However, scenes that satisfy strict task requirements, such as high-clutter environments with plausible spatial arrangement, are rare and costly to curate manually. Instead, we generate large-scale scene data using procedural models that approximate realistic environments for robotic manipulation, and adapt it to task-specific goals.\nWe do this by training a unified diffusion-based generative model that predicts which objects to place from a fixed asset library, along with their SE(3) poses. This model serves as a flexible scene prior that can be adapted using reinforcement learning-based post training, conditional generation, or inference-time search, steering generation toward downstream objectives even when they differ from the original data distribution.\nOur method enables goal-directed scene synthesis that respects physical feasibility and scales across scene types. We introduce a novel MCTS-based inference-time search strategy for diffusion models, enforce feasibility via projection and simulation, and release a dataset of over 44 million SE(3) scenes spanning five diverse environments.",
        "keywords": "Scene Generation;Simulation;Diffusion;MCTS",
        "primary_area": "",
        "supplementary_material": "/attachment/8fca0992a3e50a1339d33c5bfc2371f7f7d6fc9e.zip",
        "author": "Nicholas Ezra Pfaff;Hongkai Dai;Sergey Zakharov;Shun Iwase;Russ Tedrake",
        "authorids": "~Nicholas_Ezra_Pfaff1;~Hongkai_Dai1;~Sergey_Zakharov1;~Shun_Iwase1;~Russ_Tedrake1",
        "gender": "M;;M;M;M",
        "homepage": "https://nepfaff.github.io/;;https://zakharos.github.io/;https://sh8.github.io/;http://people.csail.mit.edu/russt",
        "dblp": ";;195/5832;;73/1296",
        "google_scholar": "https://scholar.google.com.hk/citations?hl=en;ZZsEXLAAAAAJ;https://scholar.google.de/citations?user=3DK3I-8AAAAJ;XP3hIDUAAAAJ;nxNkEiYAAAAJ",
        "orcid": "0009-0004-9169-3611;;;;",
        "linkedin": "nicholas-ezra-pfaff/;;;;",
        "or_profile": "~Nicholas_Ezra_Pfaff1;~Hongkai_Dai1;~Sergey_Zakharov1;~Shun_Iwase1;~Russ_Tedrake1",
        "aff": "Massachusetts Institute of Technology;Toyota Research Institute;Toyota Research Institute;Carnegie Mellon University;Toyota Research Institute+Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;tri.global;tri.global;cs.cmu.edu;tri.global+mit.edu",
        "position": "PhD student;Researcher;Researcher;PhD student;Researcher+Full Professor",
        "bibtex": "@inproceedings{\npfaff2025steerable,\ntitle={Steerable Scene Generation with Post Training and Inference-Time Search},\nauthor={Nicholas Ezra Pfaff and Hongkai Dai and Sergey Zakharov and Shun Iwase and Russ Tedrake},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=oOCa85Z1Ho}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=oOCa85Z1Ho",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;2;1+0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Toyota Research Institute;Carnegie Mellon University",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://web.mit.edu;https://www.tri.global;https://www.cmu.edu",
        "aff_unique_abbr": "MIT;TRI;CMU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0+0",
        "aff_country_unique": "United States"
    },
    {
        "id": "oRwcxFuN25",
        "title": "Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Simulation-based reinforcement learning (RL) has significantly advanced humanoid locomotion tasks, yet direct real-world RL from scratch or starting from pretrained policies remains rare, limiting the full potential of humanoid robots. Real-world training, despite being crucial for overcoming the sim-to-real gap, faces substantial challenges related to safety, reward design, and learning efficiency. To address these limitations, we propose Robot-Trains-Robot (RTR), a novel framework where a robotic arm teacher actively supports and guides a humanoid student robot. The RTR system provides protection, schedule, reward, perturbation, failure detection, and automatic resets, enabling efficient long-term real-world training with minimal human intervention. Furthermore, we propose a novel RL pipeline that facilitates and stabilizes sim-to-real transfer by optimizing a single dynamics-encoded latent variable in the real world. We validate our method through two challenging real-world humanoid tasks: fine-tuning a walking policy for precise speed tracking and learning a humanoid swing-up task from scratch, illustrating the promising capabilities of real-world humanoid learning realized by RTR-style systems.",
        "keywords": "Humanoid Robots;Sim-to-Real Adaptation;Real-World RL",
        "primary_area": "",
        "supplementary_material": "/attachment/b86eeb96797275a58fda8e18c302a04979456a7f.zip",
        "author": "Kaizhe Hu;Haochen Shi;Yao He;Weizhuo Wang;Karen Liu;Shuran Song",
        "authorids": "~Kaizhe_Hu1;~Haochen_Shi2;~Yao_He1;~Weizhuo_Wang2;~Karen_Liu1;~Shuran_Song3",
        "gender": "M;M;M;M;;F",
        "homepage": "https://hukz18.github.io/;https://hshi74.github.io/;https://shockwavehe.github.io/;https://me.weizhuowang.com;https://cs.stanford.edu/~karenliu;https://shurans.github.io/",
        "dblp": "330/4940;;;161/7224;;",
        "google_scholar": "mPpYLhcAAAAJ;https://scholar.google.com/citations?hl=en;;gx_A620AAAAJ;i28fU0MAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";0000-0002-3604-465X;;0009-0007-1153-1324;0000-0001-5926-0905;",
        "linkedin": "%E5%BC%80%E5%93%B2-%E8%83%A1-40137718a/?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAACyMbIEBJhMDJ4b7wLQyHotP_JGOnWDoEDU;;;weizhuo-ken-wang-ba4921119/;;",
        "or_profile": "~Kaizhe_Hu1;~Haochen_Shi2;~Yao_He1;~Weizhuo_Wang2;~Karen_Liu1;~Shuran_Song3",
        "aff": "Tsinghua University+Stanford University;Stanford University;Stanford University;Stanford University;Computer Science Department, Stanford University;Stanford University",
        "aff_domain": "tsinghua.edu.cn+stanford.edu;stanford.edu;stanford.edu;stanford.edu;cs.stanford.edu;stanford.edu",
        "position": "PhD student+Researcher;PhD student;MS student;PhD student;Full Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nhu2025robot,\ntitle={Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids},\nauthor={Kaizhe Hu and Haochen Shi and Yao He and Weizhuo Wang and Karen Liu and Shuran Song},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=oRwcxFuN25}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=oRwcxFuN25",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;1;1;1;1;1",
        "aff_unique_norm": "Tsinghua University;Stanford University",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tsinghua.edu.cn;https://www.stanford.edu",
        "aff_unique_abbr": "THU;Stanford",
        "aff_campus_unique_index": "1;1;1;1;1;1",
        "aff_campus_unique": ";Stanford",
        "aff_country_unique_index": "0+1;1;1;1;1;1",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "oYC10hiFua",
        "title": "Learning Smooth State-Dependent Traversability from Dense Point Clouds",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "A key open challenge in off-road autonomy is that the traversability of terrain often depends on the vehicle's state. In particular, some obstacles are only traversable from some orientations. However, learning this interaction by encoding the angle of approach as a model input demands a large and diverse training dataset and is computationally inefficient during planning due to repeated model inference. To address these challenges, we present SPARTA, a method for estimating approach angle conditioned traversability from point clouds. Specifically, we impose geometric structure into our network by outputting a smooth analytical function over the 1-Sphere that predicts risk distribution for any angle of approach with minimal overhead and can be reused for subsequent queries. The function is composed of Fourier basis functions, which has important advantages for generalization due to their periodic nature and smoothness. We demonstrate SPARTA both in a high-fidelity simulation platform, where our model achieves a 91% success rate crossing a 40m boulder field (compared to 73\\% for the baseline), and on hardware, illustrating the generalization ability of the model to real-world settings.",
        "keywords": "Off-road Autonomy;Traversability;Geometric Deep Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/e889a373d614a37edbb2ba129adae0dd0f47b43c.zip",
        "author": "Zihao Dong;Alan Papalia;Leonard Jung;Alenna Spiro;Philip R Osteen;Christa S. Robison;Michael Everett",
        "authorids": "~Zihao_Dong2;~Alan_Papalia2;~Leonard_Jung1;~Alenna_Spiro1;~Philip_R_Osteen1;~Christa_S._Robison1;~Michael_Everett1",
        "gender": "M;M;M;F;M;Not Specified;M",
        "homepage": ";https://alanpapalia.github.io/;https://github.com/leonardjmihs;;;;http://mfe7.github.io",
        "dblp": ";;;;117/2316;;86/869",
        "google_scholar": ";Ym3SpKgAAAAJ;;https://scholar.google.com/citations?hl=en;IcbuLWkAAAAJ;;",
        "orcid": "0009-0009-4076-130X;0000-0002-6922-135X;;;0000-0001-8266-9848;;",
        "linkedin": ";alan-papalia/;;alenna-spiro-662a38230/;phil-osteen-3bb19741;;",
        "or_profile": "~Zihao_Dong2;~Alan_Papalia2;~Leonard_Jung1;~Alenna_Spiro1;~Philip_R_Osteen1;~Christa_S._Robison1;~Michael_Everett1",
        "aff": "Northeastern University;Northeastern University+Massachusetts Institute of Technology+Woods Hole Oceanographic Institution;Northeastern University;Northeastern University;U.S. Army Research Laboratory;DEVCOM Army Research Laboratory;Northeastern University",
        "aff_domain": "northeastern.edu;neu.edu+mit.edu+whoi.edu;neu.edu;northeastern.edu;arl.army.mil;army.mil;northeastern.edu",
        "position": "PhD student;Postdoc+PhD student+PhD student;PhD student;PhD student;Researcher;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\ndong2025learning,\ntitle={Learning Smooth State-Dependent Traversability from Dense Point Clouds},\nauthor={Zihao Dong and Alan Papalia and Leonard Jung and Alenna Spiro and Philip R Osteen and Christa S. Robison and Michael Everett},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=oYC10hiFua}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=oYC10hiFua",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0+1+2;0;0;3;4;0",
        "aff_unique_norm": "Northeastern University;Massachusetts Institute of Technology;Woods Hole Oceanographic Institution;U.S. Army Research Laboratory;United States Army Research Laboratory",
        "aff_unique_dep": ";;;;Army Research Laboratory",
        "aff_unique_url": "https://www.northeastern.edu;https://web.mit.edu;https://www.whoi.edu;https://www.arl.army.mil;https://www.arl.army.mil",
        "aff_unique_abbr": "NEU;MIT;WHOI;ARL;ARL",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0+0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "pJ5FONkM9N",
        "title": "Efficient Evaluation of Multi-Task Robot Policies With Active Experiment Selection",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Evaluating learned robot control policies to determine their performance costs the experimenter time and effort. As robots become more capable in accomplishing diverse tasks, evaluating across all these tasks becomes more difficult as it is impractical to test every policy on every task multiple times. Rather than considering the average performance of a policy on a task, we consider the distribution of performance over time. In a multi-task policy evaluation setting, we actively model the distribution of robot performance across multiple tasks and policies as we sequentially execute experiments. We show that natural language is a useful prior in modeling relationships between tasks because they often share similarities that can reveal potential relationships in policy behavior. We leverage this formulation to reduce experimenter effort by using a cost-aware information gain heuristic to efficiently select informative trials. We conduct experiments on existing evaluation data from real robots and simulations and find a 50% reduction in estimates of the mean performance given a fixed cost budget. We encourage the use of our surrogate model as a scalable approach to track progress in evaluation.",
        "keywords": "robot evaluation;active testing;manipulation",
        "primary_area": "",
        "supplementary_material": "/attachment/9d6cda5e60fa9e0da15aff37d43a04fd20f3be8f.zip",
        "author": "Abrar Anwar;Rohan Gupta;Zain Merchant;Sayan Ghosh;Willie Neiswanger;Jesse Thomason",
        "authorids": "~Abrar_Anwar1;~Rohan_Gupta3;~Zain_Merchant1;~Sayan_Ghosh5;~Willie_Neiswanger2;~Jesse_Thomason1",
        "gender": "M;M;;;M;M",
        "homepage": "http://abraranwar.github.io/;;https://merchantzain.com/;https://sghosh73.github.io;https://willieneis.github.io/;https://jessethomason.com/",
        "dblp": "294/1347.html;;;67/6126;120/7593.html;130/2863",
        "google_scholar": "c6E-5tcAAAAJ;;;;QwKHApEAAAAJ;8BeTDr0AAAAJ",
        "orcid": "0000-0003-4442-4369;;;;;0000-0001-9199-0633",
        "linkedin": "abraranwar;rohan---gupta;;;;jesse-thomason-034746171/",
        "or_profile": "~Abrar_Anwar1;~Rohan_Gupta3;~Zain_Merchant1;~Sayan_Ghosh5;~Willie_Neiswanger2;~Jesse_Thomason1",
        "aff": "NVIDIA+University of Southern California;;University of Southern California;University of Southern California;University of Southern California;University of Southern California+Amazon",
        "aff_domain": "nvidia.com+usc.edu;;usc.edu;usc.edu;usc.edu;usc.edu+amazon.com",
        "position": "Intern+PhD student;;Undergrad student;PhD student;Assistant Professor;Assistant Professor+Visiting Academic",
        "bibtex": "@inproceedings{\nanwar2025efficient,\ntitle={Efficient Evaluation of Multi-Task Robot Policies With Active Experiment Selection},\nauthor={Abrar Anwar and Rohan Gupta and Zain Merchant and Sayan Ghosh and Willie Neiswanger and Jesse Thomason},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=pJ5FONkM9N}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=pJ5FONkM9N",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;2;1;1;1;1+3",
        "aff_unique_norm": "NVIDIA;University of Southern California;;Amazon",
        "aff_unique_dep": "NVIDIA Corporation;;;Amazon.com, Inc.",
        "aff_unique_url": "https://www.nvidia.com;https://www.usc.edu;;https://www.amazon.com",
        "aff_unique_abbr": "NVIDIA;USC;;Amazon",
        "aff_campus_unique_index": "1;1;1;1;1",
        "aff_campus_unique": ";Los Angeles",
        "aff_country_unique_index": "0+0;0;0;0;0+0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "pukgxvcOwL",
        "title": "\u201cStack It Up!\u201d: 3D Stable Structure Generation from 2D Hand-drawn Sketch",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Imagine a child sketching the Eiffel Tower and asking a robot to bring it to life. Today\u2019s robot manipulation systems can\u2019t act on such sketches directly\u2014they require precise 3D block poses as goals, which in turn demand structural analysis and expert tools like CAD. We present *StackItUp*, a system that enables non-experts to specify complex 3D structures using only 2D front-view hand-drawn sketches. *StackItUp* introduces an abstract relation graph to bridge the gap between rough sketches and accurate 3D block arrangements, capturing the symbolic geometric relations (e.g., *left-of*) and stability patterns (e.g.,*two-pillar-bridge*) while discarding noisy metric details from sketches. It then grounds this graph to 3D poses using compositional diffusion models and iteratively updates it by predicting hidden internal and rear supports\u2014critical for stability but absent from the sketch. Evaluated on sketches of iconic landmarks and modern house designs, *StackItUp* consistently produces stable, multilevel 3D structures and outperforms all baselines in both stability and visual resemblance.",
        "keywords": "Goal Specification;Diffusion Models",
        "primary_area": "",
        "supplementary_material": "/attachment/d754acd1def95a79d58ea7ce6f784d2661bb1d84.zip",
        "author": "Yiqing Xu;Linfeng Li;Cunjun Yu;David Hsu",
        "authorids": "~Yiqing_Xu1;~Linfeng_Li2;~Cunjun_Yu1;~David_Hsu1",
        "gender": "F;M;Unspecified;M",
        "homepage": "https://eeching.github.io/;;;http://www.comp.nus.edu.sg/~dyhsu/",
        "dblp": "27/870;;232/3014;29/331",
        "google_scholar": "bJm1-QQAAAAJ;;4xwyGM8AAAAJ;S9LHLKEAAAAJ",
        "orcid": ";0000-0001-7536-4894;;0000-0002-2309-4535",
        "linkedin": "yiqing-xu-2746a9166/;;;david-hsu-a86200a1/",
        "or_profile": "~Yiqing_Xu1;~Linfeng_Li2;~Cunjun_Yu1;~David_Hsu1",
        "aff": "National University of Singapore;national university of singaore, National University of Singapore;National University of Singapore;National University of Singapore",
        "aff_domain": "u.nus.edu;u.nus.edu;u.nus.edu;nus.edu.sg",
        "position": "PhD student;PhD student;PhD student;Professor",
        "bibtex": "@inproceedings{\nxu2025stack,\ntitle={{\\textquotedblleft}Stack It Up!{\\textquotedblright}: 3D Stable Structure Generation from 2D Hand-drawn Sketch},\nauthor={Yiqing Xu and Linfeng Li and Cunjun Yu and David Hsu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=pukgxvcOwL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=pukgxvcOwL",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "National University of Singapore;national university of singaore, National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.nus.edu.sg;",
        "aff_unique_abbr": "NUS;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Singapore;"
    },
    {
        "id": "qAGHsCsA1O",
        "title": "Predictive Red Teaming: Breaking Policies Without Breaking Robots",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Visuomotor policies trained via imitation learning are capable of performing challenging manipulation tasks, but are often extremely brittle to lighting, visual distractors, and object locations. These vulnerabilities can depend unpredictably on the specifics of training, and are challenging to expose without time-consuming and expensive hardware evaluations. We propose the problem of predictive red teaming: discovering vulnerabilities of a policy with respect to environmental factors, and predicting the corresponding performance degradation without hardware evaluations in off-nominal scenarios. In order to achieve this, we develop RoboART: an automated red teaming (ART) pipeline that (1) modifies nominal observations using generative image editing to vary different environmental factors, and (2) predicts performance under each variation using a policy-specific anomaly detector executed on edited observations. Experiments across 500+ hardware trials in twelve off-nominal conditions for visuomotor diffusion policies demonstrate that RoboART predicts performance degradation with high accuracy (less than 0.19 average difference between predicted and real success rates). We also demonstrate how predictive red teaming enables targeted data collection: fine-tuning with data collected under conditions predicted to be adverse boosts baseline performance by 2\u20137x.",
        "keywords": "red teaming;policy evaluation;targeted data collection",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Anirudha Majumdar;Mohit Sharma;Dmitry Kalashnikov;Sumeet Singh;Pierre Sermanet;Vikas Sindhwani",
        "authorids": "~Anirudha_Majumdar1;~Mohit_Sharma1;~Dmitry_Kalashnikov1;~Sumeet_Singh3;~Pierre_Sermanet1;~Vikas_Sindhwani1",
        "gender": "M;M;;M;;M",
        "homepage": "https://irom-lab.princeton.edu/majumdar/;https://mohitsharma0690.github.io/;;;https://sermanet.github.io/;http://vikas.sindhwani.org",
        "dblp": "116/6436;;222/2882;;28/6457;26/4825",
        "google_scholar": "ibu3FwsAAAAJ;;;ZGpE5cYAAAAJ;0nPi5YYAAAAJ;https://scholar.google.com/citations?hl=en",
        "orcid": ";;;;;",
        "linkedin": ";;;;sermanet/;vikassindhwani",
        "or_profile": "~Anirudha_Majumdar1;~Mohit_Sharma1;~Dmitry_Kalashnikov1;~Sumeet_Singh3;~Pierre_Sermanet1;~Vikas_Sindhwani1",
        "aff": "Google+Princeton University;Google DeepMind+Carnegie Mellon University;Google;Google Brain Robotics;Google;Google",
        "aff_domain": "google.com+princeton.edu;google.com+cmu.edu;google.com;google.com;google.com;google.com",
        "position": "Researcher+Associate Professor;Researcher+PhD student;Researcher;Researcher;Research Scientist;Senior Staff Research Scientist",
        "bibtex": "@inproceedings{\nmajumdar2025predictive,\ntitle={Predictive Red Teaming: Breaking Policies Without Breaking Robots},\nauthor={Anirudha Majumdar and Mohit Sharma and Dmitry Kalashnikov and Sumeet Singh and Pierre Sermanet and Vikas Sindhwani},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=qAGHsCsA1O}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=qAGHsCsA1O",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;0+2;0;0;0;0",
        "aff_unique_norm": "Google;Princeton University;Carnegie Mellon University",
        "aff_unique_dep": "Google;;",
        "aff_unique_url": "https://www.google.com;https://www.princeton.edu;https://www.cmu.edu",
        "aff_unique_abbr": "Google;Princeton;CMU",
        "aff_campus_unique_index": "0;;0;0;0;0",
        "aff_campus_unique": "Mountain View;",
        "aff_country_unique_index": "0+0;1+0;0;0;0;0",
        "aff_country_unique": "United States;United Kingdom"
    },
    {
        "id": "qTYD21Wpxb",
        "title": "ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Constraint-based optimization is a cornerstone of robotics, enabling the design of controllers that reliably encode task and safety requirements such as collision avoidance or formation adherence. However, handcrafted constraints can fail in multi-agent settings that demand complex coordination. We introduce ReCoDe\u2014Reinforcement-based Constraint Design\u2014a decentralized, hybrid framework that merges the reliability of optimization-based controllers with the adaptability of multi-agent reinforcement learning. Rather than discarding expert controllers, ReCoDe improves them by learning additional, dynamic constraints that capture subtler behaviors, for example, by constraining agent movements to prevent congestion in cluttered scenarios. Through local communication, agents collectively constrain their allowed actions to coordinate more effectively under changing conditions. In this work, we focus on applications of ReCoDe to multi-agent navigation tasks requiring intricate, context-based movements and consensus, where we show that it outperforms purely handcrafted controllers, other hybrid approaches, and standard MARL baselines. We give empirical (real robot) and theoretical evidence that retaining a user-defined controller, even when it is imperfect, is more efficient than learning from scratch, especially because ReCoDe can dynamically change the degree to which it relies on this controller.",
        "keywords": "multi-agent reinforcement learning;multi-robot systems",
        "primary_area": "",
        "supplementary_material": "/attachment/dabd426449651a0b1c203c7631aac93a5586b1da.zip",
        "author": "Michael Amir;Guang Yang;Zhan Gao;Keisuke Okumura;Heedo Woo;Amanda Prorok",
        "authorids": "~Michael_Amir1;~Guang_Yang16;~Zhan_Gao1;~Keisuke_Okumura1;~Heedo_Woo1;~Amanda_Prorok1",
        "gender": "M;M;;M;;",
        "homepage": "https://michaelamir.com/;https://www.guang.phd;;https://kei18.github.io/;;",
        "dblp": "207/8120;;;234/9130-1;;",
        "google_scholar": "https://scholar.google.co.il/citations?user=XaIdnakAAAAJ;Nw340R8AAAAJ;;4dFSWwMAAAAJ;;",
        "orcid": ";;;0000-0002-9491-7367;;",
        "linkedin": "michael-amir-b9881b301;gyang101/;;kei18/;;",
        "or_profile": "~Michael_Amir1;~Guang_Yang16;~Zhan_Gao1;~Keisuke_Okumura1;~Heedo_Woo1;~Amanda_Prorok1",
        "aff": "University of Cambridge;;;University of Cambridge+AIST, National Institute of Advanced Industrial Science and Technology;;",
        "aff_domain": "cam.ac.uk;;;cam.ac.uk+aist.go.jp;;",
        "position": "Postdoc;;;Postdoc+Researcher;;",
        "bibtex": "@inproceedings{\namir2025recode,\ntitle={ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination},\nauthor={Michael Amir and Guang Yang and Zhan Gao and Keisuke Okumura and Heedo Woo and Amanda Prorok},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=qTYD21Wpxb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=qTYD21Wpxb",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;0+2;1;1",
        "aff_unique_norm": "University of Cambridge;;National Institute of Advanced Industrial Science and Technology",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cam.ac.uk;;https://www.aist.go.jp",
        "aff_unique_abbr": "Cambridge;;AIST",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Cambridge;",
        "aff_country_unique_index": "0;0+2",
        "aff_country_unique": "United Kingdom;;Japan"
    },
    {
        "id": "qoKo2caB9B",
        "title": "Capability-Aware Shared Hypernetworks for Flexible Heterogeneous Multi-Robot Coordination",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Recent advances have enabled heterogeneous multi-robot teams to learn complex and effective coordination skills. \nHowever, existing neural architectures that support heterogeneous teaming tend to force a trade-off between expressivity and efficiency.\nShared-parameter designs prioritize sample efficiency by enabling a single network to be shared across all or a pre-specified subset of robots (via input augmentations), but tend to limit behavioral diversity. In contrast, recent designs employ a separate policy for each robot, enabling greater diversity and expressivity at the cost of efficiency and generalization. Our key insight is that such tradeoffs can be avoided by viewing these design choices as ends of a broad spectrum. Inspired by recent work in transfer and meta learning, and building on prior work in multi-robot task allocation, we propose Capability-Aware Shared Hypernetworks (CASH), a *soft weight sharing* architecture that uses hypernetworks to efficiently learn a *flexible* shared policy that dynamically adapts to each robot post-training. By explicitly encoding the impact of robot capabilities (e.g., speed and payload) on collective behavior, CASH enables *zero-shot generalization* to unseen robots or team compositions. Our experiments involve multiple heterogeneous tasks, three learning paradigms (imitation learning, value-based, and policy-gradient RL), and SOTA multi-robot simulation (JaxMARL) and hardware (Robotarium) platforms. Across all conditions, we find that CASH generates appropriately-diverse behaviors and consistently outperforms baseline architectures in terms of performance and sample efficiency during both training and zero-shot generalization, all with 60%-80\\% fewer learnable parameters.",
        "keywords": "Multi-Robot Learning;Heterogeneous Teams;Parameter Sharing",
        "primary_area": "",
        "supplementary_material": "/attachment/488899dd5304ebda50ac93eead211d8a34f37f99.zip",
        "author": "Kevin Fu;Shalin Jain;Pierce Howell;Harish Ravichandar",
        "authorids": "~Kevin_Fu2;~Shalin_Jain1;~Pierce_Howell1;~Harish_Ravichandar1",
        "gender": ";M;M;",
        "homepage": ";;;http://harishravichandar.com/",
        "dblp": ";;;237/9959",
        "google_scholar": ";;KTovTiQAAAAJ;d2HP6SMAAAAJ",
        "orcid": ";;;0000-0002-6635-2637",
        "linkedin": "kevin-fu-5260341b1/;shalin-jain/;;",
        "or_profile": "~Kevin_Fu2;~Shalin_Jain1;~Pierce_Howell1;~Harish_Ravichandar1",
        "aff": "Georgia Institute of Technology;Georgia Institute of Technology;Naval Information Warfare Center Pacific;Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;us.navy.mil;gatech.edu",
        "position": "Undergrad student;MS student;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nfu2025capabilityaware,\ntitle={Capability-Aware Shared Hypernetworks for Flexible Heterogeneous Multi-Robot Coordination},\nauthor={Kevin Fu and Shalin Jain and Pierce Howell and Harish Ravichandar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=qoKo2caB9B}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=qoKo2caB9B",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0",
        "aff_unique_norm": "Georgia Institute of Technology;Naval Information Warfare Center",
        "aff_unique_dep": ";Information Warfare",
        "aff_unique_url": "https://www.gatech.edu;https://www.navy.mil/niwc",
        "aff_unique_abbr": "Georgia Tech;NIWC Pacific",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pacific",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "r29CIl3ePP",
        "title": "Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Effective and efficient task planning is essential for mobile robots, especially in applications like warehouse retrieval and environmental monitoring. These tasks often involve selecting one location from each of several target clusters, forming a Generalized Traveling Salesman Problem (GTSP) that remains challenging to solve both accurately and efficiently. To address this, we propose a Multimodal Fused Learning (MMFL) framework that leverages both graph and image-based representations to capture complementary aspects of the problem, and learns a policy capable of generating high-quality task planning schemes in real time. Specifically, we first introduce a coordinate-based image builder that transforms GTSP instances into spatially informative representations. We then design an adaptive resolution scaling strategy to enhance adaptability across different problem scales, and develop a multimodal fusion module with dedicated bottlenecks that enables effective integration of geometric and spatial features. Extensive experiments show that our MMFL approach significantly outperforms state-of-the-art methods across various GTSP instances while maintaining the computational efficiency required for real-time robotic applications. Physical robot tests further validate its practical effectiveness in real-world scenarios.",
        "keywords": "Generalized Traveling Salesman Problem;Robotic Task Planning;Multimodal Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/a07dc389264b3b18cae94acb82c301b88c7c223d.zip",
        "author": "Jiaqi Cheng;Mingfeng Fan;Xuefeng Zhang;Jingsong Liang;Yuhong Cao;Guohua Wu;Guillaume Adrien Sartoretti",
        "authorids": "~Jiaqi_Cheng1;~Mingfeng_Fan2;zhang.xuefeng@u.nus.edu;~Jingsong_Liang1;~Yuhong_Cao1;guohuawu@csu.edu.cn;~Guillaume_Adrien_Sartoretti1",
        "gender": "M;F;;M;M;;M",
        "homepage": ";;;https://jingsongliang.com/;;;https://marmotlab.org/",
        "dblp": ";267/1635;;368/8380;;;118/9066",
        "google_scholar": "SLaKqSkAAAAJ;Oc7gaikAAAAJ;;https://scholar.google.com/citations?hl=en;;;n7NzZ0sAAAAJ",
        "orcid": ";;;;0000-0001-8099-0689;;0000-0002-7579-9916",
        "linkedin": ";;;jingsongliang/;;;",
        "or_profile": "~Jiaqi_Cheng1;~Mingfeng_Fan2;zhang.xuefeng@u.nus.edu;~Jingsong_Liang1;~Yuhong_Cao1;guohuawu@csu.edu.cn;~Guillaume_Adrien_Sartoretti1",
        "aff": "Central South University;National University of Singapore;;National University of Singapore;National University of Singapore;;National University of Singapore",
        "aff_domain": "csu.edu.cn;nus.edu.sg;;nus.edu.sg;nus.edu.sg;;nus.edu.sg",
        "position": "PhD student;Postdoc;;Research Staff;Postdoc;;Assistant Professor",
        "bibtex": "@inproceedings{\ncheng2025multimodal,\ntitle={Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning},\nauthor={Jiaqi Cheng and Mingfeng Fan and Xuefeng Zhang and Jingsong Liang and Yuhong Cao and Guohua Wu and Guillaume Adrien Sartoretti},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=r29CIl3ePP}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=r29CIl3ePP",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;1;1;2;1",
        "aff_unique_norm": "Central South University;National University of Singapore;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.csu.edu.cn;https://www.nus.edu.sg;",
        "aff_unique_abbr": "CSU;NUS;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;1;1;1;1",
        "aff_country_unique": "China;Singapore;"
    },
    {
        "id": "rJRFFDVTnf",
        "title": "O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Grounding object affordance is fundamental to robotic manipulation as it establishes the critical link between perception and action among interacting objects. However, prior works predominantly focus on predicting single-object affordance, overlooking the fact that most real-world interactions involve relationships between pairs of objects. In this work, we address the challenge of object-to-object affordance grounding under limited data. Inspired by recent advances in few-shot learning with 2D vision foundation models, we propose a novel one-shot 3D object-to-object affordance learning approach for robotic manipulation. Semantic features from vision foundation models combined with point cloud representation for geometric understanding enable our one-shot learning pipeline to generalize effectively to novel objects and categories. We further integrate our 3D affordance representation with large language models (LLMs) for optimization-based motion planning, significantly enhancing LLMs' capability to comprehend and reason about object interactions when generating task-specific constraint functions. Our experiments on 3D object-to-object affordance grounding and robotic manipulation demonstrate that our O$^3$Afford significantly outperforms existing baselines in terms of both accuracy and generalization capability.",
        "keywords": "Affordance Grounding;Few-shot Learning;Vision Foundation Models",
        "primary_area": "",
        "supplementary_material": "/attachment/d3f56c98c183c23294d64bbf437a84f9a8b7401c.zip",
        "author": "Tongxuan Tian;Xuhui Kang;Yen-Ling Kuo",
        "authorids": "~Tongxuan_Tian1;~Xuhui_Kang1;~Yen-Ling_Kuo1",
        "gender": "M;M;F",
        "homepage": ";;http://yenlingkuo.com",
        "dblp": ";354/8655.html;120/3172",
        "google_scholar": ";sIY8FtgAAAAJ;pNkyRs4AAAAJ",
        "orcid": ";;",
        "linkedin": "tongxuan-tian/;xuhui-joshua-kang-44314317b/;",
        "or_profile": "~Tongxuan_Tian1;~Xuhui_Kang1;~Yen-Ling_Kuo1",
        "aff": "University of Virginia, Charlottesville;University of Virginia, Charlottesville;University of Virginia, Charlottesville",
        "aff_domain": "virginia.edu;virginia.edu;virginia.edu",
        "position": "MS student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\ntian2025oafford,\ntitle={O\\${\\textasciicircum}3\\$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation},\nauthor={Tongxuan Tian and Xuhui Kang and Yen-Ling Kuo},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=rJRFFDVTnf}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=rJRFFDVTnf",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            3,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0",
        "aff_unique_norm": "University of Virginia",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.virginia.edu",
        "aff_unique_abbr": "UVA",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "Charlottesville",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "rbMoMEK4m2",
        "title": "Sequence Modeling for Time-Optimal Quadrotor Trajectory Optimization with Sampling-based Robustness Analysis",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Time-optimal trajectories drive quadrotors to their dynamic limits, but computing such trajectories involves solving non-convex problems via iterative nonlinear optimization, making them prohibitively costly for real-time applications. In this work, we investigate learning-based models that imitate a model-based time-optimal trajectory planner to accelerate trajectory generation. Given a dataset of collision-free geometric paths, we show that modeling architectures can effectively learn the patterns underlying time-optimal trajectories. We introduce a quantitative framework to analyze local analytic properties of the learned models and link them to the Backward Reachable Tube of the geometric tracking controller. To enhance robustness, we propose a data augmentation scheme that applies random perturbations to the input paths. Compared to classical planners, our method achieves substantial speedups, and we validate its real-time feasibility on a hardware quadrotor platform. Experiments demonstrate that the learned models generalize to previously unseen path lengths.",
        "keywords": "Trajectory Planning;Imitation Learning;Robustness Analysis;Aerial Robotics",
        "primary_area": "",
        "supplementary_material": "/attachment/595fa27bfe4cb1d077a014d4c69e381cb6e0c720.zip",
        "author": "Katherine Mao;Hongzhan Yu;Ruipeng Zhang;Igor Spasojevic;Sicun Gao;Vijay Kumar",
        "authorids": "~Katherine_Mao1;~Hongzhan_Yu1;~Ruipeng_Zhang2;~Igor_Spasojevic1;~Sicun_Gao1;~Vijay_Kumar2",
        "gender": "F;;;;M;",
        "homepage": ";;https://github.com/ruipengZ;;;http://kumarrobotics.org",
        "dblp": ";;;;22/8296;",
        "google_scholar": ";;;;;FUOEBDUAAAAJ",
        "orcid": ";;;0000-0002-1035-9557;;",
        "linkedin": "katherine-mao-73219b127;hongzhan-yu/;;;;",
        "or_profile": "~Katherine_Mao1;~Hongzhan_Yu1;~Ruipeng_Zhang2;~Igor_Spasojevic1;~Sicun_Gao1;~Vijay_Kumar2",
        "aff": "University of Pennsylvania;University of California, San Diego;University of California, San Diego;;University of California, San Diego;",
        "aff_domain": "seas.upenn.edu;ucsd.edu;ucsd.edu;;ucsd.edu;",
        "position": "PhD student;PhD student;PhD student;;Associate Professor;",
        "bibtex": "@inproceedings{\nmao2025sequence,\ntitle={Sequence Modeling for Time-Optimal Quadrotor Trajectory Optimization with Sampling-based Robustness Analysis},\nauthor={Katherine Mao and Hongzhan Yu and Ruipeng Zhang and Igor Spasojevic and Sicun Gao and Vijay Kumar},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=rbMoMEK4m2}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=rbMoMEK4m2",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;2;1;2",
        "aff_unique_norm": "University of Pennsylvania;University of California, San Diego;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.upenn.edu;https://www.ucsd.edu;",
        "aff_unique_abbr": "UPenn;UCSD;",
        "aff_campus_unique_index": "1;1;1",
        "aff_campus_unique": ";San Diego",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "sA2Yv4QKMr",
        "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Many robot caregiving tasks, such as bathing, dressing, and transferring, require a robot arm to make contact with a human body at multiple points rather than solely at the end effector. However, varied human touch preferences can lead to unsafe or uncomfortable multi-contact interactions. To address this, we introduce PrioriTouch, a framework integrating a novel contextual bandit algorithm with hierarchical operational space control to learn user contact preferences and translate them into low-level pose and force control policies. PrioriTouch minimizes user discomfort by initially gathering real-world feedback and subsequently refining the policy using simulation-in-the-loop, thus avoiding unsafe user experimentation. Guided by insights from a user study on physical assistance preferences, we rigorously evaluate PrioriTouch in extensive simulation and real-world experiments, demonstrating effective adaptation to user contact preferences, maintained task performance, and enhanced safety and comfort.",
        "keywords": "Physical Human-Robot Interaction;Online Preference Learning;Assistive Robotics",
        "primary_area": "",
        "supplementary_material": "/attachment/c3762a56130a2cb68af4ae9745f7db78c314edfb.zip",
        "author": "Rishabh Madan;Jiawei Lin;Mahika Goel;Amber Li;Angchen Xie;Xiaoyu Liang;Marcus Lee;Justin Guo;Pranav N. Thakkar;Rohan Banerjee;Jose Barreiros;Kate Tsui;Tom Silver;Tapomayukh Bhattacharjee",
        "authorids": "~Rishabh_Madan1;~Jiawei_Lin5;mg994@cornell.edu;adl94@cornell.edu;~Angchen_Xie1;~Xiaoyu_Liang3;mrl256@cornell.edu;jj283@cornell.edu;~Pranav_N._Thakkar1;~Rohan_Banerjee1;~Jose_Barreiros1;kate.tsui@tri.global;~Tom_Silver1;~Tapomayukh_Bhattacharjee1",
        "gender": ";F;;;M;F;;;M;;;;M;M",
        "homepage": "https://cs.cornell.edu/~madan;https://jiaweilin.com/;;;https://angchenxie.github.io/;;;;https://pranavnnt.github.io;https://rohanb2018.github.io;;;https://web.mit.edu/tslvr/www/;http://www.tapomayukh.com",
        "dblp": ";;;;;;;;;;;;202/1778;74/8368",
        "google_scholar": ";5Jo9oS0AAAAJ;;;;;;;R8mbJXUAAAAJ;cxXPYo8AAAAJ;mLFRRpkAAAAJ;;CMcsygMAAAAJ;X1zsXTgAAAAJ",
        "orcid": ";;;;;;;;;;;;;0000-0001-9457-5726",
        "linkedin": ";;;;;xiaoyuliang14/;;;;;;;;tapomayukh",
        "or_profile": "~Rishabh_Madan1;~Jiawei_Lin5;mg994@cornell.edu;adl94@cornell.edu;~Angchen_Xie1;~Xiaoyu_Liang3;mrl256@cornell.edu;jj283@cornell.edu;~Pranav_N._Thakkar1;~Rohan_Banerjee1;~Jose_Barreiros1;kate.tsui@tri.global;~Tom_Silver1;~Tapomayukh_Bhattacharjee1",
        "aff": "Cornell University;Cornell University;;;Shanghai Jiaotong University;Cornell University;;;Cornell University;Cornell University;Toyota Research Institute;;Princeton University+Cornell University;Cornell University",
        "aff_domain": "cornell.edu;cornell.edu;;;sjtu.edu.cn;cornell.edu;;;cornell.edu;cornell.edu;tri.global;;princeton.edu+cornell.edu;cornell.edu",
        "position": "PhD student;Undergrad student;;;Undergrad student;Undergrad student;;;PhD student;PhD student;Researcher;;Assistant Professor+Postdoc;Assistant Professor",
        "bibtex": "@inproceedings{\nmadan2025prioritouch,\ntitle={PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction},\nauthor={Rishabh Madan and Jiawei Lin and Mahika Goel and Amber Li and Angchen Xie and Xiaoyu Liang and Marcus Lee and Justin Guo and Pranav N. Thakkar and Rohan Banerjee and Jose Barreiros and Kate Tsui and Tom Silver and Tapomayukh Bhattacharjee},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=sA2Yv4QKMr}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sA2Yv4QKMr",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            14,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;1;2;0;1;1;0;0;3;1;4+0;0",
        "aff_unique_norm": "Cornell University;;Shanghai Jiao Tong University;Toyota Research Institute;Princeton University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.cornell.edu;;https://www.sjtu.edu.cn;https://www.tri.global;https://www.princeton.edu",
        "aff_unique_abbr": "Cornell;;SJTU;TRI;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;2;0;0;0;0;0+0;0",
        "aff_country_unique": "United States;;China"
    },
    {
        "id": "sMs4pJYhWi",
        "title": "Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "We present TacX, the first multisensory touch representations across four tactile modalities: image, audio, motion, and pressure. Trained on ~1M contact-rich interactions collected with the Digit 360 sensor, TacX captures complementary touch signals at diverse temporal and spatial scales. By leveraging self-supervised learning, TacX fuses these modalities into a unified representation that captures physical properties useful for downstream robot manipulation tasks. We study how to effectively integrate real-world touch representations for both imitation learning and tactile adaptation of sim-trained policies, showing that TacX boosts policy success rates by 63% over an end-to-end model using tactile images and improves robustness by 90% in recovering object states from touch. Finally, we benchmark TacX\u2019s ability to make inference about physical properties, such as object-action identification, material-quantity estimation and force estimation. TacX improves accuracy in characterizing physical properties by 48% compared to end-to-end approaches, demonstrating the advantages of multisensory pretraining for capturing features essential for dexterous manipulation.",
        "keywords": "Multi-sensory Touch;Self-Supervised Learning;Tactile Adaptation",
        "primary_area": "",
        "supplementary_material": "/attachment/9fe69a3221f349b1b65bbed922e074036d352ac8.zip",
        "author": "Carolina Higuera;Akash Sharma;Taosha Fan;Chaithanya Krishna Bodduluri;Byron Boots;Michael Kaess;Mike Lambeta;Tingfan Wu;Zixi Liu;Francois Robert Hogan;Mustafa Mukadam",
        "authorids": "~Carolina_Higuera1;~Akash_Sharma1;~Taosha_Fan1;~Chaithanya_Krishna_Bodduluri1;~Byron_Boots1;~Michael_Kaess1;~Mike_Lambeta1;~Tingfan_Wu2;~Zixi_Liu1;~Francois_Robert_Hogan1;~Mustafa_Mukadam1",
        "gender": "F;M;;M;;M;M;M;F;;M",
        "homepage": ";https://akashsharma02.github.io;https://github.com/fantaosha;;;https://www.cs.cmu.edu/~kaess/;;;;https://fhogan.github.io;http://www.mustafamukadam.com",
        "dblp": ";;;;;26/6036;;;;190/7406.html;",
        "google_scholar": "https://scholar.google.es/citations?hl=es;LhKc2CsAAAAJ;;;;27eupmsAAAAJ;;https://scholar.google.com/citations?hl=en;4eeVPBoAAAAJ;;yYpm9LoAAAAJ",
        "orcid": "0000-0001-5141-0817;;;;;0000-0002-7590-3357;;;;;",
        "linkedin": ";;;krishna-bck;;michaelkaess/;mike-maroje-lambeta;;;;mhmukadam/",
        "or_profile": "~Carolina_Higuera1;~Akash_Sharma1;~Taosha_Fan1;~Chaithanya_Krishna_Bodduluri1;~Byron_Boots1;~Michael_Kaess1;~Mike_Lambeta1;~Tingfan_Wu2;~Zixi_Liu1;~Francois_Robert_Hogan1;~Mustafa_Mukadam1",
        "aff": "University of Washington;Meta Facebook+Carnegie Mellon University;;Meta Facebook;;Carnegie Mellon University;Meta;;;Meta Facebook;Amazon Robotics",
        "aff_domain": "uw.edu;meta.com+cs.cmu.edu;;meta.com;;cmu.edu;meta.com;;;meta.com;amazon.com",
        "position": "PhD student;Researcher+PhD student;;Researcher;;Associate Professor;Engineer;;;Researcher;Researcher",
        "bibtex": "@inproceedings{\nhiguera2025tactile,\ntitle={Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation},\nauthor={Carolina Higuera and Akash Sharma and Taosha Fan and Chaithanya Krishna Bodduluri and Byron Boots and Michael Kaess and Mike Lambeta and Tingfan Wu and Mustafa Mukadam and Zixi Liu and Francois Robert Hogan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=sMs4pJYhWi}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sMs4pJYhWi",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            11,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1+2;3;1;3;2;1;3;3;1;4",
        "aff_unique_norm": "University of Washington;Meta;Carnegie Mellon University;;Amazon",
        "aff_unique_dep": ";Meta Platforms, Inc.;;;Amazon Robotics",
        "aff_unique_url": "https://www.washington.edu;https://meta.com;https://www.cmu.edu;;https://www.amazonrobotics.com",
        "aff_unique_abbr": "UW;Meta;CMU;;Amazon Robotics",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0+0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "sUWOSP6SUJ",
        "title": "Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Imitation Learning can train robots to perform complex and diverse manipulation tasks, but learned policies are brittle with observations outside of the training distribution. 3D scene representations that incorporate observations from calibrated RGBD cameras have been proposed as a way to mitigate this, but in our evaluations with unseen embodiments and camera viewpoints they show only modest improvement. To address those challenges, we propose Adapt3R, a general-purpose 3D observation encoder which synthesizes data from calibrated RGBD cameras into a vector that can be used as conditioning for arbitrary IL algorithms. The key idea is to use a pretrained 2D backbone to extract semantic information, using 3D only as a medium to localize this information with respect to the end-effector. We show across 93 simulated and 6 real tasks that when trained end-to-end with a variety of IL algorithms, Adapt3R maintains these algorithms' learning capacity while enabling zero-shot transfer to novel embodiments and camera poses. For more results, visit https://adapt3r-robot.github.io.",
        "keywords": "Imitation Learning;3D Perception;Cross-Embodiment Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/e196bb0d34d56de9faf8db808acd708611ae829e.zip",
        "author": "Albert Wilcox;Mohamed Ghanem;Masoud Moghani;Pierre Barroso;Benjamin Joffe;Animesh Garg",
        "authorids": "~Albert_Wilcox1;~Mohamed_Ghanem2;~Masoud_Moghani1;~Pierre_Barroso1;~Benjamin_Joffe1;~Animesh_Garg1",
        "gender": "M;M;;M;;M",
        "homepage": "https://albertwilcox.github.io/;;https://masoudmoghani.com/;;https://research.gatech.edu/people/benjamin-joffe;http://animesh.garg.tech",
        "dblp": ";;181/3949;;126/2732;123/5728",
        "google_scholar": "bj628LsAAAAJ;;https://scholar.google.ca/citations?user=7wfzKHam13MC;;IaNhZ9AAAAAJ;zp8V7ZMAAAAJ",
        "orcid": ";;;;0000-0001-8574-0136;0000-0003-0482-4296",
        "linkedin": "albert-wilcox-314898184/;mohghanem/;;pierre-barroso;bpjoffe;animeshgarg/",
        "or_profile": "~Albert_Wilcox1;~Mohamed_Ghanem2;~Masoud_Moghani1;~Pierre_Barroso1;~Benjamin_Joffe1;~Animesh_Garg1",
        "aff": "Georgia Institute of Technology;Georgia Institute of Technology;NVIDIA+University of Toronto;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology",
        "aff_domain": "gatech.edu;gatech.edu;nvidia.com+utoronto.ca;gatech.edu;gatech.edu;gatech.edu",
        "position": "PhD student;MS student;Intern+PhD student;Researcher;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nwilcox2025adaptr,\ntitle={Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning},\nauthor={Albert Wilcox and Mohamed Ghanem and Masoud Moghani and Pierre Barroso and Benjamin Joffe and Animesh Garg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=sUWOSP6SUJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sUWOSP6SUJ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1+2;0;0;0",
        "aff_unique_norm": "Georgia Institute of Technology;NVIDIA;University of Toronto",
        "aff_unique_dep": ";NVIDIA Corporation;",
        "aff_unique_url": "https://www.gatech.edu;https://www.nvidia.com;https://www.utoronto.ca",
        "aff_unique_abbr": "Georgia Tech;NVIDIA;U of T",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0+1;0;0;0",
        "aff_country_unique": "United States;Canada"
    },
    {
        "id": "sVWKm4UiTL",
        "title": "Multi-critic Learning for Whole-body End-effector Twist Tracking",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Learning whole-body control for locomotion and arm motions in a single policy has challenges, as the two tasks have conflicting goals. For instance, efficient locomotion typically favors a horizontal base orientation, while end-effector tracking may benefit from base tilting to extend reachability. Additionally, current Reinforcement Learning (RL) approaches using a pose-based task specification lack the ability to directly control the end-effector velocity, making smoothly executing trajectories very challenging. To address these limitations, we propose an RL-based framework that allows for dynamic, velocity-aware whole-body end-effector control. Our method introduces a multi-critic actor architecture that decouples the reward signals for locomotion and manipulation, simplifying reward tuning and allowing the policy to resolve task conflicts more effectively. Furthermore, we design a twist-based end-effector task formulation that can track both discrete poses and motion trajectories. We validate our approach through a set of simulation and hardware experiments using a quadruped robot equipped with a robotic arm. The resulting controller can simultaneously walk and move its end-effector and shows emergent whole-body behaviors, where the base assists the arm in extending the workspace, despite a lack of explicit formulations.",
        "keywords": "Loco-Manipulation;Multi-critic Reinforcement Learning;Whole-Body Control",
        "primary_area": "",
        "supplementary_material": "/attachment/7e1e732da63987820a2194624912d00e12899949.zip",
        "author": "Aravind Elanjimattathil Vijayan;Andrei Cramariuc;Mattia Risiglione;Christian Gehring;Marco Hutter",
        "authorids": "~Aravind_Elanjimattathil_Vijayan1;~Andrei_Cramariuc1;~Mattia_Risiglione1;cgehring@anybotics.com;~Marco_Hutter1",
        "gender": "M;M;M;;M",
        "homepage": ";;;;http://www.rsl.ethz.ch",
        "dblp": "226/6428;;;;04/2753",
        "google_scholar": "https://scholar.google.co.in/citations?user=BbO9oFkAAAAJ;QZKCzOQAAAAJ;PAuJ1cUAAAAJ;;https://scholar.google.ch/citations?user=DO3quJYAAAAJ",
        "orcid": "0000-0002-0626-5021;0000-0002-9301-0253;;;0000-0002-4285-4990",
        "linkedin": "https://ch.linkedin.com/in/evaravind;andrei-cramariuc-43625b163/;;;",
        "or_profile": "~Aravind_Elanjimattathil_Vijayan1;~Andrei_Cramariuc1;~Mattia_Risiglione1;cgehring@anybotics.com;~Marco_Hutter1",
        "aff": "ETHZ - ETH Zurich;ETHZ - ETH Zurich;ETHZ - ETH Zurich;;ETHZ - ETH Zurich",
        "aff_domain": "ethz.ch;ethz.ch;ethz.ch;;ethz.ch",
        "position": "PhD student;Postdoc;Researcher;;Associate Professor",
        "bibtex": "@inproceedings{\nvijayan2025multicritic,\ntitle={Multi-critic Learning for Whole-body End-effector Twist Tracking},\nauthor={Aravind Elanjimattathil Vijayan and Andrei Cramariuc and Mattia Risiglione and Christian Gehring and Marco Hutter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=sVWKm4UiTL}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sVWKm4UiTL",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1;0",
        "aff_unique_norm": "ETH Zurich;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ethz.ch;",
        "aff_unique_abbr": "ETHZ;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0",
        "aff_country_unique": "Switzerland;"
    },
    {
        "id": "sXoaNAECCK",
        "title": "Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Manipulating clothing is challenging due to their complex, variable configurations and frequent self-occlusion. While prior systems often rely on flattening garments, humans routinely identify keypoints in highly crumpled and suspended states. We present a novel, task-agnostic, visuotactile framework that operates directly on crumpled clothing\u2014including in-air configurations that have not been addressed before. Our approach combines global visual perception with local tactile feedback to enable robust, reactive manipulation. We train dense visual descriptors on a custom simulated dataset using a distributional loss that captures cloth symmetries and generates correspondence confidence estimates. These estimates guide a reactive state machine that dynamically selects between folding strategies based on perceptual uncertainty. In parallel, we train a visuotactile grasp affordance network using high-resolution tactile feedback to supervise grasp success. The same tactile classifier is used during execution for real-time grasp validation. Together, these components enable a reactive, task-agnostic framework for in-air garment manipulation, including folding and hanging tasks. Moreover, our dense descriptors serve as a versatile intermediate representation for other planning modalities, such as extracting grasp targets from human video demonstrations, paving the way for more generalizable and scalable garment manipulation.",
        "keywords": "Deformable Object Manipulation;Dense Correspondence Learning;Confidence-Aware Planning;Visuotactile Perception",
        "primary_area": "",
        "supplementary_material": "/attachment/b0b109db77e2a0718aac47e91e804c1e9ff6fb3a.pdf",
        "author": "Neha Sunil;Megha Tippur;Arnau Saumell Portillo;Edward H Adelson;Alberto Rodriguez Garcia",
        "authorids": "~Neha_Sunil1;~Megha_Tippur1;~Arnau_Saumell_Portillo1;~Edward_H_Adelson1;~Alberto_Rodriguez_Garcia1",
        "gender": "F;F;M;;M",
        "homepage": ";;;;http://mcube.mit.edu/",
        "dblp": ";;;;",
        "google_scholar": "2bZ5LxEAAAAJ;ucnPAUAAAAAJ;;;AC93g9kAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";;arnau-saumell/;;",
        "or_profile": "~Neha_Sunil1;~Megha_Tippur1;~Arnau_Saumell_Portillo1;~Edward_H_Adelson1;~Alberto_Rodriguez_Garcia1",
        "aff": "Massachusetts Institute of Technology;Massachusetts Institute of Technology;Universidad Polit\u00e9cnica de Cataluna;;Massachusetts Institute of Technology",
        "aff_domain": "mit.edu;mit.edu;upc.edu;;mit.edu",
        "position": "PhD student;PhD student;MS student;;Associate Professor",
        "bibtex": "@inproceedings{\nsunil2025reactive,\ntitle={Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance},\nauthor={Neha Sunil and Megha Tippur and Arnau Saumell Portillo and Edward H Adelson and Alberto Rodriguez Garcia},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=sXoaNAECCK}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=sXoaNAECCK",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;2;0",
        "aff_unique_norm": "Massachusetts Institute of Technology;Universitat Polit\u00e8cnica de Catalunya;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://web.mit.edu;https://www.upc.edu;",
        "aff_unique_abbr": "MIT;UPC;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;1;0",
        "aff_country_unique": "United States;Spain;"
    },
    {
        "id": "seSw6ssEid",
        "title": "Residual Neural Terminal Constraint for MPC-based Collision Avoidance in Dynamic Environments",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In this paper, we propose a hybrid MPC local planner that uses a learning-based approximation of a time-varying safe set, derived from local observations and applied as the MPC terminal constraint. This set can be represented as a zero-superlevel set of the value function computed via Hamilton-Jacobi (HJ) reachability analysis, which is infeasible in real-time. We exploit the property that the HJ value function can be expressed as a difference of the corresponding signed distance function (SDF) and a non-negative residual function. The residual component is modeled as a neural network with non-negative output and subtracted from the computed SDF, resulting in a real-time value function estimate that is at least as safe as the SDF by design. Additionally, we parametrize the neural residual by a hypernetwork to improve real-time performance and generalization properties. The proposed method is compared with three state-of-the-art methods in simulations and hardware experiments, achieving up to 30\\% higher success rates compared to the best baseline while requiring a similar computational effort and producing high-quality (low travel-time) solutions.",
        "keywords": "MPC;Obstacle Avoidance;Learning for Control",
        "primary_area": "",
        "supplementary_material": "/attachment/0ca6f7206b7517109a52e64e06312bb473b6eadf.zip",
        "author": "Bojan Derajic;Mohamed-Khalil Bouzidi;Sebastian Bernhard;Wolfgang H\u00f6nig",
        "authorids": "~Bojan_Derajic2;~Mohamed-Khalil_Bouzidi1;~Sebastian_Bernhard1;~Wolfgang_H\u00f6nig1",
        "gender": "M;M;M;M",
        "homepage": "https://github.com/bojan-derajic;;;https://whoenig.github.io",
        "dblp": ";;182/7580.html;180/9152",
        "google_scholar": ";https://scholar.google.de/citations?user=GuuWSskAAAAJ;;XUQcbFAAAAAJ",
        "orcid": ";;0000-0002-7194-7539;",
        "linkedin": "bojan-derajic/;mohamed-khalil-bouzidi-94a834175;dr-ing-sebastian-bernhard-79a763205/;wolfgang-h%C3%B6nig-71385599",
        "or_profile": "~Bojan_Derajic2;~Mohamed-Khalil_Bouzidi1;~Sebastian_Bernhard1;~Wolfgang_H\u00f6nig1",
        "aff": "Technische Universit\u00e4t Berlin+Continental AG;Freie Universit\u00e4t Berlin+Continental AG;Continental AG;Technische Universit\u00e4t Berlin",
        "aff_domain": "tu-berlin.de+continental-corporation.com;fu-berlin.de+continental.com;continental.com;tu-berlin.de",
        "position": "PhD student+Researcher;PhD student+Researcher;Principal Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nderajic2025residual,\ntitle={Residual Neural Terminal Constraint for {MPC}-based Collision Avoidance in Dynamic Environments},\nauthor={Bojan Derajic and Mohamed-Khalil Bouzidi and Sebastian Bernhard and Wolfgang H{\\\"o}nig},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=seSw6ssEid}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=seSw6ssEid",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;2+1;1;0",
        "aff_unique_norm": "Technische Universit\u00e4t Berlin;Continental AG;Freie Universit\u00e4t Berlin",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.tu-berlin.de;https://www.continental-tyres.com/;https://www.fu-berlin.de",
        "aff_unique_abbr": "TU Berlin;Continental;FU Berlin",
        "aff_campus_unique_index": ";",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0+0;0;0",
        "aff_country_unique": "Germany"
    },
    {
        "id": "t2asRJv2SD",
        "title": "Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Robots equipped with rich sensor suites can localize reliably in partially-observable environments---but powering every sensor continuously is wasteful and often infeasible. Belief-space planners address this by propagating pose-belief covariance through analytic models and switching sensors heuristically--a brittle, runtime expensive approach. Data-driven approaches--including diffusion models--learn multi-modal trajectories from demonstrations, but presuppose an accurate, always-on state estimate. We address the largely open problem: for a given task in a  mapped environment, which *minimal sensor subset* must be active at each location to maintain state uncertainty *just low enough* to complete the task? Our key insight is that when a diffusion planner is explicitly conditioned on a pose-belief raster and a sensor mask, the spread of its denoising trajectories yields a calibrated, differentiable proxy for the expected localization error. Building on this insight, we present Belief-Conditioned One-Step Diffusion (B-COD), the first planner that, in a 10 ms forward pass, returns a short-horizon trajectory, per-waypoint aleatoric variances, and a proxy for localization error--eliminating external covariance rollouts. We show that this single proxy suffices for a soft-actor\u2013critic to choose sensors online, optimising energy while bounding pose-covariance growth. We deploy B-COD in real-time marine trials on an unmanned surface vehicle and show that it reduces sensing energy consumption while matching the goal-reach performance of an always-on baseline.",
        "keywords": "Navigation;Planning Under Uncertainty;Multi-Modal Sensing",
        "primary_area": "",
        "supplementary_material": "/attachment/57b498e54dd7685f62cc9c0c623c51aba6eadf56.zip",
        "author": "Gokul Puthumanaillam;Aditya Penumarti;Manav Vora;Paulo Padrao;Jose Fuentes;Leonardo Bobadilla;Jane Shin;Melkior Ornik",
        "authorids": "~Gokul_Puthumanaillam1;~Aditya_Penumarti1;~Manav_Vora1;~Paulo_Padrao1;~Jose_Fuentes2;~Leonardo_Bobadilla1;~Jane_Shin1;~Melkior_Ornik1",
        "gender": "M;M;M;;M;M;F;",
        "homepage": "https://gokulp01.github.io;;;;;http://users.cis.fiu.edu/~jabobadi/;;",
        "dblp": "347/0555;;;;;43/2125;;",
        "google_scholar": "xyxJ5VEAAAAJ;https://scholar.google.ca/citations?hl=en;https://scholar.google.com/citations?hl=en;;w8-ptKYAAAAJ;-lKA19EAAAAJ;;",
        "orcid": ";;;;;0000-0003-2097-2432;0000-0002-5755-8385;",
        "linkedin": "https://linkedin.com/in/gokulp01/;;;;;;;",
        "or_profile": "~Gokul_Puthumanaillam1;~Aditya_Penumarti1;~Manav_Vora1;~Paulo_Padrao1;~Jose_Fuentes2;~Leonardo_Bobadilla1;~Jane_Shin1;~Melkior_Ornik1",
        "aff": "University of Illinois, Urbana Champaign;University of Florida;University of Illinois, Urbana Champaign;;Florida International University;Florida International University;University of Florida;",
        "aff_domain": "illinois.edu;ufl.edu;illinois.edu;;fiu.edu;fiu.edu;ufl.edu;",
        "position": "PhD student;PhD student;PhD student;;PhD student;Associate Professor;Assistant Professor;",
        "bibtex": "@inproceedings{\nputhumanaillam2025beliefconditioned,\ntitle={Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing},\nauthor={Gokul Puthumanaillam and Aditya Penumarti and Manav Vora and Paulo Padrao and Jose Fuentes and Leonardo Bobadilla and Jane Shin and Melkior Ornik},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=t2asRJv2SD}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=t2asRJv2SD",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;2;3;3;1;2",
        "aff_unique_norm": "University of Illinois Urbana-Champaign;University of Florida;;Florida International University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://illinois.edu;https://www.ufl.edu;;https://www.fiu.edu",
        "aff_unique_abbr": "UIUC;UF;;FIU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Urbana-Champaign;",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "tXY6VQlXfA",
        "title": "CHD: Coupled Hierarchical Diffusion for Long-Horizon Tasks",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Diffusion-based planners have shown strong performance in short-horizon tasks but often fail in complex, long-horizon settings. We trace the failure to loose coupling between high-level (HL) sub-goal selection and low-level (LL) trajectory generation, which leads to incoherent plans and degraded performance. We propose Coupled Hierarchical Diffusion (CHD), a framework that models HL sub-goals and LL trajectories jointly within a unified diffusion process. A shared classifier passes LL feedback upstream so that sub-goals self-correct while sampling proceeds. This tight HL\u2013LL coupling improves trajectory coherence and enables scalable long-horizon diffusion planning. Experiments across maze navigation, tabletop manipulation, and household environments show that CHD consistently outperforms both flat and hierarchical diffusion baselines.",
        "keywords": "Diffusion Planner;Long-horizon Planning;Hierarchical Planning",
        "primary_area": "",
        "supplementary_material": "/attachment/a44f3becbc9abe520cee0bee2db3d7cbdbc79368.zip",
        "author": "Ce Hao;Anxing Xiao;Zhiwei Xue;Harold Soh",
        "authorids": "~Ce_Hao1;~Anxing_Xiao1;~Zhiwei_Xue1;~Harold_Soh1",
        "gender": "M;M;Not Specified;M",
        "homepage": ";https://anxingxiao.com;;http://www.haroldsoh.com",
        "dblp": ";272/5104;;06/4578",
        "google_scholar": "Yk3s7HUAAAAJ;qrgIuiEAAAAJ;;https://scholar.google.com.sg/citations?user=lkgd1BsAAAAJ",
        "orcid": "0009-0000-7653-9713;;;",
        "linkedin": ";;zhiwei-xue-280b84256/;",
        "or_profile": "~Ce_Hao1;~Anxing_Xiao1;~Zhiwei_Xue1;~Harold_Soh1",
        "aff": "national university of singaore, National University of Singapore;National University of Singapore;National University of Singapore;National University of Singapore",
        "aff_domain": "u.nus.edu;nus.edu.sg;nus.edu.sg;nus.edu.sg",
        "position": "PhD student;PhD student;PhD student;Assistant Professor",
        "bibtex": "@inproceedings{\nhao2025chd,\ntitle={{CHD}: Coupled Hierarchical Diffusion for Long-Horizon Tasks},\nauthor={Ce Hao and Anxing Xiao and Zhiwei Xue and Harold Soh},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=tXY6VQlXfA}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=tXY6VQlXfA",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": "national university of singaore, National University of Singapore;National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.nus.edu.sg",
        "aff_unique_abbr": ";NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";Singapore"
    },
    {
        "id": "thVTNoJ4Lx",
        "title": "ObjectReact: Learning Object-Relative Control for Visual Navigation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Visual navigation using only a single camera and a topological map has recently become an appealing alternative to methods that require additional sensors and 3D maps. This is typically achieved through an \"image-relative\" approach to estimating control from a given pair of current observation and subgoal image. However, image-level representations of the world have limitations because images are strictly tied to the agent's pose and embodiment. In contrast, objects, being a property of the map, offer an embodiment- and trajectory-invariant world representation. In this work, we present a new paradigm of learning \"object-relative\" control that exhibits several desirable characteristics: a) new routes can be traversed without strictly requiring to imitate prior experience, b) the control prediction problem can be decoupled from solving the image matching problem, and c) high invariance can be achieved in cross-embodiment deployment for variations across both training-testing and mapping-execution settings. We propose a topometric map representation in the form of a relative\" 3D scene graph, which is used to obtain more informative object-level global path planning costs. We train a local controller, dubbed \"ObjectReact\", conditioned directly on a high-level ``WayObject Costmap'' representation that eliminates the need for an explicit RGB input.\n We demonstrate the advantages of learning object-relative control over its image-relative counterpart across sensor height variations and multiple navigation tasks that challenge the underlying spatial understanding capability, e.g., navigating a map trajectory in the reverse direction. We further show that our sim-only policy is able to generalize well to real-world indoor environments.",
        "keywords": "visual navigation;topological;object;learning",
        "primary_area": "",
        "supplementary_material": "/attachment/1d9e823ffe6cbe238fba323c3822a342edc08741.zip",
        "author": "Sourav Garg;Dustin Craggs;Vineeth Bhat;Lachlan Mares;Stefan Podgorski;Madhava Krishna;Feras Dayoub;Ian Reid",
        "authorids": "~Sourav_Garg1;~Dustin_Craggs1;~Vineeth_Bhat1;~Lachlan_Mares1;stefan.podgorski@adelaide.edu.au;~Madhava_Krishna2;~Feras_Dayoub1;~Ian_Reid1",
        "gender": "M;M;M;M;;;M;M",
        "homepage": "https://oravus.github.io/;;https://flightvin.github.io/;https://www.adelaide.edu.au/directory/lachlan.mares;;;http://www.ferasdayoub.com/;",
        "dblp": "142/0073;;;;;;98/978;r/IanDReid1",
        "google_scholar": "oVS3HHIAAAAJ;https://scholar.google.com.au/citations?user=M0mOedsAAAAJ;vsqwwPYAAAAJ;;;;https://scholar.google.com.au/citations?user=Lzs8CuEAAAAJ;https://scholar.google.com.au/citations?user=ATkNLcQAAAAJ",
        "orcid": "0000-0001-6068-3307;;;0000-0003-0901-046X;;;0000-0002-4234-7374;0000-0001-7790-6423",
        "linkedin": "gargsourav/;;vineethbhat/;lachlan-mares-b469a493/;;;feras-dayoub-6b8454114/;",
        "or_profile": "~Sourav_Garg1;~Dustin_Craggs1;~Vineeth_Bhat1;~Lachlan_Mares1;stefan.podgorski@adelaide.edu.au;~Madhava_Krishna2;~Feras_Dayoub1;~Ian_Reid1",
        "aff": "University of Adelaide;University of Adelaide;International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad;University of Adelaide;;;University of Adelaide;Mohamed bin Zayed University of Artificial Intelligence+University of Adelaide",
        "aff_domain": "adelaide.edu.au;adelaide.edu.au;students.iiit.ac.in;adelaide.edu.au;;;adelaide.edu.au;mbzuai.ac.ae+adelaide.edu.au",
        "position": "Postdoc;MS student;Undergrad student;Researcher;;;Associate Professor;Full Professor+Professor",
        "bibtex": "@inproceedings{\ngarg2025objectreact,\ntitle={ObjectReact: Learning Object-Relative Control for Visual Navigation},\nauthor={Sourav Garg and Dustin Craggs and Vineeth Bhat and Lachlan Mares and Stefan Podgorski and Madhava Krishna and Feras Dayoub and Ian Reid},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=thVTNoJ4Lx}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=thVTNoJ4Lx",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1;0;2;2;0;3+0",
        "aff_unique_norm": "University of Adelaide;International Institute of Information Technology, Hyderabad;;Mohamed bin Zayed University of Artificial Intelligence",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.adelaide.edu.au;https://iiit Hyderabad.ac.in;;https://mbzuai.ac.ae",
        "aff_unique_abbr": "Adelaide;IIIT Hyderabad;;MBZUAI",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Hyderabad",
        "aff_country_unique_index": "0;0;1;0;0;3+0",
        "aff_country_unique": "Australia;India;;United Arab Emirates"
    },
    {
        "id": "uA9GZEmGiT",
        "title": "CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Path planning in unknown environments is a crucial yet inherently challenging capability for mobile robots, which primarily encompasses two coupled tasks: autonomous exploration and point-goal navigation. In both cases, the robot must perceive the environment, update its belief, and accurately estimate potential information gain on-the-fly to guide planning. In this work, we propose CogniPlan, a novel path planning framework that leverages multiple plausible layouts predicted by a conditional generative inpainting model, mirroring how humans rely on cognitive maps during navigation. These predictions, based on the partially observed map and a set of layout conditioning vectors, enable our planner to reason effectively under uncertainty. We demonstrate strong synergy between generative image-based layout prediction and graph-attention-based path planning, allowing CogniPlan to combine the scalability of graph representations with the fidelity and predictiveness of occupancy maps, yielding notable performance gains in both exploration and navigation. We extensively evaluate CogniPlan on two datasets (hundreds of maps and realistic floor plans), consistently outperforming state-of-the-art planners. We further deploy it in a high-fidelity simulator and on hardware, showcasing its high-quality path planning and real-world applicability.",
        "keywords": "Path Planning under Uncertainty;Map Prediction;Graph Attention",
        "primary_area": "",
        "supplementary_material": "/attachment/27fd7c0c3a7ca14f2c5d23eacac9063da2fbd1ec.zip",
        "author": "Yizhuo Wang;Haodong He;Jingsong Liang;Yuhong Cao;Ritabrata Chakraborty;Guillaume Adrien Sartoretti",
        "authorids": "~Yizhuo_Wang1;~Haodong_He2;~Jingsong_Liang1;~Yuhong_Cao1;~Ritabrata_Chakraborty2;~Guillaume_Adrien_Sartoretti1",
        "gender": "M;Not Specified;M;M;M;M",
        "homepage": "https://github.com/wyzh98;;https://jingsongliang.com/;;;https://marmotlab.org/",
        "dblp": ";;368/8380;;;118/9066",
        "google_scholar": ";;https://scholar.google.com/citations?hl=en;;;n7NzZ0sAAAAJ",
        "orcid": ";;;0000-0001-8099-0689;;0000-0002-7579-9916",
        "linkedin": ";haodong-he-085933361/;jingsongliang/;;ritabrata-chakraborty-a63268251/;",
        "or_profile": "~Yizhuo_Wang1;~Haodong_He2;~Jingsong_Liang1;~Yuhong_Cao1;~Ritabrata_Chakraborty2;~Guillaume_Adrien_Sartoretti1",
        "aff": "national university of singaore, National University of Singapore;Tongji University;National University of Singapore;National University of Singapore;Birla Institute of Technology & Science, Pilani (BITS Pilani);National University of Singapore",
        "aff_domain": "u.nus.edu;tongji.edu.cn;nus.edu.sg;nus.edu.sg;pilani.bits-pilani.ac.in;nus.edu.sg",
        "position": "PhD student;Undergrad student;Research Staff;Postdoc;Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025cogniplan,\ntitle={CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction},\nauthor={Yizhuo Wang and Haodong He and Jingsong Liang and Yuhong Cao and Ritabrata Chakraborty and Guillaume Adrien Sartoretti},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=uA9GZEmGiT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=uA9GZEmGiT",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;2;3;2",
        "aff_unique_norm": "national university of singaore, National University of Singapore;Tongji University;National University of Singapore;Birla Institute of Technology & Science, Pilani (BITS Pilani)",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";https://www.tongji.edu.cn;https://www.nus.edu.sg;",
        "aff_unique_abbr": ";Tongji;NUS;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;2;2;2",
        "aff_country_unique": ";China;Singapore"
    },
    {
        "id": "uWFlkufjFJ",
        "title": "FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via Flow Variational Inference",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from partial observations remains a critical challenge in robot learning. Prior generative methods struggle to model the intricate grasp distribution of dexterous hands and often fail to reason about shape uncertainty inherent in partial point clouds, leading to unreliable or overly conservative grasps. We propose FFHFlow, a flow-based variational framework that generates diverse, robust multi-finger grasps while explicitly quantifying perceptual uncertainty in the partial point clouds. Our approach leverages a normalizing flow-based deep latent variable model to learn a hierarchical grasp manifold, overcoming the mode collapse and rigid prior limitations of conditional Variational Autoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of flows, FFHFlow introspects shape uncertainty in partial observations and identifies novel object structures, enabling risk-aware grasp synthesis. To further enhance reliability, we integrate a discriminative grasp evaluator with the flow likelihoods, formulating an uncertainty-aware ranking strategy that prioritizes grasps robust to shape ambiguity. Extensive experiments in simulation and real-world setups demonstrate that FFHFlow outperforms state-of-the-art baselines (including diffusion models) in grasp diversity and success rate, while achieving run-time efficient sampling. We also showcase its practical value in cluttered and confined environments, where diversity-driven sampling excels by mitigating collisions.",
        "keywords": "Dexterous Grasping;Normalizing Flows;Uncertainty-Awareness",
        "primary_area": "",
        "supplementary_material": "/attachment/9bcc193443f02ade37dde548a9ca6b8acf46c45e.pdf",
        "author": "Qian Feng;Jianxiang Feng;Zhaopeng Chen;Rudolph Triebel;Alois Knoll",
        "authorids": "~Qian_Feng2;~Jianxiang_Feng1;zhaopeng.chen@agile-robots.com;~Rudolph_Triebel1;~Alois_Knoll1",
        "gender": "M;M;;;M",
        "homepage": ";https://jianxiangfeng.github.io/;;;https://www.in.tum.de/i06/people/prof-dr-ing-habil-alois-knoll/",
        "dblp": ";267/9411;;;k/AloisKnoll",
        "google_scholar": "KJ5hWpYAAAAJ;b-5CscIAAAAJ;;;https://scholar.google.de/citations?user=-CA8QgwAAAAJ",
        "orcid": ";;;;0000-0003-4840-076X",
        "linkedin": ";;;;alois-knoll-505480166",
        "or_profile": "~Qian_Feng2;~Jianxiang_Feng1;zhaopeng.chen@agile-robots.com;~Rudolph_Triebel1;~Alois_Knoll1",
        "aff": "Technische Universit\u00e4t M\u00fcnchen+Agile Robots SE;Agile Robots SE;;;Technical University Munich",
        "aff_domain": "tum.de+agile-robots.com;agile-robots.com;;;tum.de",
        "position": "PhD student+Researcher;Researcher;;;Full Professor",
        "bibtex": "@inproceedings{\nfeng2025ffhflow,\ntitle={{FFHF}low: Diverse and Uncertainty-Aware Dexterous Grasp Generation via Flow Variational Inference},\nauthor={Qian Feng and Jianxiang Feng and Zhaopeng Chen and Rudolph Triebel and Alois Knoll},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=uWFlkufjFJ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=uWFlkufjFJ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;1;2;2;3",
        "aff_unique_norm": "Technische Universit\u00e4t M\u00fcnchen;Agile Robots SE;;Technical University of Munich",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.tum.de;https://www.agilerobots.com;;https://www.tum.de",
        "aff_unique_abbr": "TUM;;;TUM",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0+0;0;0",
        "aff_country_unique": "Germany;"
    },
    {
        "id": "udH3b2Lsx5",
        "title": "$\\texttt{SPIN}$: distilling $\\texttt{Skill-RRT}$ for long-horizon prehensile and non-prehensile manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Current robots struggle with long-horizon manipulation tasks requiring sequences of prehensile and non-prehensile skills, contact-rich interactions, and long-term reasoning. We present $\\texttt{SPIN}$ ($\\textbf{S}$kill $\\textbf{P}$lanning to $\\textbf{IN}$ference), a framework that distills a computationally intensive planning algorithm into a policy via imitation learning.\nWe propose $\\texttt{Skill-RRT}$, an extension of RRT that incorporates skill applicability checks and intermediate object pose sampling for solving such long-horizon problems. To chain independently trained skills, we introduce $\\textit{connectors}$, goal-conditioned policies trained to minimize object disturbance during transitions. High-quality demonstrations are generated with $\\texttt{Skill-RRT}$ and distilled through noise-based replay in order to reduce online computation time. The resulting policy, trained entirely in simulation, transfers zero-shot to the real world and achieves over 80\\% success across three challenging long-horizon manipulation tasks and outperforms state-of-the-art hierarchical RL and planning methods.",
        "keywords": "Robot Skill Chaining;Imitation Learning;Planning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Haewon Jung;Donguk Lee;Haecheol Park;Kim Jun Hyeop;Beomjoon Kim",
        "authorids": "~Haewon_Jung1;~Donguk_Lee1;~Haecheol_Park2;~Kim_Jun_Hyeop1;~Beomjoon_Kim2",
        "gender": "M;M;M;M;M",
        "homepage": "https://github.com/j-hae1;;https://github.com/hcp5004;;https://beomjoonkim.github.io/",
        "dblp": ";;;;88/1505",
        "google_scholar": ";;;;https://scholar.google.ca/citations?user=dw3rEwgAAAAJ",
        "orcid": ";;;;",
        "linkedin": ";donguk-lee-1a8a91246/;;jun-hyeop-kim-3b8020345/;",
        "or_profile": "~Haewon_Jung1;~Donguk_Lee1;~Haecheol_Park2;~Kim_Jun_Hyeop1;~Beomjoon_Kim2",
        "aff": "KAIST;Korea Advanced Institute of Science & Technology;Korea Advanced Institute of Science & Technology;Korea Advanced Institute of Science & Technology;Korea Advanced Institute of Science & Technology",
        "aff_domain": "kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr;kaist.ac.kr",
        "position": "MS student;MS student;Undergrad student;Undergrad student;Assistant Professor",
        "bibtex": "@inproceedings{\njung2025textttspin,\ntitle={\\${\\textbackslash}texttt\\{{SPIN}\\}\\$: distilling \\${\\textbackslash}texttt\\{Skill-{RRT}\\}\\$ for long-horizon prehensile and non-prehensile manipulation},\nauthor={Haewon Jung and Donguk Lee and Haecheol Park and Kim Jun Hyeop and Beomjoon Kim},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=udH3b2Lsx5}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=udH3b2Lsx5",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;0",
        "aff_unique_norm": "Korea Advanced Institute of Science and Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.kaist.ac.kr",
        "aff_unique_abbr": "KAIST",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "South Korea"
    },
    {
        "id": "up2ZpQZbeb",
        "title": "TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Robotic manipulation tasks involving cutting deformable objects remain challenging due to complex topological behaviors, difficulties in perceiving dense object states, and the lack of efficient evaluation methods for cutting outcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for multi-step robotic cutting tasks that integrates a cutting environment and generalized policy learning. TopoCut is built upon three core components: (1) a high-fidelity simulation environment based on a particle-based elastoplastic solver with compliant von Mises constitutive models, augmented by a novel damage-driven topology discovery mechanism for accurate tracking of multiple cutting pieces; (2) a comprehensive reward design that combines this topology discovery with a pose-invariant spectral reward model based on Laplace\u2013Beltrami eigenanalysis, enabling consistent and robust assessment of cutting quality; and (3) an integrated policy learning pipeline, where a dynamics-informed perception module predicts topological evolution and produces particle-wise, topology-aware embeddings to support PDDP\u2014Particle-based Score-Entropy Discrete Diffusion Policy\u2014for goal-conditioned policy learning. Extensive experiments demonstrate that TopoCut enables trajectory generation, scalable learning, precise evaluation, and strong generalization across diverse object geometries, scales, poses, and cutting goals.",
        "keywords": "Deformable object manipulation;multi-step cutting;topology tracking;spectral reward;perception;discrete diffusion policy",
        "primary_area": "",
        "supplementary_material": "/attachment/df574d3cbcdf2acf6b81e8c1014b2cc2ff32711f.pdf",
        "author": "Liquan Wang;Jiangjie Bian;Eric Heiden;Animesh Garg",
        "authorids": "~Liquan_Wang2;~Jiangjie_Bian1;~Eric_Heiden1;~Animesh_Garg1",
        "gender": "M;;;M",
        "homepage": "https://www.linkedin.com/in/liquan-wang-a37634196/;;https://eric-heiden.com/;http://animesh.garg.tech",
        "dblp": ";;;123/5728",
        "google_scholar": ";;iWmtv7gAAAAJ;zp8V7ZMAAAAJ",
        "orcid": ";;;0000-0003-0482-4296",
        "linkedin": ";jiangjie-bian;;animeshgarg/",
        "or_profile": "~Liquan_Wang2;~Jiangjie_Bian1;~Eric_Heiden1;~Animesh_Garg1",
        "aff": "Department of Computer Science;Georgia Institute of Technology;NVIDIA;Georgia Institute of Technology",
        "aff_domain": "cs.toronto.edu;gatech.edu;nvidia.com;gatech.edu",
        "position": "PhD student;MS student;Researcher;Assistant Professor",
        "bibtex": "@inproceedings{\nwang2025topocut,\ntitle={TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies},\nauthor={Liquan Wang and Jiangjie Bian and Eric Heiden and Animesh Garg},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=up2ZpQZbeb}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=up2ZpQZbeb",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "Unknown Institution;Georgia Institute of Technology;NVIDIA",
        "aff_unique_dep": "Department of Computer Science;;NVIDIA Corporation",
        "aff_unique_url": ";https://www.gatech.edu;https://www.nvidia.com",
        "aff_unique_abbr": ";Georgia Tech;NVIDIA",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "ux5EptB7xZ",
        "title": "Geometric Red-Teaming for Robotic Manipulation",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "Standard evaluation protocols in robotic manipulation typically assess policy performance over curated, in-distribution test sets, offering limited insight into how systems fail under plausible variation. \n    We introduce a red-teaming framework that probes robustness through object-centric geometric perturbations, automatically generating CrashShapes---structurally valid, user-constrained mesh deformations that trigger catastrophic failures in pre-trained manipulation policies. \n    The method integrates a Jacobian field\u2013based deformation model with a gradient-free, simulator-in-the-loop optimization strategy.\n    Across insertion, articulation, and grasping tasks, our approach consistently discovers deformations that collapse policy performance, revealing brittle failure modes missed by static benchmarks. \n    By combining task-level policy rollouts with constraint-aware shape exploration, we aim to build a general purpose framework for structured, object-centric robustness evaluation in robotic manipulation.\n    We additionally show that fine-tuning on individual CrashShapes, a process we refer to as blue-teaming, improves task success by up to 60 percentage points on those shapes, while preserving performance on the original object, demonstrating the utility of red-teamed geometries for targeted policy refinement.\n    Finally, we validate both red-teaming and blue-teaming results with a real robotic arm, observing that simulated CrashShapes reduce task success from 90\\% to as low as 22.5\\%, and that blue-teaming recovers performance to up to 90\\% on the corresponding real-world geometry---closely matching simulation outcomes.",
        "keywords": "Red-Teaming;Manipulation;Geometry Perturbation",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Divyam Goel;Yufei Wang;Tiancheng Wu;Guixiu Qiao;Pavel Piliptchak;David Held;Zackory Erickson",
        "authorids": "~Divyam_Goel1;~Yufei_Wang4;~Tiancheng_Wu1;~Guixiu_Qiao1;pavel.piliptchak@nist.gov;~David_Held1;~Zackory_Erickson1",
        "gender": "M;;M;F;;M;M",
        "homepage": "https://dv-fenix.github.io;https://yufeiwang63.github.io/;https://github.com/TonyBest318;https://www.nist.gov/people/helen-qiao;;http://davheld.github.io/;https://zackory.com",
        "dblp": ";;;;;22/11147;",
        "google_scholar": "mE8bVPMAAAAJ;HQl9718AAAAJ;;;;0QtU-NsAAAAJ;wElkTtIAAAAJ",
        "orcid": "0000-0001-7194-9940;;;;;;",
        "linkedin": "divyamgoel3/;;;;;;",
        "or_profile": "~Divyam_Goel1;~Yufei_Wang4;~Tiancheng_Wu1;~Guixiu_Qiao1;pavel.piliptchak@nist.gov;~David_Held1;~Zackory_Erickson1",
        "aff": "Carnegie Mellon University;School of Computer Science, Carnegie Mellon University;Carnegie Mellon University;National Institute of Standards and Technology;;Carnegie Mellon University;Carnegie Mellon University",
        "aff_domain": "cmu.edu;cs.cmu.edu;cmu.edu;nist.gov;;cmu.edu;cmu.edu",
        "position": "MS student;PhD student;MS student;Researcher;;Associate Professor;Assistant Professor",
        "bibtex": "@inproceedings{\ngoel2025geometric,\ntitle={Geometric Red-Teaming for Robotic Manipulation},\nauthor={Divyam Goel and Yufei Wang and Tiancheng Wu and Guixiu Qiao and Pavel Piliptchak and David Held and Zackory Erickson},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ux5EptB7xZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ux5EptB7xZ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1;2;0;0",
        "aff_unique_norm": "Carnegie Mellon University;National Institute of Standards and Technology;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.cmu.edu;https://www.nist.gov;",
        "aff_unique_abbr": "CMU;NIST;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Pittsburgh",
        "aff_country_unique_index": "0;0;0;0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "v2KevjWScT",
        "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Real-world household tasks present significant challenges for mobile manipulation robots. An analysis of existing robotics benchmarks reveals that successful task performance hinges on three key whole-body control capabilities: bimanual coordination, stable and precise navigation, and extensive end-effector reachability. Achieving these capabilities requires careful hardware design, but the resulting system complexity further complicates visuomotor policy learning. To address these challenges, we introduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for whole-body manipulation in diverse household tasks. Built on a bimanual, wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body teleoperation interface for data collection and a novel algorithm for learning whole-body visuomotor policies. We evaluate BRS on five challenging household tasks that not only emphasize the three core capabilities but also introduce additional complexities, such as long-range navigation, interaction with articulated and deformable objects, and manipulation in confined spaces. We believe that BRS's integrated robotic embodiment, data collection interface, and learning framework mark a significant step toward enabling real-world whole-body manipulation for everyday household tasks. BRS is open-sourced at https://behavior-robot-suite.github.io/.",
        "keywords": "Whole-Body Manipulation;Mobile Manipulation;Household Tasks;Imitation Learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yunfan Jiang;Ruohan Zhang;Josiah Wong;Chen Wang;Yanjie Ze;Hang Yin;Cem Gokmen;Shuran Song;Jiajun Wu;Li Fei-Fei",
        "authorids": "~Yunfan_Jiang1;~Ruohan_Zhang1;~Josiah_Wong1;~Chen_Wang16;~Yanjie_Ze1;~Hang_Yin11;~Cem_Gokmen1;~Shuran_Song3;~Jiajun_Wu1;~Li_Fei-Fei1",
        "gender": "M;M;M;M;M;M;M;F;M;F",
        "homepage": "https://yunfanj.com/;https://ai.stanford.edu/~zharu/;https://www.jdw.ong;http://www.chenwangjeremy.net/;http://yanjieze.com;https://hang-yin.github.io/;https://www.cemgokmen.com;https://shurans.github.io/;https://jiajunwu.com;https://profiles.stanford.edu/fei-fei-li",
        "dblp": "311/5581-1;;178/8895;;312/5407;81/7626-10;220/3187;;117/4768;79/2528",
        "google_scholar": "https://scholar.google.com/citations?hl=en;-bqvNWoAAAAJ;Y0a0n5wAAAAJ;lStkAzsAAAAJ;BO_b2O8AAAAJ;VVANWE0AAAAJ;wCiI8oUAAAAJ;https://scholar.google.com/citations?hl=en;2efgcS0AAAAJ;rDfyQnIAAAAJ",
        "orcid": ";;;;;;0000-0001-9446-6052;;0000-0002-4176-343X;",
        "linkedin": ";;josiahw/;;yanjie-ze-a71a0a247/;hangyin0226;cgokmen/;;jiajunwu/;fei-fei-li-4541247/",
        "or_profile": "~Yunfan_Jiang1;~Ruohan_Zhang1;~Josiah_Wong1;~Chen_Wang16;~Yanjie_Ze1;~Hang_Yin11;~Cem_Gokmen1;~Shuran_Song3;~Jiajun_Wu1;~Li_Fei-Fei1",
        "aff": "Stanford University;Stanford University;Stanford University+Stanford University;Computer Science Department, Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University;Stanford University",
        "aff_domain": "cs.stanford.edu;stanford.edu;stanford.edu+stanford.edu;cs.stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu;stanford.edu",
        "position": "PhD student;Postdoc;PhD student+MS student;PhD student;PhD student;Researcher;PhD student;Assistant Professor;Assistant Professor;Full Professor",
        "bibtex": "@inproceedings{\njiang2025behavior,\ntitle={{BEHAVIOR} Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities},\nauthor={Yunfan Jiang and Ruohan Zhang and Josiah Wong and Chen Wang and Yanjie Ze and Hang Yin and Cem Gokmen and Shuran Song and Jiajun Wu and Li Fei-Fei},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=v2KevjWScT}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=v2KevjWScT",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            10,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0+0;0;0;0;0;0;0;0",
        "aff_unique_norm": "Stanford University",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.stanford.edu",
        "aff_unique_abbr": "Stanford",
        "aff_campus_unique_index": "0;0;0+0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Stanford",
        "aff_country_unique_index": "0;0;0+0;0;0;0;0;0;0;0",
        "aff_country_unique": "United States"
    },
    {
        "id": "vLtS0ZDL73",
        "title": "ActLoc: Learning to Localize on the Move via Active Viewpoint Selection",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Reliable localization is critical for robot navigation, yet many existing systems assume that all viewpoints along a trajectory are equally informative. In practice, localization becomes unreliable when the robot observes unmapped, ambiguous, or uninformative regions. To address this, we present ActLoc, an active viewpoint-aware planning framework for enhancing localization accuracy for general robot navigation tasks. At the core of ActLoc is an attention-based model trained at scale for viewpoint selection. This model encodes a metric map of the scene, along with camera poses used during map construction, and estimates localization accuracy over camera pitch and yaw directions at arbitrary 3D waypoint in space. This per-point accuracy distribution is integrated into the path planning process, allowing the robot to actively choose camera orientation that maximize localization robustness while respecting task and motion constraints. ActLoc achieves state-of-the-art performance in single-viewpoint selection task, and generalizes effectively to full-trajectory planning. It provides a modular enhancement to a wide range of navigation and inspection tasks in structured environments.",
        "keywords": "Active Vision; Robot Navigation",
        "primary_area": "",
        "supplementary_material": "/attachment/2a335b6927e0c0a4f13b64cbe35db4e8ab0ab137.zip",
        "author": "Jiajie Li;Boyang Sun;Luca Di Giammarino;Hermann Blum;Marc Pollefeys",
        "authorids": "~Jiajie_Li4;~Boyang_Sun2;~Luca_Di_Giammarino1;~Hermann_Blum1;~Marc_Pollefeys2",
        "gender": ";M;M;;M",
        "homepage": ";https://boysun045.github.io/boysun-website/;https://digiamm.github.io/;https://hermannblum.net;",
        "dblp": ";;287/9293;204/8759;p/MarcPollefeys",
        "google_scholar": ";TXsJ1rUAAAAJ;cZeizisAAAAJ;2Pxx8QIAAAAJ;YYH0BjEAAAAJ",
        "orcid": ";;0000-0002-1332-3496;0000-0002-1713-7877;",
        "linkedin": ";boysun045/;;;marc-pollefeys-30a7075/",
        "or_profile": "~Jiajie_Li4;~Boyang_Sun2;~Luca_Di_Giammarino1;~Hermann_Blum1;~Marc_Pollefeys2",
        "aff": ";ETHZ - ETH Zurich;University of Roma \"La Sapienza\";Rheinische Friedrich-Wilhelms Universit\u00e4t Bonn+Computer Vision and Geometry Lab, ETH Z\u00fcrich;Microsoft+ETHZ - ETH Zurich+Swiss Federal Institute of Technology",
        "aff_domain": ";ethz.ch;uniroma1.it;uni-bonn.de+ethz.ch;microsoft.com+ethz.ch+ethz.ch",
        "position": ";PhD student;Postdoc;Assistant Professor+Postdoc;Director+Professor+Full Professor",
        "bibtex": "@inproceedings{\nli2025actloc,\ntitle={ActLoc: Learning to Localize on the Move via Active Viewpoint Selection},\nauthor={Jiajie Li and Boyang Sun and Luca Di Giammarino and Hermann Blum and Marc Pollefeys},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=vLtS0ZDL73}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=vLtS0ZDL73",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3+1;4+1+5",
        "aff_unique_norm": ";ETH Zurich;University of Rome La Sapienza;Rheinische Friedrich-Wilhelms Universit\u00e4t Bonn;Microsoft;Swiss Federal Institute of Technology",
        "aff_unique_dep": ";;;;Microsoft Corporation;",
        "aff_unique_url": ";https://www.ethz.ch;https://www.uniroma1.it;https://www.uni-bonn.de/;https://www.microsoft.com;https://www.ethz.ch",
        "aff_unique_abbr": ";ETHZ;La Sapienza;Uni Bonn;Microsoft;ETH Zurich",
        "aff_campus_unique_index": "1;;",
        "aff_campus_unique": ";Rome",
        "aff_country_unique_index": "1;2;3+1;4+1+1",
        "aff_country_unique": ";Switzerland;Italy;Germany;United States"
    },
    {
        "id": "vlhoswksBO",
        "title": "$\\pi_{0.5}$: a Vision-Language-Action Model with Open-World Generalization",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "In order for robots to be useful, they must perform practically relevant tasks in the real world, outside of the lab. While vision-language-action (VLA) models have demonstrated impressive results for end-to-end robot control, it remains an open question how far such models can generalize in the wild. We describe $\\pi_{0.5}$, a new model based on $\\pi_0$ that uses co-training on heterogeneous tasks to enable broad generalization. $\\pi_{0.5}$ uses data from multiple robots, high-level semantic prediction, web data, and other sources to enable broadly generalizable real-world robotic manipulation. Our system uses a combination of co-training and hybrid multi-modal examples that combine image observations, language commands, object detections, semantic subtask prediction, and low-level actions. Our experiments show that this kind of knowledge transfer is essential for effective generalization, and we demonstrate for the first time that an end-to-end learning-enabled robotic system can perform long-horizon and dexterous manipulation skills, such as cleaning a kitchen or bedroom, in entirely new homes.",
        "keywords": "vla;generalization;mobile manipulator",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Kevin Black;Noah Brown;James Darpinian;Karan Dhabalia;Danny Driess;Adnan Esmail;Michael Robert Equi;Chelsea Finn;Niccolo Fusai;Manuel Y. Galliker;Dibya Ghosh;Lachy Groom;Karol Hausman;brian ichter;Szymon Jakubczak;Tim Jones;Liyiming Ke;Devin LeBlanc;Sergey Levine;Adrian Li-Bell;Mohith Mothukuri;Suraj Nair;Karl Pertsch;Allen Z. Ren;Lucy Xiaoyang Shi;Laura Smith;Jost Tobias Springenberg;Kyle Stachowicz;James Tanner;Quan Vuong;Homer Walke;Anna Walling;Haohuan Wang;Lili Yu;Ury Zhilinsky",
        "authorids": "~Kevin_Black2;~Noah_Brown1;james.darpinian@physicalintelligence.company;karan@physicalintelligence.company;~Danny_Driess1;adnan@physicalintelligence.company;~Michael_Robert_Equi1;~Chelsea_Finn1;~Niccolo_Fusai1;manuel@physicalintelligence.company;~Dibya_Ghosh1;~Lachy_Groom1;~Karol_Hausman2;~brian_ichter1;szymon@physicalintelligence.company;tim@physicalintelligence.company;~Liyiming_Ke1;devin@physicalintelligence.company;~Sergey_Levine1;~Adrian_Li-Bell1;mohith@physicalintelligence.company;~Suraj_Nair1;~Karl_Pertsch1;~Allen_Z._Ren1;~Lucy_Xiaoyang_Shi1;~Laura_Smith1;~Jost_Tobias_Springenberg1;~Kyle_Stachowicz1;~James_Tanner1;~Quan_Vuong2;~Homer_Walke1;~Anna_Walling1;~Haohuan_Wang1;~Lili_Yu2;ury@physicalintelligence.company",
        "gender": ";M;;;;;M;F;M;;M;M;;;;;F;;M;;;M;;M;F;F;M;M;M;M;;F;M;;",
        "homepage": "https://kevin.black;;;;https://dannydriess.github.io/;;;https://ai.stanford.edu/~cbfinn/;;;https://dibyaghosh.com;http://lachy.com;;;;;http://kayke.xyz/;;https://people.eecs.berkeley.edu/~svlevine/;;;https://suraj-nair-1.github.io/;https://kpertsch.github.io/;http://allenzren.github.io/;https://lucys0.github.io/;;http://www.springenberg-tobias.de;https://kylesta.ch;;https://quanvuong.github.io;;;;;",
        "dblp": "66/9687;;;;;;;131/1783;;;210/2547;;;;;;178/8670;;80/7594;;;;211/7137;;324/5129;54/11024;;;;;;;;;",
        "google_scholar": "axX7PCwAAAAJ;wHuVMCkAAAAJ;;;https://scholar.google.de/citations?user=wxnzyjwAAAAJ;;;vfPE6hgAAAAJ;https://scholar.google.com/citations?view_op=list_works;;znnl0kwAAAAJ;;;-w5DuHgAAAAJ;;;EhOtO3cAAAAJ;;8R35rCwAAAAJ;;;EHSuFcwAAAAJ;https://scholar.google.com/citations?view_op=list_works;mgMzkYMAAAAJ;;;;;;NSWI3OwAAAAJ;;;;;",
        "orcid": ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;",
        "linkedin": ";;;;;;michael-equi/;;niccolo-fusai-472525188/;;;lachy-groom-b218895;;;;;;;;;;;;allenzren/;lucy-xiaoyang-shi/;;;;james-tanner-a42641342/;;;annawalling/;haohuanw/;;",
        "or_profile": "~Kevin_Black2;~Noah_Brown1;james.darpinian@physicalintelligence.company;karan@physicalintelligence.company;~Danny_Driess1;adnan@physicalintelligence.company;~Michael_Robert_Equi1;~Chelsea_Finn1;~Niccolo_Fusai1;manuel@physicalintelligence.company;~Dibya_Ghosh1;~Lachy_Groom1;~Karol_Hausman2;~brian_ichter1;szymon@physicalintelligence.company;tim@physicalintelligence.company;~Liyiming_Ke1;devin@physicalintelligence.company;~Sergey_Levine1;~Adrian_Li-Bell1;mohith@physicalintelligence.company;~Suraj_Nair1;~Karl_Pertsch1;~Allen_Z._Ren1;~Lucy_Xiaoyang_Shi1;~Laura_Smith1;~Jost_Tobias_Springenberg1;~Kyle_Stachowicz1;~James_Tanner1;~Quan_Vuong2;~Homer_Walke1;~Anna_Walling1;~Haohuan_Wang1;~Lili_Yu2;ury@physicalintelligence.company",
        "aff": "Physical Intelligence+University of California, Berkeley;Research, Google;;;Physical Intelliigence;;Physical Intelligence;Physical Intelligence+Stanford University;Columbia University;;University of California, Berkeley;Physical Intelligence;;Physical Intelligence;;;Physical Intelligence;;University of California, Berkeley;;;Physical Intelligence;University of California, Berkeley+Stanford University;Physical Intelligence+Princeton University;Stanford University;University of California, Berkeley;Physical Intelligence+Google DeepMind;University of California, Berkeley;Physical Intelligence;physical intelligence;;Physical Intelligence;Physical Intelligence, Inc;;",
        "aff_domain": "physicalintelligence.company+berkeley.edu;research.google.com;;;physicalintelligence.company;;physicalintelligence.company;physicalintelligence.company+stanford.edu;columbia.edu;;berkeley.edu;physicalintelligence.company;;physicalintelligence.company;;;physicalintelligence.company;;berkeley.edu;;;physicalintelligence.company;berkeley.edu+stanford.edu;physicalintelligence.company+princeton.edu;stanford.edu;berkeley.edu;physicalintelligence.company+google.com;berkeley.edu;physicalintelligence.company;physicalintelligence.company;;physicalintelligence.company;physicalintelligence.company;;",
        "position": "Researcher+PhD student;Researcher;;;Researcher;;Researcher;Researcher+Assistant Professor;Undergrad student;;PhD student;Intern;;Researcher;;;Researcher;;Associate Professor;;;Researcher;Postdoc+Postdoc;Researcher+PhD student;PhD student;PhD student;Researcher+Researcher;PhD student;Principal Researcher;Researcher;;Operations;Engineer;;",
        "bibtex": "@inproceedings{\nblack2025pi,\ntitle={\\${\\textbackslash}pi\\_\\{0.5\\}\\$: a Vision-Language-Action Model with Open-World Generalization},\nauthor={Kevin Black and Noah Brown and James Darpinian and Karan Dhabalia and Danny Driess and Adnan Esmail and Michael Robert Equi and Chelsea Finn and Niccolo Fusai and Manuel Y. Galliker and Dibya Ghosh and Lachy Groom and Karol Hausman and brian ichter and Szymon Jakubczak and Tim Jones and Liyiming Ke and Devin LeBlanc and Sergey Levine and Adrian Li-Bell and Mohith Mothukuri and Suraj Nair and Karl Pertsch and Allen Z. Ren and Lucy Xiaoyang Shi and Laura Smith and Jost Tobias Springenberg and Kyle Stachowicz and James Tanner and Quan Vuong and Homer Walke and Anna Walling and Haohuan Wang and Lili Yu and Ury Zhilinsky},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=vlhoswksBO}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=vlhoswksBO",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            35,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0+1;2;3;3;0;3;0;0+4;5;3;1;0;3;0;3;3;0;3;1;3;3;0;1+4;0+6;4;1;0+2;1;0;0;3;0;7;3;3",
        "aff_unique_norm": "Physical Intelligence;University of California, Berkeley;Google;;Stanford University;Columbia University;Princeton University;Physical Intelligence, Inc",
        "aff_unique_dep": ";;Google Research;;;;;",
        "aff_unique_url": ";https://www.berkeley.edu;https://research.google;;https://www.stanford.edu;https://www.columbia.edu;https://www.princeton.edu;",
        "aff_unique_abbr": ";UC Berkeley;Google;;Stanford;Columbia;Princeton;",
        "aff_campus_unique_index": "1;2;3;1;1;1+3;;3;1;;1",
        "aff_campus_unique": ";Berkeley;Mountain View;Stanford",
        "aff_country_unique_index": "1;1;1;1;1;1;1+1;1;1;1;2;1",
        "aff_country_unique": ";United States;United Kingdom"
    },
    {
        "id": "w0zDVjLscj",
        "title": "KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Learning robot policies that capture multimodality in the training data has been a long-standing open challenge for behavior cloning. Recent approaches tackle the problem by modeling the conditional action distribution with generative models. One of these approaches is Diffusion Policy, which relies on a diffusion model to denoise random points into robot action trajectories. While achieving state-of-the-art performance, it has two main drawbacks that may lead the robot out of the data distribution during policy execution. First, the stochasticity of the denoising process can highly impact on the quality of generated trajectory of actions. Second, being a supervised learning approach, it can learn data outliers from the dataset used for training. Recent work focuses on mitigating these limitations by combining Diffusion Policy either with large-scale training or with classical behavior cloning algorithms. Instead, we propose KDPE, a Kernel Density Estimation-based strategy that filters out potentially harmful trajectories output of Diffusion Policy while keeping a low test-time computational overhead.\nFor Kernel Density Estimation, we propose a manifold-aware kernel to model a probability density function for actions composed of end-effector Cartesian position, orientation, and gripper state. KDPE overall achieves better performance than Diffusion Policy on simulated single-arm RoboMimic and MimicGen tasks, and on three real robot experiments:PickPlush, a tabletop grasping task, CubeSort, a multimodal pick and place task, and CoffeeMaking, a task that requires long-horizon capabilities and precise execution.\n\nThe code will be released upon acceptance and additional material is provided on our anonymized project page:https://kdpe-robotics.github.io.",
        "keywords": "Behavior Cloning;Manipulation;Trajectory Selection",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Andrea Rosasco;Federico Ceola;Giulia Pasquale;Lorenzo Natale",
        "authorids": "~Andrea_Rosasco1;~Federico_Ceola1;~Giulia_Pasquale1;~Lorenzo_Natale1",
        "gender": "M;M;F;",
        "homepage": "https://andrearosasco.github.io/;;;",
        "dblp": "289/0769;;144/4306;",
        "google_scholar": "GwzgVbgAAAAJ;https://scholar.google.it/citations?user=jl8BW1kAAAAJ;https://scholar.google.it/citations?user=8EKDQjcAAAAJ;",
        "orcid": "0000-0002-3918-5829;;0000-0002-7221-3553;",
        "linkedin": ";;;",
        "or_profile": "~Andrea_Rosasco1;~Federico_Ceola1;~Giulia_Pasquale1;~Lorenzo_Natale1",
        "aff": "Universit\u00e0 degli Studi di Genova, Istituto Italiano di Tecnologia;Istituto Italiano di Tecnologia;Istituto Italiano di Tecnologia;",
        "aff_domain": "unige.it;iit.it;iit.it;",
        "position": "PhD student;Postdoc;Senior Technician;",
        "bibtex": "@inproceedings{\nrosasco2025kdpe,\ntitle={{KDPE}: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection},\nauthor={Andrea Rosasco and Federico Ceola and Giulia Pasquale and Lorenzo Natale},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=w0zDVjLscj}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=w0zDVjLscj",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;2",
        "aff_unique_norm": "Universit\u00e0 degli Studi di Genova;Istituto Italiano di Tecnologia;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.unige.it;https://www.iit.it;",
        "aff_unique_abbr": "UniGe;IIT;",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "Italy;"
    },
    {
        "id": "wZUQq0JaL6",
        "title": "Robot Operating Home Appliances by Reading User Manuals",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Operating home appliances, among the most common tools in every\nhousehold, is a critical capability for assistive home robots. This paper presents\nApBot, a robot system that operates novel household appliances by \u201creading\u201d their\nuser manuals. ApBot faces multiple challenges: (i) infer goal-conditioned partial\npolicies from their unstructured, textual descriptions in a user manual document,\n(ii) ground the policies to the appliance in the physical world, and (iii) execute\nthe policies reliably over potentially many steps, despite compounding errors. To\ntackle these challenges, ApBot constructs a structured, symbolic model of an appliance from its manual, with the help of a large vision-language model (VLM). It\ngrounds the symbolic actions visually to control panel elements. Finally, ApBot\ncloses the loop by updating the model based on visual feedback. Our experiments show that across a wide range of simulated and real-world appliances, ApBot achieves consistent and statistically significant improvements in task success\nrate, compared with state-of-the-art large VLMs used directly as control policies.\nThese results suggest that a structured internal representations plays an important\nrole in robust robot operation of home appliances, especially, complex ones.",
        "keywords": "Home Appliance Operation;Structured Model for Decision Making;Foundation Models for Robotics",
        "primary_area": "",
        "supplementary_material": "/attachment/7117445e58c27bfb57e118b77b5afb2331e33f52.zip",
        "author": "Jian Zhang;Hanbo Zhang;Anxing Xiao;David Hsu",
        "authorids": "~Jian_Zhang34;~Hanbo_Zhang1;~Anxing_Xiao1;~David_Hsu1",
        "gender": "F;M;M;M",
        "homepage": "https://github.com/zhangj1an;;https://anxingxiao.com;http://www.comp.nus.edu.sg/~dyhsu/",
        "dblp": ";119/1807;272/5104;29/331",
        "google_scholar": ";1qfEEwsAAAAJ;qrgIuiEAAAAJ;S9LHLKEAAAAJ",
        "orcid": ";;;0000-0002-2309-4535",
        "linkedin": ";;;david-hsu-a86200a1/",
        "or_profile": "~Jian_Zhang34;~Hanbo_Zhang1;~Anxing_Xiao1;~David_Hsu1",
        "aff": ";National University of Singapore;National University of Singapore;National University of Singapore",
        "aff_domain": ";nus.edu.sg;nus.edu.sg;nus.edu.sg",
        "position": ";Postdoc;PhD student;Professor",
        "bibtex": "@inproceedings{\nzhang2025robot,\ntitle={Robot Operating Home Appliances by Reading User Manuals},\nauthor={Jian Zhang and Hanbo Zhang and Anxing Xiao and David Hsu},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=wZUQq0JaL6}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=wZUQq0JaL6",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;1",
        "aff_unique_norm": ";National University of Singapore",
        "aff_unique_dep": ";",
        "aff_unique_url": ";https://www.nus.edu.sg",
        "aff_unique_abbr": ";NUS",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "1;1;1",
        "aff_country_unique": ";Singapore"
    },
    {
        "id": "wnWYoetLhC",
        "title": "Data Retrieval with Importance Weights for Few-Shot Imitation Learning",
        "track": "main",
        "status": "Accept(Oral)",
        "tldr": "",
        "abstract": "While large-scale robot datasets have propelled recent progress in imitation learning, learning from smaller task specific datasets remains critical for deployment in new environments and unseen tasks. One such approach to few-shot imitation learning is retrieval-based imitation learning, which extracts relevant samples from large, widely available prior datasets to augment a limited demonstration dataset. To determine the relevant data from prior datasets, retrieval-based approaches most commonly calculate a prior data point's minimum distance to a point in the target dataset in latent space. While retrieval-based methods have shown success using this metric for data selection, we demonstrate its equivalence to the limit of a Gaussian kernel density (KDE) estimate of the target data distribution. This reveals two shortcomings of the retrieval rule used in prior work. First, it relies on high-variance nearest neighbor estimates that are susceptible to noise. Second, it does not account for the distribution of prior data when retrieving data. To address these issues, we introduce Importance Weighted Retrieval (IWR), which estimates importance weights, or the ratio between the target and prior data distributions for retrieval, using Gaussian KDEs. By considering the probability ratio, IWR overcomes the bias of previous selection rules, and by using reasonable modeling parameters, IWR effectively smooths estimates using all data points.  Across both simulation environments and real-world evaluations on the Bridge dataset we find that our method, IWR, consistently improves performance of existing retrieval-based methods, despite only requiring minor modifications.",
        "keywords": "Few-shot imitation learning;Retrieval;Data selection",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Amber Xie;Rahul Chand;Dorsa Sadigh;Joey Hejna",
        "authorids": "~Amber_Xie1;~Rahul_Chand1;~Dorsa_Sadigh1;~Joey_Hejna1",
        "gender": ";M;F;",
        "homepage": ";https://rahulschand.github.io;https://dorsa.fyi/;",
        "dblp": ";;117/3174;",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;ZaJEZpYAAAAJ;",
        "orcid": ";;;",
        "linkedin": ";;;",
        "or_profile": "~Amber_Xie1;~Rahul_Chand1;~Dorsa_Sadigh1;~Joey_Hejna1",
        "aff": ";;Google+Stanford University;",
        "aff_domain": ";;google.com+stanford.edu;",
        "position": ";;Researcher+Assistant Professor;",
        "bibtex": "@inproceedings{\nxie2025data,\ntitle={Data Retrieval with Importance Weights for Few-Shot Imitation Learning},\nauthor={Amber Xie and Rahul Chand and Dorsa Sadigh and Joey Hejna},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=wnWYoetLhC}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=wnWYoetLhC",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;1+2;0",
        "aff_unique_norm": ";Google;Stanford University",
        "aff_unique_dep": ";Google;",
        "aff_unique_url": ";https://www.google.com;https://www.stanford.edu",
        "aff_unique_abbr": ";Google;Stanford",
        "aff_campus_unique_index": "1+2",
        "aff_campus_unique": ";Mountain View;Stanford",
        "aff_country_unique_index": "1+1",
        "aff_country_unique": ";United States"
    },
    {
        "id": "xPryDEv2YH",
        "title": "AT-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Adaptive teaming\u2014the capability of agents to effectively collaborate with unfamiliar teammates without prior coordination\u2014is widely explored in virtual video games but overlooked in real-world multi-robot contexts. Yet, such adaptive collaboration is crucial for real-world applications, including border surveillance, search-and-rescue, and counter-terrorism operations. To address this gap, we introduce AT-Drone, the first dedicated benchmark explicitly designed to facilitate comprehensive training and evaluation of adaptive teaming strategies in multi-drone pursuit scenarios. AT-Drone makes the following key contributions: (1) An adaptable simulation environment configurator that enables intuitive and rapid setup of adaptive teaming multi-drone pursuit tasks, including four predefined pursuit environments. (2) A streamlined real-world deployment pipeline that seamlessly translates simulation insights into practical drone evaluations using edge devices (such as Jetson Orin Nano) and Crazyflie drones. (3) A novel algorithm zoo integrated with a distributed training framework, featuring diverse algorithms explicitly tailored, for the first time, to multi-pursuer and multi-evader drone pursuit task. (4) Standardized evaluation protocols with newly designed unseen drone zoos, explicitly designed to rigorously assess the performance of adaptive teaming. Comprehensive experimental evaluations across four progressively challenging multi-drone pursuit scenarios confirm AT-Drone's effectiveness in advancing adaptive teaming research. Real-world drone experiments further validate its practical feasibility and utility for realistic robotic operations. Videos, code and weights are available at \\url{https://sites.google.com/view/at-drone}.",
        "keywords": "adaptive teaming;multi-robot collaboration;multi-drone pursuit",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Yang Li;Junfan Chen;Feng Xue;Jiabin Qiu;Wenbin Li;Qingrui Zhang;Ying Wen;Wei Pan",
        "authorids": "~Yang_Li40;~Junfan_Chen3;~Feng_Xue8;~Jiabin_Qiu1;~Wenbin_Li5;~Qingrui_Zhang1;~Ying_Wen1;~Wei_Pan2",
        "gender": "M;M;M;M;M;M;M;M",
        "homepage": "https://liyang.page;;;https://github.com/qjb-ldqmz;https://cs.nju.edu.cn/liwenbin/;;https://yingwen.io;http://panweihit.github.io",
        "dblp": ";;;;27/1736-6.html;;41/4203-1;",
        "google_scholar": "msAmwaoAAAAJ;;;;K-kC4yYAAAAJ;Bt1jFVcAAAAJ;_A1CxG8AAAAJ;GqryWPsAAAAJ",
        "orcid": ";;0009-0002-0990-0737;;;0000-0002-1733-159X;0000-0003-1247-2382;0000-0003-1121-9879",
        "linkedin": ";junfan-chen-714621265;;;;;wenying45;wei-pan-6b558b17/",
        "or_profile": "~Yang_Li40;~Junfan_Chen3;~Feng_Xue8;~Jiabin_Qiu1;~Wenbin_Li5;~Qingrui_Zhang1;~Ying_Wen1;~Wei_Pan2",
        "aff": "University of Manchester;;SUN YAT-SEN UNIVERSITY;Nanjing University;Nanjing University;SUN YAT-SEN UNIVERSITY;Shanghai Jiaotong University;University of Manchester",
        "aff_domain": "cs.manchester.ac.uk;;mail2.sysu.edu.cn;smail.nju.edu.cn;nju.edu.cn;sysu.edu.cn;sjtu.edu.cn;manchester.ac.uk",
        "position": "PhD student;;MS student;Undergrad student;Associate Professor;Associate Professor;Associate Professor;Associate Professor",
        "bibtex": "@inproceedings{\nli2025atdrone,\ntitle={{AT}-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit},\nauthor={Yang Li and Junfan Chen and Feng Xue and Jiabin Qiu and Wenbin Li and Qingrui Zhang and Ying Wen and Wei Pan},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=xPryDEv2YH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=xPryDEv2YH",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            8,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;3;3;2;4;0",
        "aff_unique_norm": "University of Manchester;;Sun Yat-sen University;Nanjing University;Shanghai Jiao Tong University",
        "aff_unique_dep": ";;;;",
        "aff_unique_url": "https://www.manchester.ac.uk;;http://www.sysu.edu.cn;https://www.nju.edu.cn;https://www.sjtu.edu.cn",
        "aff_unique_abbr": "UoM;;SYSU;Nanjing U;SJTU",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;2;2;2;2;2;0",
        "aff_country_unique": "United Kingdom;;China"
    },
    {
        "id": "xVDj9uq6K3",
        "title": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Robot navigation in dynamic, human-centered environments requires socially-compliant decisions grounded in robust scene understanding, including spatiotemporal awareness and the ability to interpret human intentions. Recent Vision-Language Models (VLMs) show exhibit promising capabilities such as object recognition, common-sense reasoning, and contextual understanding\u2014that align with the nuanced requirements of social robot navigation. However, it remains unclear whether VLMs can reliably perform the complex spatiotemporal reasoning and intent inference needed for safe and socially compliant robot navigation. In this paper, we introduce the Social Navigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question Answering (VQA) dataset and benchmark designed to evaluate VLMs for scene understanding in real-world social robot navigation scenarios. SocialNav-SUB provides a unified framework for evaluating VLMs against human and rule-based baselines across VQA tasks requiring spatial, spatiotemporal, and social reasoning in social robot navigation. Through experiments with state-of-the-art VLMs, we find that while the best-performing VLM achieves an encouraging probability of agreeing with human answers, it still underperforms a simpler rule-based approach and human consensus, indicating critical gaps in social scene understanding of current VLMs. Our benchmark sets the stage for further research on foundation models for social robot navigation, offering a framework to explore how VLMs can be tailored to meet real-world social robot navigation needs. We will open source the code and release the benchmark.",
        "keywords": "social robot navigation;scene understanding;vision-language models;VLM;benchmark",
        "primary_area": "",
        "supplementary_material": "/attachment/746552d7d02197aa902f3a3e5cc3273ae288367b.zip",
        "author": "Michael Joseph Munje;Chen Tang;Shuijing Liu;Zichao Hu;Yifeng Zhu;Jiaxun Cui;Garrett Warnell;Joydeep Biswas;Peter Stone",
        "authorids": "~Michael_Joseph_Munje1;~Chen_Tang2;~Shuijing_Liu1;~Zichao_Hu1;~Yifeng_Zhu2;~Jiaxun_Cui1;~Garrett_Warnell1;~Joydeep_Biswas1;~Peter_Stone1",
        "gender": ";M;F;M;M;F;M;M;M",
        "homepage": "https://michaelmunje.com/about/;https://chentangmark.github.io;https://shuijing725.github.io;;https://cs.utexas.edu/~yifengz;https://cuijiaxun.github.io;;https://www.joydeepb.com/;http://www.cs.utexas.edu/~pstone",
        "dblp": ";;211/7210;;;286/8124;173/5902;84/73;s/PeterStone",
        "google_scholar": ";x78TL58AAAAJ;I4k7ukgAAAAJ;Qk-v-okAAAAJ;;https://scholar.google.com/citations?hl=en;Ndp8dmgAAAAJ;https://scholar.google.com.tw/citations?user=f28F1YUAAAAJ;qnwjcfAAAAAJ",
        "orcid": ";;;0009-0007-6433-8878;;0009-0009-1987-9549;;0000-0002-1211-1731;0000-0002-6795-420X",
        "linkedin": ";chen-tang-08377b5b/;shuijing-liu-4089b3123;;;cuijiaxun/;;;",
        "or_profile": "~Michael_Joseph_Munje1;~Chen_Tang2;~Shuijing_Liu1;~Zichao_Hu1;~Yifeng_Zhu2;~Jiaxun_Cui1;~Garrett_Warnell1;~Joydeep_Biswas1;~Peter_Stone1",
        "aff": "University of Texas at Austin;University of Texas at Austin;, University of Texas at Austin;University of Texas at Austin;The University of Texas at Austin;Meta MSL+The University of Texas at Austin;University of Texas, Austin+Army Research Laboratory;NVIDIA+The University of Texas at Austin;Sony AI+University of Texas, Austin",
        "aff_domain": "utexas.edu;utexas.edu;cs.utexas.edu;utexas.edu;utexas.edu;meta.com+utexas.edu;utexas.edu+army.mil;nvidia.com+cs.utexas.edu;sony.com+utexas.edu",
        "position": "PhD student;Postdoc;Postdoc;PhD student;PhD student;Researcher+PhD student;Visiting Researcher+Research Scientist;Visiting Professor+Associate Professor;Principal Researcher+Full Professor",
        "bibtex": "@inproceedings{\nmunje2025socialnavsub,\ntitle={SocialNav-{SUB}: Benchmarking {VLM}s for Scene Understanding in Social Robot Navigation},\nauthor={Michael Joseph Munje and Chen Tang and Shuijing Liu and Zichao Hu and Yifeng Zhu and Jiaxun Cui and Garrett Warnell and Joydeep Biswas and Peter Stone},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=xVDj9uq6K3}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=xVDj9uq6K3",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            9,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;0;1+0;0+2;3+0;4+0",
        "aff_unique_norm": "University of Texas at Austin;Meta MSL;Army Research Laboratory;NVIDIA;Sony",
        "aff_unique_dep": ";;;NVIDIA Corporation;Sony AI",
        "aff_unique_url": "https://www.utexas.edu;;https://www.arl.army.mil;https://www.nvidia.com;https://www.sony.com",
        "aff_unique_abbr": "UT Austin;;ARL;NVIDIA;Sony AI",
        "aff_campus_unique_index": "0;0;0;0;0;0;0;0;0",
        "aff_campus_unique": "Austin;",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0;0+0;2+0",
        "aff_country_unique": "United States;;Japan"
    },
    {
        "id": "xpEjjGC82v",
        "title": "COMBO-Grasp: Learning Constraint-Based Manipulation for Bimanual Occluded Grasping",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "This paper addresses the challenge of occluded robot grasping, i.e. grasping in situations where the desired grasp poses are kinematically infeasible due to environmental constraints such as surface collisions. Existing RL methods struggle with task complexity, and collecting expert demonstrations is often impractical. Instead, inspired by human bimanual manipulation strategies, where two hands coordinate to stabilise and reorient objects, we focus on a bimanual robotic setup to tackle this challenge. In particular, we introduce Constraint-based Manipulation for Bimanual Occluded Grasping (COMBO-Grasp), an approach which leverages two coordinated policies: a constraint policy trained using self-supervised datasets to generate stabilising poses and a grasping policy trained using RL that reorients and grasps the target object. A key contribution lies in value function-guided policy coordination, where gradients from a jointly trained value function refine the constraint policy during RL training to improve bimanual coordination and task performance. Lastly, COMBO-Grasp employs teacher-student policy distillation to effectively deploy vision-based policies in real-world environments. Experiments show that COMBO-Grasp significantly outperforms baselines and generalises to unseen objects in both simulation and real environments.",
        "keywords": "Occluded Grasping;Bimanual Manipulation;Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/c598daa720fc8ea1a8033ade77bbff73f2bd5478.zip",
        "author": "Jun Yamada;Alexander Luis Mitchell;Jack Collins;Ingmar Posner",
        "authorids": "~Jun_Yamada1;~Alexander_Luis_Mitchell1;~Jack_Collins2;~Ingmar_Posner1",
        "gender": "M;M;M;",
        "homepage": "http://junjungoal.github.io;;https://jacktcollins.com/;",
        "dblp": ";268/8155;222/3913;59/542",
        "google_scholar": "ESeyBEEAAAAJ;https://scholar.google.co.uk/citations?user=7YV2TGMAAAAJ;https://scholar.google.com/citations?hl=en;dPk-iwsAAAAJ",
        "orcid": ";;0000-0002-5970-1624;0000-0001-6270-700X",
        "linkedin": ";;jacktcollins/;ingmar-posner-20b49a",
        "or_profile": "~Jun_Yamada1;~Alexander_Luis_Mitchell1;~Jack_Collins2;~Ingmar_Posner1",
        "aff": "University of Oxford;University of Oxford;University of Oxford;Amazon+University of Oxford",
        "aff_domain": "ox.ac.uk;oxford.ac.uk;oxford.ac.uk;amazon.com+ox.ac.uk",
        "position": "PhD student;Postdoc;Postdoc;Principal Researcher+Full Professor",
        "bibtex": "@inproceedings{\nyamada2025combograsp,\ntitle={{COMBO}-Grasp: Learning Constraint-Based Manipulation for Bimanual Occluded Grasping},\nauthor={Jun Yamada and Alexander Luis Mitchell and Jack Collins and Ingmar Posner},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=xpEjjGC82v}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=xpEjjGC82v",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1+0",
        "aff_unique_norm": "University of Oxford;Amazon",
        "aff_unique_dep": ";Amazon.com, Inc.",
        "aff_unique_url": "https://www.ox.ac.uk;https://www.amazon.com",
        "aff_unique_abbr": "Oxford;Amazon",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;1+0",
        "aff_country_unique": "United Kingdom;United States"
    },
    {
        "id": "xzR8rBRgPp",
        "title": "Estimating Value of Assistance for Online POMDP Robotic Agents",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Robotic agents operating in dynamic, partially observable environments often benefit from teammate assistance. We address the challenge of determining when and how to assist in multi-robot systems where agents can modify the physical environment, such as moving obstacles that block perception or manipulation. For robots using online POMDP planning, evaluating assistance impacts requires computationally intensive policy evaluation, making real-time decisions difficult. We formulate Value of Assistance (VOA) for POMDP agents and develop efficient heuristics that approximate VOA without requiring complete policy evaluation. Our empirical evaluation on both a standard POMDP benchmark and a collaborative manipulation task demonstrates that our Full Information heuristic enables real-time assistance decisions while maintaining sufficient accuracy for effective helping action selection.",
        "keywords": "POMDP;Online Planning",
        "primary_area": "",
        "supplementary_material": "/attachment/c973cfa7420cc063233e7fc5f767daf2398fa8a9.zip",
        "author": "Yuval Goshen;Sarah Keren",
        "authorids": "~Yuval_Goshen1;~Sarah_Keren1",
        "gender": "M;",
        "homepage": "https://yuvalgos.github.io/;https://sarahk.cs.technion.ac.il",
        "dblp": ";132/0317",
        "google_scholar": ";Lmco3q8AAAAJ",
        "orcid": ";",
        "linkedin": ";",
        "or_profile": "~Yuval_Goshen1;~Sarah_Keren1",
        "aff": "Technion - Israel Institute of Technology, Technion;Technion",
        "aff_domain": "campus.technion;technion.ac.il",
        "position": "MS student;Assistant Professor",
        "bibtex": "@inproceedings{\ngoshen2025estimating,\ntitle={Estimating Value of Assistance for Online {POMDP} Robotic Agents},\nauthor={Yuval Goshen and Sarah Keren},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=xzR8rBRgPp}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=xzR8rBRgPp",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            2,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0",
        "aff_unique_norm": "Technion - Israel Institute of Technology",
        "aff_unique_dep": "",
        "aff_unique_url": "https://www.technion.ac.il",
        "aff_unique_abbr": "Technion",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "Israel"
    },
    {
        "id": "yOWUy97hmd",
        "title": "Learning Long-Horizon Robot Manipulation Skills via Privileged Action",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Long-horizon contact-rich tasks are challenging to learn with reinforcement learning, due to ineffective exploration of high-dimensional state spaces with sparse rewards. The learning process often gets stuck in local optimum and demands task-specific reward fine-tuning for complex scenarios.  In this work, we propose a structured framework that leverages privileged actions with curriculum learning, enabling the policy to efficiently acquire long-horizon skills without relying on extensive reward engineering or reference trajectories. Specifically, we use privileged actions in simulation with a general training procedure that would be infeasible to implement in real-world scenarios. These privileges include relaxed constraints and virtual forces that enhance interaction and exploration with objects. Our results successfully achieve complex multi-stage long-horizon tasks that naturally combine non-prehensile manipulation with grasping to lift objects from non-graspable poses. We demonstrate generality by maintaining a parsimonious reward structure and showing convergence to diverse and robust behaviors across various environments. Our approach outperforms state-of-the-art methods in these tasks, converging to solutions where others fail.",
        "keywords": "Reinforcement learning;Grasping & Manipulation;Currirulum Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/938bacb02a4c99033b1e104a1fbde92690717ecf.zip",
        "author": "Xiaofeng Mao;Yucheng XU;Zhaole Sun;Elle Miller;Daniel Layeghi;Michael Mistry",
        "authorids": "~Xiaofeng_Mao6;~Yucheng_XU1;~Zhaole_Sun1;~Elle_Miller1;~Daniel_Layeghi1;~Michael_Mistry1",
        "gender": "M;M;M;F;M;",
        "homepage": ";;https://sunzhaole.github.io/;https://elle-miller.github.io;https://daniellayeghi.github.io/;https://homepages.inf.ed.ac.uk/mmistry/index.html",
        "dblp": ";;;;;",
        "google_scholar": "pWzUNKIAAAAJ;;onTsdhYAAAAJ;wJn4Og0AAAAJ;iYLENLcAAAAJ;e378qEIAAAAJ",
        "orcid": ";0000-0001-9023-0974;;;;",
        "linkedin": "xiaofengmao;;;ellemiller101;;",
        "or_profile": "~Xiaofeng_Mao6;~Yucheng_XU1;~Zhaole_Sun1;~Elle_Miller1;~Daniel_Layeghi1;~Michael_Mistry1",
        "aff": "University of Edinburgh;University of Edinburgh;School of Informatics, University of Edinburgh;University of Edinburgh;;University of Edinburgh",
        "aff_domain": "ed.ac.uk;ed.ac.uk;ed.ac.uk;ed.ac.uk;;ed.ac.uk",
        "position": "PhD student;PhD student;PhD student;PhD student;;Full Professor",
        "bibtex": "@inproceedings{\nmao2025learning,\ntitle={Learning Long-Horizon Robot Manipulation Skills via Privileged Action},\nauthor={Xiaofeng Mao and Yucheng XU and Zhaole Sun and Elle Miller and Daniel Layeghi and Michael Mistry},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=yOWUy97hmd}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=yOWUy97hmd",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            6,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;1;0",
        "aff_unique_norm": "University of Edinburgh;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ed.ac.uk;",
        "aff_unique_abbr": "Edinburgh;",
        "aff_campus_unique_index": "1",
        "aff_campus_unique": ";Edinburgh",
        "aff_country_unique_index": "0;0;0;0;0",
        "aff_country_unique": "United Kingdom;"
    },
    {
        "id": "yeuA6M8JIX",
        "title": "Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit Tactile Calibration",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "In unstructured environments, robotic manipulation tasks involving objects with constrained motion trajectories\u2014such as door opening\u2014often experience discrepancies between the robot's vision-guided end-effector trajectory and the object's constrained motion path. \nSuch discrepancies generate unintended harmful forces, which, if exacerbated, may lead to task failure and potential damage to the manipulated objects or the robot itself. To address this issue, this paper introduces a novel diffusion framework, termed SafeDiff. Unlike conventional methods that sequentially fuse visual and tactile data to predict future robot states, our approach generates a prospective state sequence based on the current robot state and visual context observations, using real-time force feedback as a calibration signal. \nThis implicitly adjusts the robot\u2019s state within the state space, enhancing operational success rates and significantly reducing harmful forces during manipulation, thus ensuring manipulation force safety. Additionally, we develop a large-scale simulation dataset named SafeDoorManip50k, offering extensive multimodal data to train and evaluate the proposed method. Extensive experiments show that our visual-tactile model substantially mitigates the risk of harmful forces in the door opening task, across both simulated and real-world settings.",
        "keywords": "Imitation Learning;Trajectory Constrainted Task;Multi-modality;Force Safety",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Lai Wei;Jiahua Ma;Yibo Hu;Ruimao Zhang",
        "authorids": "~Lai_Wei4;~Jiahua_Ma2;~Yibo_Hu4;~Ruimao_Zhang1",
        "gender": "M;M;;M",
        "homepage": "https://i-am-future.github.io/;;;http://zhangruimao.site/#",
        "dblp": ";;;54/10697",
        "google_scholar": ";;;ZJwZdtgAAAAJ",
        "orcid": ";0009-0006-6357-5284;;",
        "linkedin": ";;;",
        "or_profile": "~Lai_Wei4;~Jiahua_Ma2;~Yibo_Hu4;~Ruimao_Zhang1",
        "aff": "University of California, San Diego;SUN YAT-SEN UNIVERSITY;;SUN YAT-SEN UNIVERSITY",
        "aff_domain": "ucsd.edu;sysu.edu.cn;;sysu.edu.cn",
        "position": "MS student;PhD student;;Associate Professor",
        "bibtex": "@inproceedings{\nwei2025ensuring,\ntitle={Ensuring Force Safety in Vision-Guided Robotic Manipulation via Implicit Tactile Calibration},\nauthor={Lai Wei and Jiahua Ma and Yibo Hu and Ruimao Zhang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=yeuA6M8JIX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=yeuA6M8JIX",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;2;1",
        "aff_unique_norm": "University of California, San Diego;Sun Yat-sen University;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.ucsd.edu;http://www.sysu.edu.cn;",
        "aff_unique_abbr": "UCSD;SYSU;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;1;1",
        "aff_country_unique": "United States;China;"
    },
    {
        "id": "ypDETG94BS",
        "title": "Multi-Loco: Unifying Multi-Embodiment Legged Locomotion via Reinforcement Learning Augmented Diffusion",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Generalizing locomotion policies across diverse legged robots with varying morphologies is a key challenge due to differences in observation/action dimensions and system dynamics. In this work, we propose \\textit{Multi-Loco}, a novel unified framework combining a morphology-agnostic generative diffusion model with a lightweight residual policy optimized via reinforcement learning (RL). The diffusion model captures morphology-invariant locomotion patterns from diverse cross-embodiment datasets, improving generalization and robustness. The residual policy is shared across all embodiments and refines the actions generated by the diffusion model, enhancing task-aware performance and robustness for real-world deployment. We evaluated our method with a rich library of four legged robots in both simulation and real-world experiments. Compared to a standard RL framework with PPO, our approach - replacing the Gaussian policy with a diffusion model and residual term - achieves a 10.35\\% average return improvement, with gains up to 13.57\\% in wheeled-biped locomotion tasks. These results highlight the benefits of cross-embodiment data and composite generative architectures in learning robust, generalized locomotion skills.",
        "keywords": "Locomotion;Legged Robots;Multi-Embodiment;Diffusion Model;Reinforcement Learning",
        "primary_area": "",
        "supplementary_material": "/attachment/02a6f9328ef6355a9317c3086c96d9ef813e426d.zip",
        "author": "Shunpeng Yang;Zhen Fu;Zhefeng Cao;Guo Junde;Patrick Wensing;Wei Zhang;Hua Chen",
        "authorids": "~Shunpeng_Yang1;~Zhen_Fu1;~Zhefeng_Cao1;~Guo_Junde1;~Patrick_Wensing1;~Wei_Zhang40;~Hua_Chen2",
        "gender": "M;M;M;M;;M;M",
        "homepage": ";https://github.com/zhenfu128;;https://judera9.github.io/;http://sites.nd.edu/pwensing/;https://www.wzhanglab.site/;",
        "dblp": ";;;;;;",
        "google_scholar": "tPZhG0oAAAAJ;;;;qD9Hb8wAAAAJ;HQ6j-KsAAAAJ;",
        "orcid": ";;0000-0002-5660-7683;;;;0000-0002-4252-8693",
        "linkedin": ";;;;;;",
        "or_profile": "~Shunpeng_Yang1;~Zhen_Fu1;~Zhefeng_Cao1;~Guo_Junde1;~Patrick_Wensing1;~Wei_Zhang40;~Hua_Chen2",
        "aff": "Hong Kong University of Science and Technology;Southern University of Science and Technology;Hong Kong University of Science and Technology;Southern University of Science and Technology;University of Notre Dame;Southern University of Science and Technology of China;Zhejiang University",
        "aff_domain": "hkust.edu;mail.sustech.edu.cn;connect.ust.hk;mail.sustech.edu.cn;nd.edu;sustech.edu.cn;intl.zju.edu.cn",
        "position": "PhD student;PhD student;PhD student;MS student;Associate Professor;Professor;Assistant Professor",
        "bibtex": "@inproceedings{\nyang2025multiloco,\ntitle={Multi-Loco: Unifying Multi-Embodiment Legged Locomotion via Reinforcement Learning Augmented Diffusion},\nauthor={Shunpeng Yang and Zhen Fu and Zhefeng Cao and Guo Junde and Patrick Wensing and Wei Zhang and Hua Chen},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=ypDETG94BS}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=ypDETG94BS",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            7,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;1;2;1;3",
        "aff_unique_norm": "Hong Kong University of Science and Technology;Southern University of Science and Technology;University of Notre Dame;Zhejiang University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.ust.hk;https://www.sustech.edu.cn;https://www.nd.edu;https://www.zju.edu.cn",
        "aff_unique_abbr": "HKUST;SUSTech;Notre Dame;ZJU",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Hong Kong SAR;",
        "aff_country_unique_index": "0;0;0;0;1;0;0",
        "aff_country_unique": "China;United States"
    },
    {
        "id": "zEC8TOXDkH",
        "title": "GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Embodied foundation models are gaining increasing attention for their zero-shot generalization, scalability, and adaptability to new tasks through few-shot post-training. However, existing models rely heavily on real-world data, which is costly and labor-intensive to collect. Synthetic data offers a cost-effective alternative, yet its potential remains largely underexplored. To bridge this gap, we explore the feasibility of training Vision-Language-Action (VLA) models entirely with large-scale synthetic action data. We curate SynGrasp-1B, a billion-frame robotic grasping dataset generated in simulation with photorealistic rendering and extensive domain randomization. Building on this, we present GraspVLA, a VLA model pretrained on large-scale synthetic action data as a foundational model for grasping tasks. GraspVLA integrates autoregressive perception tasks and a flow-matching-based action generation into a unified Chain-of-Thought process, enabling joint training on synthetic action data and Internet semantics data. This design helps mitigate sim-to-real gaps and facilitates the transfer of learned actions to a broader range of Internet-covered objects, achieving open-vocabulary generalization in grasping. Extensive evaluations across real-world and simulation benchmarks demonstrate GraspVLA\u2019s advanced zero-shot generalizability and few-shot adaptability to specific human preferences. We will release SynGrasp-1B dataset and pre-trained weights to benefit the community.",
        "keywords": "Vision-Language-Action;Large-scale Robot Learning;Grasping",
        "primary_area": "",
        "supplementary_material": "/attachment/65e5ddd97209836ad8daa18e6b7139b727ae0d75.zip",
        "author": "Shengliang Deng;Mi Yan;Songlin Wei;Haixin Ma;Yuxin Yang;Jiayi Chen;Zhiqi Zhang;Taoyu Yang;Xuheng Zhang;Heming Cui;Zhizheng Zhang;He Wang",
        "authorids": "~Shengliang_Deng1;~Mi_Yan1;~Songlin_Wei1;~Haixin_Ma2;~Yuxin_Yang10;~Jiayi_Chen5;~Zhiqi_Zhang3;~Taoyu_Yang1;~Xuheng_Zhang1;~Heming_Cui1;~Zhizheng_Zhang1;~He_Wang5",
        "gender": ";;M;M;M;;M;M;M;M;M;M",
        "homepage": ";https://github.com/MiYanDoris;http://songlin.github.io;https://github.com/mahechine;;;;https://imyangty.com;https://github.com/catburgg;https://www.cs.hku.hk/people/academic-staff/heming;;https://hughw19.github.io",
        "dblp": ";95/4123.html;;;;;;;;59/5565.html;67/4758;01/6368-10",
        "google_scholar": ";;jmtAxTgAAAAJ;;7HYFANUAAAAJ;;;;;lW9bpFIAAAAJ;X7M0I8kAAAAJ;roCAWkoAAAAJ",
        "orcid": ";;0000-0002-1487-1494;;0009-0002-3007-3705;;0009-0004-9713-4458;;;0000-0001-7746-440X;;",
        "linkedin": ";;;;;;;;;;;",
        "or_profile": "~Shengliang_Deng1;~Mi_Yan1;~Songlin_Wei1;~Haixin_Ma2;~Yuxin_Yang10;~Jiayi_Chen5;~Zhiqi_Zhang3;~Taoyu_Yang1;~Xuheng_Zhang1;~Heming_Cui1;~Zhizheng_Zhang1;~He_Wang5",
        "aff": ";Peking University;Peking University;;;;Peking University;Peking University;Peking University;the University of Hong Kong, University of Hong Kong;Beijing Galbot Co., Ltd;Galbot+Peking University",
        "aff_domain": ";pku.edu.cn;stu.pku.edu.cn;;;;stu.pku.edu.cn;pku.edu.cn;stu.pku.edu.cn;cs.hku.hk;galbot.com;galbot.com+pku.edu.cn",
        "position": ";PhD student;PhD student;;;;Undergrad student;Undergrad student;Undergrad student;Associate Professor;Principal Researcher;CTO+Assistant Professor",
        "bibtex": "@inproceedings{\ndeng2025graspvla,\ntitle={Grasp{VLA}: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data},\nauthor={Shengliang Deng and Mi Yan and Songlin Wei and Haixin Ma and Yuxin Yang and Jiayi Chen and Zhiqi Zhang and Taoyu Yang and Xuheng Zhang and Heming Cui and Zhizheng Zhang and He Wang},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=zEC8TOXDkH}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zEC8TOXDkH",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            12,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;0;0;0;1;1;1;2;3;3+1",
        "aff_unique_norm": ";Peking University;University of Hong Kong;Galbot",
        "aff_unique_dep": ";;;",
        "aff_unique_url": ";http://www.pku.edu.cn;https://www.hku.hk;",
        "aff_unique_abbr": ";Peking U;HKU;",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Hong Kong SAR",
        "aff_country_unique_index": "1;1;1;1;1;1;1;1",
        "aff_country_unique": ";China"
    },
    {
        "id": "zK2SK6WbYn",
        "title": "Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Vision is well-known for its use in manipulation, especially using visual servoing. To make it robust, multiple cameras are needed to expand the field of view. That is computationally challenging. Merging multiple views and using Q-learning allows the design of more effective representations and optimization of sample efficiency. Such a solution might be expensive to deploy. To mitigate this, we introduce a merge and disentanglement (MAD) algorithm that efficiently merges views to increase sample efficiency while augmenting with single-view features to allow lightweight deployment and ensure robust policies. We demonstrate the efficiency and robustness of our approach using Meta-World and ManiSkill3.",
        "keywords": "reinforcement learning;rl;visual reinforcement learning;robot learning;robot manipulation;representation learning;augmentation;augmentations reinforcement learning;multi view reinforcement learning;multi view robot learning",
        "primary_area": "",
        "supplementary_material": "",
        "author": "Abdulaziz Almuzairee;Rohan Prashant Patil;Dwait Bhatt;Henrik I Christensen",
        "authorids": "~Abdulaziz_Almuzairee1;~Rohan_Prashant_Patil1;~Dwait_Bhatt1;~Henrik_I_Christensen1",
        "gender": ";;M;M",
        "homepage": "http://aalmuzairee.github.io;;https://dwaitbhatt.com;https://www.hichristensen.com",
        "dblp": ";;;c/HIChristensen",
        "google_scholar": ";;dDW1ydgAAAAJ;MA8rI0MAAAAJ",
        "orcid": ";;;0000-0002-7465-7502",
        "linkedin": ";;dwait-bhatt/;henrikichristensen/",
        "or_profile": "~Abdulaziz_Almuzairee1;~Rohan_Prashant_Patil1;~Dwait_Bhatt1;~Henrik_I_Christensen1",
        "aff": "University of California, San Diego;;University of California, San Diego;Computer Science and Engineering Department, University of California, San Diego",
        "aff_domain": "ucsd.edu;;ucsd.edu;cse.ucsd.edu",
        "position": "PhD student;;PhD student;Full Professor",
        "bibtex": "@inproceedings{\nalmuzairee2025merging,\ntitle={Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation},\nauthor={Abdulaziz Almuzairee and Rohan Prashant Patil and Dwait Bhatt and Henrik I Christensen},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=zK2SK6WbYn}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zK2SK6WbYn",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;0",
        "aff_unique_norm": "University of California, San Diego;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.ucsd.edu;",
        "aff_unique_abbr": "UCSD;",
        "aff_campus_unique_index": "0;0;0",
        "aff_campus_unique": "San Diego;",
        "aff_country_unique_index": "0;0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "zQXurgHUVX",
        "title": "DEQ-MPC : Deep Equilibrium Model Predictive Control",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Incorporating task-specific priors within a policy or network architecture is crucial for enhancing safety and improving representation and generalization in robotic control problems. Differentiable Model Predictive Control (MPC) layers have proven effective for embedding these priors, such as constraints and cost functions, directly within the architecture, enabling end-to-end training. However, current methods often treat the solver and the neural network as separate, independent entities, leading to suboptimal integration. In this work, we propose a novel approach that co-develops the solver and architecture unifying the optimization solver and network inference problems. Specifically, we formulate this as a \\textit{joint fixed-point problem} over the coupled network outputs and necessary conditions of the optimization problem. We solve this problem in an iterative manner where we alternate between network forward passes and optimization iterations. Through extensive ablations in various robotic control tasks, we demonstrate that our approach results in richer representations and more stable training, while naturally accommodating warm starting, a key requirement for MPC.",
        "keywords": "MPC;Model Predictive Control;Optimization;Differentiable Optimization;Control",
        "primary_area": "",
        "supplementary_material": "/attachment/8cd07a75ac836efec859c45f0efc937406a888b5.zip",
        "author": "Swaminathan Gurumurthy;Khai Nguyen;Arun L Bishop;J Zico Kolter;Zachary Manchester",
        "authorids": "~Swaminathan_Gurumurthy1;~Khai_Nguyen2;~Arun_L_Bishop1;~J_Zico_Kolter1;~Zachary_Manchester1",
        "gender": "M;M;M;;M",
        "homepage": "https://swami1995.github.io/;https://xkhainguyen.github.io/;;;http://roboticexplorationlab.org/",
        "dblp": "202/1828;;;;192/3194",
        "google_scholar": "do8COWIAAAAJ;;;;utFbPYUAAAAJ",
        "orcid": ";;0009-0004-3816-7268;;",
        "linkedin": ";khainx/;;;",
        "or_profile": "~Swaminathan_Gurumurthy1;~Khai_Nguyen2;~Arun_L_Bishop1;~J_Zico_Kolter1;~Zachary_Manchester1",
        "aff": "School of Computer Science, Carnegie Mellon University;VinUniversity;Carnegie Mellon University;;Carnegie Mellon University",
        "aff_domain": "cs.cmu.edu;vinuni.edu.vn;cmu.edu;;cmu.edu",
        "position": "PhD student;Researcher;PhD student;;Assistant Professor",
        "bibtex": "@inproceedings{\ngurumurthy2025deqmpc,\ntitle={{DEQ}-{MPC} : Deep Equilibrium Model Predictive Control},\nauthor={Swaminathan Gurumurthy and Khai Nguyen and Arun L Bishop and J Zico Kolter and Zachary Manchester},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=zQXurgHUVX}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zQXurgHUVX",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            5,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;0;2;0",
        "aff_unique_norm": "Carnegie Mellon University;VinUniversity;",
        "aff_unique_dep": "School of Computer Science;;",
        "aff_unique_url": "https://www.cmu.edu;https://vinuni.edu.vn;",
        "aff_unique_abbr": "CMU;VinUni;",
        "aff_campus_unique_index": "0",
        "aff_campus_unique": "Pittsburgh;",
        "aff_country_unique_index": "0;1;0;0",
        "aff_country_unique": "United States;Vietnam;"
    },
    {
        "id": "zgVaMD0QjZ",
        "title": "CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Robust robot manipulation in unstructured environments often requires understanding object properties that extend beyond geometry, such as material or compliance\u2014properties that can be challenging to infer using vision alone. Multimodal haptic sensing provides a promising avenue for inferring such properties, yet progress has been constrained by the lack of large, diverse, and realistic haptic datasets. In this work, we introduce the CLAMP device, a low-cost (< $200) sensorized reacher-grabber designed to collect large-scale, in-the-wild multimodal haptic data from non-expert users in everyday settings. We deployed 16 CLAMP devices to 41 participants, resulting in the CLAMP dataset, the largest open-source multimodal haptic dataset to date, comprising 12.3 million datapoints across 5357 household objects. Using this dataset, we train a haptic encoder that can infer material and compliance object properties from multimodal haptic data. We leverage this encoder to create the CLAMP model, a visuo-haptic perception model for material recognition that generalizes to novel objects and three robot embodiments with minimal finetuning. We also demonstrate the effectiveness of our model in three real-world robot manipulation tasks: sorting recyclable and non-recyclable waste, retrieving objects from a cluttered bag, and distinguishing overripe from ripe bananas. Our results show that large-scale, in-the-wild haptic data collection can unlock new capabilities for generalizable robot manipulation.",
        "keywords": "Multimodal haptic perception;Robot manipulation;Datasets;Data-acquisition device",
        "primary_area": "",
        "supplementary_material": "/attachment/bdc4bd15dbb77155e1c386153da8387244247f2e.zip",
        "author": "Pranav N. Thakkar;Shubhangi Sinha;Karan Baijal;Yuhan (Anjelica) Bian;Leah Lackey;Ben Dodson;Heisen Kong;Jueun Kwon;Amber Li;Yifei Hu;alexios rekoutis;Tom Silver;Tapomayukh Bhattacharjee",
        "authorids": "~Pranav_N._Thakkar1;~Shubhangi_Sinha1;~Karan_Baijal1;yb265@cornell.edu;lml276@cornell.edu;bzd4@cornell.edu;hk593@cornell.edu;~Jueun_Kwon1;adl94@cornell.edu;~Yifei_Hu4;~alexios_rekoutis1;~Tom_Silver1;~Tapomayukh_Bhattacharjee1",
        "gender": "M;F;;;;;;F;;;M;M;M",
        "homepage": "https://pranavnnt.github.io;;;;;;;;;;https://horacemann.digication.com/alexios-rekoutis/about-me;https://web.mit.edu/tslvr/www/;http://www.tapomayukh.com",
        "dblp": ";;;;;;;;;;;202/1778;74/8368",
        "google_scholar": "R8mbJXUAAAAJ;;;;;;;PEHv35sAAAAJ;;;;CMcsygMAAAAJ;X1zsXTgAAAAJ",
        "orcid": ";0009-0009-6569-238X;;;;;;0009-0003-5735-0845;;;;;0000-0001-9457-5726",
        "linkedin": ";;karanbaijal/;;;;;jueunkwon/;;yifei-hu-2397b4247?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=ios_app;;;tapomayukh",
        "or_profile": "~Pranav_N._Thakkar1;~Shubhangi_Sinha1;~Karan_Baijal1;yb265@cornell.edu;lml276@cornell.edu;bzd4@cornell.edu;hk593@cornell.edu;~Jueun_Kwon1;adl94@cornell.edu;~Yifei_Hu4;~alexios_rekoutis1;~Tom_Silver1;~Tapomayukh_Bhattacharjee1",
        "aff": "Cornell University;Cornell University;Cornell University;;;;;Northwestern University;;Cornell University;Cornell University;Princeton University+Cornell University;Cornell University",
        "aff_domain": "cornell.edu;cornell.edu;cornell.edu;;;;;u.northwestern.edu;;cornell.edu;cornell.edu;princeton.edu+cornell.edu;cornell.edu",
        "position": "PhD student;Researcher;MS student;;;;;PhD student;;Undergrad student;Intern;Assistant Professor+Postdoc;Assistant Professor",
        "bibtex": "@inproceedings{\nthakkar2025clamp,\ntitle={{CLAMP}: Crowdsourcing a {LA}rge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception},\nauthor={Pranav N. Thakkar and Shubhangi Sinha and Karan Baijal and Yuhan (Anjelica) Bian and Leah Lackey and Ben Dodson and Heisen Kong and Jueun Kwon and Amber Li and Yifei Hu and alexios rekoutis and Tom Silver and Tapomayukh Bhattacharjee},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=zgVaMD0QjZ}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zgVaMD0QjZ",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            13,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;1;1;1;1;2;1;0;0;3+0;0",
        "aff_unique_norm": "Cornell University;;Northwestern University;Princeton University",
        "aff_unique_dep": ";;;",
        "aff_unique_url": "https://www.cornell.edu;;https://www.northwestern.edu;https://www.princeton.edu",
        "aff_unique_abbr": "Cornell;;NU;Princeton",
        "aff_campus_unique_index": "",
        "aff_campus_unique": "",
        "aff_country_unique_index": "0;0;0;0;0;0;0+0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "zk4fRmHF0Q",
        "title": "FlashBack: Consistency Model-Accelerated Shared Autonomy",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Abstract: Shared autonomy is an enabling technology that provides users with control authority over robots that would otherwise be difficult if not impossible to directly control. Yet, standard methods make assumptions that limit their adoption in practice\u2014for example, prior knowledge of the user\u2019s goals or the objective (i.e., reward) function that they wish to optimize, knowledge of the user\u2019s policy, or query-level access to the user during training. Diffusion-based approaches to shared autonomy do not make such assumptions and instead only require access to demonstrations of desired behaviors, while allowing the user to maintain control authority. However, these advantages have come at the expense of high computational complexity, which has made real-time shared autonomy all but impossible. To overcome this limitation, we propose Consistency Shared Autonomy (CSA), a shared autonomy framework that employs a consistency model-based formulation of diffusion. Key to CSA is that it employs the distilled probability flow of ordinary differential equations (PF ODE) to generate high-fidelity samples in a single step. This results in inference speeds significantly than what is possible with previous diffusion-based approaches to shared autonomy, enabling real-time assistance in complex domains with only a single function evaluation. Further, by intervening on flawed actions at intermediate states of the PF ODE, CSA enables varying levels of assistance. We evaluate CSA on a variety of challenging simulated and real-world robot control problems, demonstrating significant improvements over state-of-the-art methods both in terms of task performance and computational efficiency.",
        "keywords": "Consistency Model;Shared Autonomy;ODE Distillation",
        "primary_area": "",
        "supplementary_material": "/attachment/1bb974fa3a496a97226a3732e9a1b09b98a94d55.zip",
        "author": "Luzhe Sun;Jingtian Ji;Xiangshan Tan;Matthew Walter",
        "authorids": "~Luzhe_Sun2;~Jingtian_Ji2;~Xiangshan_Tan1;~Matthew_Walter1",
        "gender": "M;;M;M",
        "homepage": "https://tllokn.github.io/;;https://vincent-tann.github.io/;http://ttic.edu/walter",
        "dblp": "311/4460.html;;;50/7734",
        "google_scholar": "https://scholar.google.com/citations?hl=en;;0XbUhDUAAAAJ;RAiewnEAAAAJ",
        "orcid": ";;;0000-0003-1425-6050",
        "linkedin": ";;;",
        "or_profile": "~Luzhe_Sun2;~Jingtian_Ji2;~Xiangshan_Tan1;~Matthew_Walter1",
        "aff": "Toyota Technological Institute at Chicago;;;Toyota Technological Institute at Chicago",
        "aff_domain": "ttic.edu;;;ttic.edu",
        "position": "PhD student;;;Associate Professor",
        "bibtex": "@inproceedings{\nsun2025flashback,\ntitle={FlashBack: Consistency Model-Accelerated Shared Autonomy},\nauthor={Luzhe Sun and Jingtian Ji and Xiangshan Tan and Matthew Walter},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=zk4fRmHF0Q}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zk4fRmHF0Q",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            4,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;1;1;0",
        "aff_unique_norm": "Toyota Technological Institute at Chicago;",
        "aff_unique_dep": ";",
        "aff_unique_url": "https://www.tti-chicago.org;",
        "aff_unique_abbr": "TTI Chicago;",
        "aff_campus_unique_index": "0;0",
        "aff_campus_unique": "Chicago;",
        "aff_country_unique_index": "0;0",
        "aff_country_unique": "United States;"
    },
    {
        "id": "zxnUgr88rd",
        "title": "Wheeled Lab: Modern Sim2Real for Low-cost, Open-source Wheeled Robotics",
        "track": "main",
        "status": "Accept(Poster)",
        "tldr": "",
        "abstract": "Simulation has been pivotal in recent robotics milestones and is poised to play a prominent role in the field's future.\nHowever, recent robotic advances often rely on expensive and high-maintenance platforms, limiting access to broader robotics audiences. This work introduces Wheeled Lab, a framework for integrating the low-cost, open-source wheeled platforms that are already widely established in education and research with Isaac Lab, an open-source, widely adopted, and rapidly growing simulation framework for robotics research. Wheeled Lab thus introduces to new user communities modern techniques in Sim2Real, such as domain randomization, sensor simulation, and end-to-end learning. To kickstart educational uses, we demonstrate three state-of-the-art policies for small-scale RC cars: controlled drifting, elevation traversal, and visual navigation, each trained and deployed through zero-shot reinforcement learning. By bridging the gap between advanced Sim2Real methods and affordable, available robotics, Wheeled Lab aims to democratize access to cutting-edge tools, fostering innovation and education in a broader robotics context. The full stack, from hardware to software, is low cost and open-source.",
        "keywords": "Sim2Real;Mobile Robots;Education",
        "primary_area": "",
        "supplementary_material": "/attachment/bcde78bbded3bd8b5dd5d924d095f92d018f766b.zip",
        "author": "Tyler Han;Preet Shah;Sidharth Rajagopal;Yanda Bao;Sanghun Jung;Sidharth Talia;Gabriel Guo;Bryan Xu;Bhaumik Mehta;Emma Romig;Rosario Scalise;Byron Boots",
        "authorids": "~Tyler_Han1;~Preet_Shah1;~Sidharth_Rajagopal1;~Yanda_Bao1;~Sanghun_Jung1;~Sidharth_Talia1;~Gabriel_Guo3;~Bryan_Xu1;~Bhaumik_Mehta1;~Emma_Romig1;~Rosario_Scalise1;~Byron_Boots1",
        "gender": "M;M;M;M;M;M;M;M;M;F;Not Specified;",
        "homepage": "https://thanandnow.github.io/;;https://github.com/titanium-47;;https://shjung13.github.io/;https://www.sidharthtalia.com/;;;;;https://robotics.cs.washington.edu;",
        "dblp": ";;;;246/1662;341/3748;;;;;;",
        "google_scholar": "W-wk8X0AAAAJ;;;;e7X7O8gAAAAJ;https://scholar.google.com/citations?hl=en;;;;;;",
        "orcid": ";;;;;;;;;;;",
        "linkedin": ";preet-shah-795a0a190/;sidharth-rajagopal-66932a221;yanda-bao/;;;gabriel-guo/;bryan-hao-xu/;bhaumik-m/;emma-romig-97201617/;;",
        "or_profile": "~Tyler_Han1;~Preet_Shah1;~Sidharth_Rajagopal1;~Yanda_Bao1;~Sanghun_Jung1;~Sidharth_Talia1;~Gabriel_Guo3;~Bryan_Xu1;~Bhaumik_Mehta1;~Emma_Romig1;~Rosario_Scalise1;~Byron_Boots1",
        "aff": "University of Washington;University of Washington;University of Washington;University of Washington;University of Washington;University of Washington;University of Washington;University of Washington;University of Washington;Department of Computer Science, University of Washington;Carnegie Mellon University+University of Washington;",
        "aff_domain": "cs.washington.edu;uw.edu;uw.edu;uw.edu;cs.washington.edu;uw.edu;uw.edu;washington.edu;uw.edu;cs.washington.edu;cmu.edu+uw.edu;",
        "position": "PhD student;MS student;Undergrad student;Undergrad student;PhD student;PhD student;Undergrad student;Undergrad student;Undergrad student;Principal Researcher;MS student+PhD student;",
        "bibtex": "@inproceedings{\nhan2025wheeled,\ntitle={Wheeled Lab: Modern Sim2Real for Low-cost, Open-source Wheeled Robotics},\nauthor={Tyler Han and Preet Shah and Sidharth Rajagopal and Yanda Bao and Sanghun Jung and Sidharth Talia and Gabriel Guo and Bryan Xu and Bhaumik Mehta and Emma Romig and Rosario Scalise and Byron Boots},\nbooktitle={9th Annual Conference on Robot Learning},\nyear={2025},\nurl={https://openreview.net/forum?id=zxnUgr88rd}\n}",
        "github": "",
        "project": "",
        "reviewers": "",
        "site": "https://openreview.net/forum?id=zxnUgr88rd",
        "pdf_size": 0,
        "rating": "",
        "confidence": "",
        "wc_review": "",
        "rating_avg": [
            0,
            0
        ],
        "confidence_avg": [
            0,
            0
        ],
        "replies_avg": [
            1,
            0
        ],
        "authors#_avg": [
            12,
            0
        ],
        "wc_review_avg": [
            0,
            0
        ],
        "corr_rating_confidence": 0,
        "gs_citation": -1,
        "gs_cited_by_link": "",
        "gs_version_total": -1,
        "aff_unique_index": "0;0;0;0;0;0;0;0;0;0;1+0;2",
        "aff_unique_norm": "University of Washington;Carnegie Mellon University;",
        "aff_unique_dep": ";;",
        "aff_unique_url": "https://www.washington.edu;https://www.cmu.edu;",
        "aff_unique_abbr": "UW;CMU;",
        "aff_campus_unique_index": "1;",
        "aff_campus_unique": ";Seattle",
        "aff_country_unique_index": "0;0;0;0;0;0;0;0;0;0;0+0",
        "aff_country_unique": "United States;"
    }
]